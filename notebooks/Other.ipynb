{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTNg0aoYxumN"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')      \n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4h40mg5Ocpz2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vUQ7MBldvx5d"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import fnmatch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import os\n",
    "from pdb import set_trace\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, auc, precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import torch\n",
    "import uuid\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/content/drive/My Drive/collab/TADAT/\") \n",
    "#local\n",
    "from tadat.pipeline import plots\n",
    "import tadat.core as core\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK8cWdcXmzju"
   },
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tWBAvaw5zIe_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# BASE_PATH = \"/content/drive/My Drive/collab/MIMIC/\"\n",
    "BASE_PATH = \"/Users/samir/Dev/projects/MIMIC/MIMIC/\"\n",
    "INPUT_PATH = BASE_PATH+\"/DATA/input/\"\n",
    "FEATURES_PATH = BASE_PATH+\"/DATA/features/\"\n",
    "OUTPUT_PATH = BASE_PATH+\"/DATA/results/\"\n",
    "TMP_PATH = BASE_PATH+\"/DATA/processed/\"\n",
    "\n",
    "TUNE_OUTPUT_PATH = BASE_PATH+\"/DATA/results_fine/\"\n",
    "TUNE_TMP_PATH = BASE_PATH+\"/DATA/processed_fine/\"\n",
    "\n",
    "GRID_OUTPUT_PATH = BASE_PATH+\"/DATA/results_grid/\"\n",
    "GRID_TMP_PATH = BASE_PATH+\"/DATA/processed_grid/\"\n",
    "\n",
    "#configs\n",
    "N_SEEDS=4\n",
    "N_VAL_SEEDS = 5\n",
    "N_VAL_RUNS = 5\n",
    "N_TASKS = 3\n",
    "N_TASKS = 50\n",
    "# PLOT_VARS=[\"auroc\",\"auprc\",\"sensitivity\",\"specificity\"]\n",
    "PLOT_VARS=[\"auroc\",\"sensitivity\"]\n",
    "MODEL=\"BERT-POOL\"\n",
    "METRIC = \"auroc\"\n",
    "\n",
    "GROUPS = { \"GENDER\": [\"M\",\"F\"],   \n",
    "         \"ETHNICITY\": [\"WHITE\",\"BLACK\",\"ASIAN\",\"HISPANIC\"]\n",
    "}\n",
    "\n",
    "CLASSIFIER = 'sklearn'\n",
    "CLASSIFIER = 'torch'\n",
    "# CLASSIFIER = 'mseq'\n",
    "CLINICALBERT = \"emilyalsentzer/Bio_ClinicalBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-GEYGvy7CzIn"
   },
   "outputs": [],
   "source": [
    "\n",
    "SMALL_SIZE = 18\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('text', usetex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sygeVFaACzI8"
   },
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H5SjTOgye0pB"
   },
   "outputs": [],
   "source": [
    "def train_classifier(X_train, Y_train, X_val, Y_val, \n",
    "                     init_seed, shuffle_seed=None, input_dimension=None):    \n",
    "    \"\"\" train a classifier\n",
    "        X_train: training instances \n",
    "        Y_yrain: training labels\n",
    "        X_val: validation instances\n",
    "        Y_val: validation labels\n",
    "        init_seed: parameter initialization seed\n",
    "        shuffle_seed: data shuffling seed\n",
    "        input_dimension: number of input features\n",
    "        \n",
    "        returns: fitted classifier\n",
    "    \"\"\"\n",
    "    #CLASSIFIER is a global variable indicating the type of classifier\n",
    "    if CLASSIFIER == \"torch\":        \n",
    "        x = core.models.MyLinearModel(in_dim=input_dimension, out_dim=1, \n",
    "                    loss_fn=torch.nn.BCEWithLogitsLoss(), \n",
    "                    init_seed=init_seed, n_epochs=500, \n",
    "                    default_lr=0.1, batch_size=None, \n",
    "                    shuffle_seed=shuffle_seed, silent=True,\n",
    "                    shuffle=True) \n",
    "        x.fit(X_train, Y_train, X_val, Y_val)\n",
    "    elif CLASSIFIER == \"mseq\":        \n",
    "        x = core.models.MultiSeqLinearModel(in_dim=input_dimension, out_dim=1, \n",
    "                    loss_fn=torch.nn.BCELoss(), \n",
    "                    init_seed=init_seed, n_epochs=500, \n",
    "                    default_lr=0.1, batch_size=None, \n",
    "                    shuffle_seed=shuffle_seed, silent=True,\n",
    "                    shuffle=True) \n",
    "        x.fit(X_train, Y_train, X_val, Y_val)\n",
    "    elif CLASSIFIER == \"sklearn\":\n",
    "        x = SGDClassifier(loss=\"log\", random_state=shuffle_seed)\n",
    "        x.fit(X_train, Y_train)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return x\n",
    "\n",
    "def evaluate_classifier(model, X_test, Y_test,\n",
    "                   labels, model_name, random_seed, subgroup, res_path=None):\n",
    "    \"\"\" evaluate a classifier\n",
    "        model: classifier to be evaluated        \n",
    "        X_test: test instances\n",
    "        Y_test: test labels\n",
    "        labels: label set\n",
    "        model_name: model name\n",
    "        random_seed: random seed that was used to train the classifier\n",
    "        subgroup: demographic subgroup represented in the data\n",
    "        res_path: path to save the results\n",
    "        \n",
    "        returns: dictionary of evaluation wrt to different metrics\n",
    "    \"\"\"\n",
    "    Y_hat = model.predict(X_test)\n",
    "    Y_hat_prob = model.predict_proba(X_test)\n",
    "    #get probabilities for the positive class\n",
    "    if CLASSIFIER == 'sklearn':\n",
    "        Y_hat_prob = Y_hat_prob[:,labels[1]]    \n",
    "    microF1 = f1_score(Y_test, Y_hat, average=\"micro\") \n",
    "    macroF1 = f1_score(Y_test, Y_hat, average=\"macro\") \n",
    "    try:\n",
    "        aurocc = roc_auc_score(Y_test, Y_hat_prob)\n",
    "    except ValueError:\n",
    "        aurocc = 0\n",
    "    try:\n",
    "        prec, rec, thresholds = precision_recall_curve(Y_test, Y_hat_prob)       \n",
    "        auprc = auc(rec, prec)\n",
    "    except ValueError:\n",
    "        auprc = 0\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_test, Y_hat).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "        sensitivity = tp / (fn+tp)\n",
    "    except ValueError:\n",
    "        specificity, sensitivity = 0, 0\n",
    "    \n",
    "    res = {\"model\":model_name, \n",
    "            \"seed\":random_seed,  \n",
    "            \"group\":subgroup,    \n",
    "            \"microF1\":round(microF1,3),\n",
    "            \"macroF1\":round(macroF1,3),\n",
    "            \"auroc\":round(aurocc,3),\n",
    "            \"auprc\":round(auprc,3),\n",
    "            \"specificity\":round(specificity,3),\n",
    "            \"sensitivity\":round(sensitivity,3)           \n",
    "            }\n",
    "\n",
    "    if res_path is not None:    \n",
    "        core.helpers.save_results(res, res_path, sep=\"\\t\")\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def vectorize(df_train, df_val, df_test, subject_ids):\n",
    "    \"\"\" vectorize the instances and stratify them by demographic subgroup\n",
    "        df_train: training data as a DataFrame\n",
    "        df_test: test data as a DataFrame\n",
    "        df_val: validation data as a DataFrame\n",
    "        subject_ids: list of subject ids (the order corresponds to order of the features that were extracted)\n",
    "        \n",
    "        returns: vectorized train, validation and test datasets, stratified by demographic subgroup\n",
    "                 label vocabulary                 \n",
    "    \"\"\"\n",
    "\n",
    "    #vectorize labels\n",
    "    train_Y = df_train[\"Y\"]\n",
    "    val_Y = df_val[\"Y\"]           \n",
    "    test_Y = df_test[\"Y\"]               \n",
    "    label_vocab = core.vectorizer.get_labels_vocab(train_Y+val_Y)\n",
    "    train_Y,_ = core.vectorizer.label2idx(train_Y, label_vocab)\n",
    "    val_Y,_ = core.vectorizer.label2idx(val_Y, label_vocab)\n",
    "    test_Y,_ = core.vectorizer.label2idx(test_Y, label_vocab)      \n",
    "    \n",
    "    #get indices into the feature matrix\n",
    "    train_idxs = [subject_ids.index(i) for i in list(df_train[\"SUBJECT_ID\"])] \n",
    "    val_idxs = [subject_ids.index(i) for i in list(df_val[\"SUBJECT_ID\"])] \n",
    "    test_idxs = [subject_ids.index(i) for i in list(df_test[\"SUBJECT_ID\"])] \n",
    "    #construct datasets\n",
    "    train = {}\n",
    "    test = {}\n",
    "    val = {}\n",
    "    #unstratified \n",
    "    train[\"all\"] = [train_idxs, train_Y]\n",
    "    test[\"all\"] = [test_idxs, test_Y]\n",
    "    val[\"all\"] = [val_idxs, val_Y]\n",
    "    #stratified by demographics \n",
    "    for group in list(GROUPS.keys()):\n",
    "        #and subgroups\n",
    "        for subgroup in GROUPS[group]:                \n",
    "            df_train_sub = df_train[df_train[group] == subgroup]\n",
    "            df_test_sub = df_test[df_test[group] == subgroup]\n",
    "            df_val_sub = df_val[df_val[group] == subgroup]\n",
    "            # print(\"[subgroup: {} | tr: {} | ts: {} | val: {}]\".format(subgroup, len(df_train_sub), len(df_test_sub), len(df_val_sub)))\n",
    "\n",
    "            #vectorize labels               \n",
    "            train_Y_sub,_ = core.vectorizer.label2idx(df_train_sub[\"Y\"], label_vocab)            \n",
    "            test_Y_sub,_ = core.vectorizer.label2idx(df_test_sub[\"Y\"], label_vocab)            \n",
    "            val_Y_sub,_ = core.vectorizer.label2idx(df_val_sub[\"Y\"], label_vocab)      \n",
    "            #get indices into the feature matrix\n",
    "            train_idxs_sub = [subject_ids.index(i) for i in list(df_train_sub[\"SUBJECT_ID\"])] \n",
    "            test_idxs_sub = [subject_ids.index(i) for i in list(df_test_sub[\"SUBJECT_ID\"])] \n",
    "            val_idxs_sub = [subject_ids.index(i) for i in list(df_val_sub[\"SUBJECT_ID\"])] \n",
    "            if subgroup == \"M\":\n",
    "                subgroup = \"men\"\n",
    "            elif subgroup == \"F\":\n",
    "                subgroup = \"women\"\n",
    "            train[subgroup.lower()] = [train_idxs_sub, train_Y_sub]\n",
    "            test[subgroup.lower()] = [test_idxs_sub, test_Y_sub]\n",
    "            val[subgroup.lower()] = [val_idxs_sub, val_Y_sub]\n",
    "\n",
    "    return train, val, test, label_vocab\n",
    "\n",
    "\n",
    "def get_features(data, vocab_size, feature_type, word_vectors=None):\n",
    "    \"\"\" compute features from the data\n",
    "        data: data instances\n",
    "        vocab_size: size of the vocabulary\n",
    "        feature_type: type of feature (e.g bag of words, BERT)\n",
    "        word_vectors: path to pretrained (static) word vectors\n",
    "        \n",
    "        returns: feature matrix\n",
    "    \"\"\"\n",
    "    if feature_type == \"BOW-BIN\":\n",
    "        X = core.features.BOW(data, vocab_size,sparse=True)\n",
    "    elif feature_type == \"BOW-FREQ\":\n",
    "        X = core.features.BOW_freq(data, vocab_size,sparse=True)\n",
    "    elif feature_type == \"BOE-BIN\":\n",
    "        X = core.features.BOE(data, word_vectors,\"bin\")\n",
    "    elif feature_type == \"BOE-SUM\": \n",
    "        X = core.features.BOE(data, word_vectors,\"sum\")\n",
    "    elif feature_type == \"BERT-POOL\":\n",
    "        X =  core.transformer_encoders.encode_sequences(data, batchsize=64)        \n",
    "    elif feature_type == \"BERT-CLS\":\n",
    "        X =  core.transformer_encoders.encode_sequences(data, cls_features=True,\n",
    "                                                        batchsize=64)            \n",
    "    elif feature_type == \"MULTI-BERT-POOL\":\n",
    "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, batchsize=32,\n",
    "                                                         tmp_path=TMP_PATH)\n",
    "    elif feature_type == \"MULTI-BERT-CLS\":\n",
    "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, \n",
    "                                                         cls_features=True,\n",
    "                                                         batchsize=32,\n",
    "                                                         tmp_path=TMP_PATH)\n",
    "    elif feature_type == \"CLINICALBERT-POOL\":\n",
    "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
    "        X =  core.transformer_encoders.encode_sequences(data, batchsize=64, tokenizer=tokenizer,\n",
    "                                                                    encoder=encoder)        \n",
    "    elif feature_type == \"CLINICALBERT-CLS\":\n",
    "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
    "        X =  core.transformer_encoders.encode_sequences(data, cls_features=True,batchsize=64,\n",
    "                                                                    tokenizer=tokenizer, encoder=encoder)        \n",
    "    elif feature_type == \"CLINICALMULTI-BERT-POOL\":\n",
    "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
    "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, batchsize=32,tmp_path=TMP_PATH,\n",
    "                                                              tokenizer=tokenizer, encoder=encoder)\n",
    "    elif feature_type == \"CLINICALMULTI-BERT-CLS\":\n",
    "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
    "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, cls_features=True, \n",
    "                                                                batchsize=32,tmp_path=TMP_PATH,\n",
    "                                                                tokenizer=tokenizer, encoder=encoder)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return X\n",
    "\n",
    "def extract_features(feature_type, path):\n",
    "    \"\"\" extract features and save features\n",
    "\n",
    "        method will first look for computed features on disk and return them if found;\n",
    "        otherwise it will look for a file with name *patients.csv*;        \n",
    "        \n",
    "        feature_type: type of feature (e.g bag of words, BERT)\n",
    "        path: directory where the data can be found\n",
    "                \n",
    "        returns: list of subject ids and feature matrix -- the order of ids corresponds to order of the instances in the feature matrix\n",
    "    \"\"\"\n",
    "    X = read_cache(path+\"feats_{}\".format(feature_type))\n",
    "    if X:\n",
    "        print(\"[reading cached features]\")\n",
    "        subject_ids, X_feats = X\n",
    "    else:\n",
    "        print(\"[computing {} features]\".format(feature_type))\n",
    "        df = pd.read_csv(path+\"patients.csv\", sep=\"\\t\", header=0)\n",
    "        subject_ids = list(df[\"SUBJECT_ID\"])\n",
    "        docs = list(df[\"TEXT\"])\n",
    "        if \"BERT\" in feature_type:\n",
    "            X_feats = get_features(docs, None, feature_type)\n",
    "        else:\n",
    "            X, word_vocab = core.vectorizer.docs2idx(docs)\n",
    "            X_feats = get_features(X,len(word_vocab),feature_type)\n",
    "        #save features\n",
    "        print(\"[saving features]\")\n",
    "        write_cache(path+\"feats_{}\".format(feature_type), \n",
    "                    [subject_ids, X_feats])\n",
    "    return subject_ids, X_feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCwwyC1RCzJC"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6v8lR5X0cp0i"
   },
   "outputs": [],
   "source": [
    "def read_dataset(path, dataset_name, df_patients):    \n",
    "    \n",
    "    \"\"\" read dataset        \n",
    "        path: path to the dataset\n",
    "        dataset_name: name of the dataset\n",
    "        df_patients: DataFrame of patients\n",
    "                \n",
    "        returns: train, test and validation sets as DataFrames\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(\"{}/{}_train.csv\".format(path, dataset_name), \n",
    "                           sep=\"\\t\", header=0)\n",
    "    df_test  = pd.read_csv(\"{}/{}_test.csv\".format(path, dataset_name),\n",
    "                           sep=\"\\t\", header=0)\n",
    "    df_val   = pd.read_csv(\"{}/{}_val.csv\".format(path, dataset_name),\n",
    "                           sep=\"\\t\", header=0)\n",
    "    #set indices\n",
    "    df_patients.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "    df_train.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "    df_test.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "    df_val.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "\n",
    "    df_train = df_train.join(df_patients, on=\"SUBJECT_ID\", \n",
    "                             how=\"inner\", lsuffix=\"N_\").reset_index()\n",
    "    df_test = df_test.join(df_patients, on=\"SUBJECT_ID\", \n",
    "                           how=\"inner\", lsuffix=\"N_\").reset_index()\n",
    "    df_val = df_val.join(df_patients, on=\"SUBJECT_ID\", \n",
    "                         how=\"inner\", lsuffix=\"N_\").reset_index()\n",
    "\n",
    "    return df_train, df_test, df_val   \n",
    "\n",
    "\n",
    "def read_cache(path):\n",
    "    \"\"\" read a pickled object\n",
    "        \n",
    "        path: path\n",
    "        returns: object\n",
    "    \"\"\"\n",
    "    X = None\n",
    "    try:\n",
    "        with open(path, \"rb\") as fi:            \n",
    "            X = pickle.load(fi)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def write_cache(path, o):\n",
    "    \"\"\" pickle an object\n",
    "            \n",
    "        path: path\n",
    "        o: object\n",
    "    \"\"\"\n",
    "    dirname = os.path.dirname(path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    with open(path, \"wb\") as fo:\n",
    "        pickle.dump(o, fo)\n",
    "\n",
    "def run(data_path, dataset, features_path, feature_type, cache_path, metric, n_seeds=N_SEEDS, clear_results=False):\n",
    "    \"\"\" \n",
    "        train classifiers with different random seeds and compare the performance over each demographic subgroup\n",
    "\n",
    "        data_path: path to the data\n",
    "        dataset: dataset to be evaluted\n",
    "        features_path: path to the features\n",
    "        feature_type: type of feature (e.g bag of words, BERT)\n",
    "        cache_path: cache path \n",
    "        metric: evaluation metric\n",
    "        n_seeds: number of seeds\n",
    "\n",
    "        returns: results for each subgroup\n",
    "    \"\"\"\n",
    "    #read patients data\n",
    "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
    "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
    "    #read dataset\n",
    "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
    "    \n",
    "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
    "    print(\"[running {} classifier]\".format(CLASSIFIER))\n",
    "    #extract features\n",
    "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
    "    train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
    "    train_idx, train_Y = train[\"all\"]\n",
    "    val_idx, val_Y = val[\"all\"]\n",
    "    #slice the feature matrix to get the corresponding instances\n",
    "    train_X = feature_matrix[train_idx, :]    \n",
    "    val_X = feature_matrix[val_idx, :]    \n",
    "    #create the cache directory if it does not exist\n",
    "    dirname = os.path.dirname(cache_path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    #try to open a cached results file or create a new one if it does not exist\n",
    "    res_fname = cache_path+\"/seeds_{}_{}_{}.pkl\".format(dataset, feature_type, metric).lower()    \n",
    "    try:\n",
    "        df_results = pd.read_csv(res_fname)\n",
    "    except FileNotFoundError:\n",
    "        df_results = pd.DataFrame(columns = [\"seed\"] +  list(val.keys()))\n",
    "        df_results.to_csv(res_fname, index=False, header=True)        \n",
    "    #we can skip seeds that have already been evaluated\n",
    "    skip_seeds = set([]) if clear_results else set(df_results[\"seed\"])\n",
    "    groups = list(val.keys())\n",
    "    random.seed(1) #ensure repeateable runs \n",
    "    random_seeds = random.sample(range(0, 10000), n_seeds)        \n",
    "    incremental_results = {}     \n",
    "    ##train/test classifier for each random seed pair\n",
    "    for init_seed, shuffle_seed in itertools.product(random_seeds,repeat=2):   \n",
    "        seed = \"{}x{}\".format(init_seed, shuffle_seed)          \n",
    "        if seed in skip_seeds:\n",
    "            print(\"skipped seed: {}\".format(seed))\n",
    "            continue\n",
    "        curr_results = {\"seed\":seed}\n",
    "        print(\" > seed: {}\".format(seed))                        \n",
    "        model = train_classifier(train_X, train_Y,val_X, val_Y,  \n",
    "                                    input_dimension=train_X.shape[-1],\n",
    "                                    init_seed=init_seed, \n",
    "                                    shuffle_seed=shuffle_seed)                                                                                \n",
    "        #test each subgroup (note thtat *all* is also a subgroup)\n",
    "        for subgroup in groups:                                \n",
    "            test_idx_sub, test_Y_sub = test[subgroup]                 \n",
    "            test_X_sub = feature_matrix[test_idx_sub, :]                \n",
    "            res_sub = evaluate_classifier(model, test_X_sub, test_Y_sub, \n",
    "                                        label_vocab, feature_type, seed, subgroup)                \n",
    "            curr_results[subgroup]= res_sub[metric]     \n",
    "        #save results\n",
    "        df_results = df_results.append(curr_results, ignore_index=True)\n",
    "        df_results.to_csv(res_fname, index=False, header=True)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def subsample_run(data_path, dataset, features_path, feature_type, cache_path, n_draws=10):\n",
    "    #read patients data\n",
    "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
    "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
    "\n",
    "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
    "    \n",
    "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
    "    print(\"[{} classifier]\".format(CLASSIFIER))\n",
    "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
    "    train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
    "    train_idx, train_Y = train[\"all\"]\n",
    "    val_idx, val_Y = val[\"all\"]\n",
    "    #slice the feature matrix to get the corresponding instances\n",
    "    train_X = feature_matrix[train_idx, :]    \n",
    "    val_X = feature_matrix[val_idx, :]    \n",
    "    random.seed(1) #ensure repeateable runs \n",
    "    random_seeds = random.sample(range(0, 10000), N_SEEDS)        \n",
    "    incremental_results = {}     \n",
    "    sample_size = min([len(test[subgroup][0]) for subgroup in test.keys()])\n",
    "    print(sample_size)\n",
    "    ##train/test classifier for each random seed pair    \n",
    "    for init_seed, shuffle_seed in itertools.product(random_seeds,repeat=2):        \n",
    "        seed = \"{}x{}\".format(init_seed, shuffle_seed)\n",
    "        res_fname = \"{}_{}_res{}.pkl\".format(dataset, feature_type, seed).lower()                        \n",
    "        curr_results = {}\n",
    "        print(\" > seed: {}\".format(seed))                        \n",
    "        model = train_classifier(train_X, train_Y,val_X, val_Y,  \n",
    "                                    input_dimension=train_X.shape[-1],\n",
    "                                    init_seed=init_seed, \n",
    "                                    shuffle_seed=shuffle_seed)                                                      \n",
    "        for i in range(n_draws):\n",
    "            #evaluate different random samples of the data\n",
    "            #test each subgroup (note thtat *all* is also a subgroup)            \n",
    "            for subgroup in test.keys():                                \n",
    "                test_idx_sub, test_Y_sub = test[subgroup]            \n",
    "                test_Y_sub = np.array(test_Y_sub)\n",
    "                test_idx_sub = np.array(test_idx_sub)                    \n",
    "                random_sample = random.sample(range(len(test_idx_sub)), sample_size)                \n",
    "                test_Y_sub_sample = test_Y_sub[random_sample]\n",
    "                test_idx_sub_sample = test_idx_sub[random_sample]                    \n",
    "                test_X_sub_sample = feature_matrix[test_idx_sub_sample, :]                \n",
    "                res_sub = evaluate_classifier(model, test_X_sub_sample, test_Y_sub_sample, \n",
    "                                            label_vocab, feature_type, seed+\"x\"+str(i), subgroup)                \n",
    "                curr_results[subgroup]= res_sub                       \n",
    "        \n",
    "            incremental_results = merge_results(curr_results, incremental_results, \n",
    "                                            list(test.keys()))\n",
    "    #build dataframes \n",
    "    df_results = results_to_df(incremental_results, test.keys())\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation vs Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we investigate the correlation between validation and test performance as a function of the random seed\n",
    "\n",
    "def val_vs_test(data_path, dataset, features_path, feature_type, cache_path, metric, n_seeds=N_SEEDS):\n",
    "    \"\"\" \n",
    "        compare the validation and test performance of classifiers as a function of the random seeds\n",
    "        \n",
    "        data_path: path to the data\n",
    "        dataset: dataset to be evaluted\n",
    "        features_path: path to the features\n",
    "        feature_type: type of feature (e.g bag of words, BERT)\n",
    "        cache_path: cache path \n",
    "        metric: evaluation metric \n",
    "        n_seeds: number of random seeds to be evaluated \n",
    "\n",
    "        returns: results for each demographic group\n",
    "    \"\"\"\n",
    "    #read patients data\n",
    "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
    "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
    "\n",
    "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
    "    \n",
    "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
    "    print(\"[running {} classifier]\".format(CLASSIFIER))\n",
    "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
    "    train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
    "    train_idx, train_Y = train[\"all\"]\n",
    "    val_idx, val_Y = val[\"all\"]\n",
    "    #slice the feature matrix to get the corresponding instances\n",
    "    train_X = feature_matrix[train_idx, :]    \n",
    "    val_X = feature_matrix[val_idx, :]    \n",
    "    \n",
    "    dirname = os.path.dirname(cache_path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    res_fname = cache_path+\"/val_vs_test_{}_{}_{}.pkl\".format(dataset, feature_type, metric).lower()    \n",
    "    try:\n",
    "        df_results = pd.read_csv(res_fname)\n",
    "    except FileNotFoundError:\n",
    "        df_results = pd.DataFrame(columns = [\"seed\",\"data\"] +  list(val.keys()))\n",
    "        df_results.to_csv(res_fname, index=False, header=True)        \n",
    "    groups = list(val.keys())\n",
    "    skip_seeds = set(df_results[\"seed\"])\n",
    "\n",
    "    random.seed(1) #ensure repeateable runs \n",
    "    random_seeds = random.sample(range(0, 10000), n_seeds)            \n",
    "    ##train/test classifier for each random seed pair\n",
    "    for init_seed, shuffle_seed in itertools.product(random_seeds,repeat=2):        \n",
    "        seed = \"{}x{}\".format(init_seed, shuffle_seed)          \n",
    "        if seed in skip_seeds:\n",
    "            print(\"skipped seed: {}\".format(seed))\n",
    "            continue\n",
    "        test_results = {\"seed\":seed, \"data\":\"test\"}\n",
    "        print(\" > seed: {}\".format(seed))                        \n",
    "        model = train_classifier(train_X, train_Y,val_X, val_Y,  \n",
    "                                    input_dimension=train_X.shape[-1],\n",
    "                                    init_seed=init_seed, \n",
    "                                    shuffle_seed=shuffle_seed)                                                                                \n",
    "        #test each subgroup on test data (note thtat *all* is also a subgroup)\n",
    "        for subgroup in groups:                                \n",
    "            test_idx_sub, test_Y_sub = test[subgroup]                 \n",
    "            test_X_sub = feature_matrix[test_idx_sub, :]                \n",
    "            res_sub = evaluate_classifier(model, test_X_sub, test_Y_sub, \n",
    "                                        label_vocab, feature_type, seed, subgroup)                \n",
    "            test_results[subgroup]= res_sub[metric]     \n",
    "        df_results = df_results.append(test_results, ignore_index=True)\n",
    "        val_results = {\"seed\":seed, \"data\":\"val\"}\n",
    "        #test each subgroup on validation data (note thtat *all* is also a subgroup)\n",
    "        for subgroup in groups:                                \n",
    "            val_idx_sub, val_Y_sub = val[subgroup]                 \n",
    "            val_X_sub = feature_matrix[val_idx_sub, :]                \n",
    "            res_sub = evaluate_classifier(model, val_X_sub, val_Y_sub, \n",
    "                                        label_vocab, feature_type, seed, subgroup)                \n",
    "            val_results[subgroup]= res_sub[metric]     \n",
    "\n",
    "        df_results = df_results.append(val_results, ignore_index=True)\n",
    "        df_results.to_csv(res_fname, index=False, header=True)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "def all_val_vs_test(data_path, tasks_fname, features_path, feature_type, cache_path, metric, n_seeds=N_SEEDS):\n",
    "    \"\"\" \n",
    "        compare the validation and test performance of classifiers as a function of the random seeds for all the datasets\n",
    "        \n",
    "        data_path: path to the data\n",
    "        dataset: dataset to be evaluted\n",
    "        features_path: path to the features\n",
    "        feature_type: type of feature (e.g bag of words, BERT)\n",
    "        cache_path: cache path \n",
    "        metric: evaluation metric \n",
    "        n_seeds: number of random seeds to be evaluated \n",
    "    \"\"\"\n",
    "    with open(data_path+\"/\"+tasks_fname,\"r\") as fid:        \n",
    "        for i,l in enumerate(fid):            \n",
    "            task_abv, task_name = l.strip(\"\\n\").split(\",\")        \n",
    "            print(task_name+\"\\n\")    \n",
    "            val_vs_test(data_path, \"mini-\"+task_abv, features_path, feature_type, cache_path, metric, n_seeds=n_seeds)\n",
    "\n",
    "\n",
    "def plot_val_vs_test(cache_path, dataset, feature_type, metric):\n",
    "    res_fname = cache_path+\"/val_vs_test_{}_{}_{}.pkl\".format(dataset, feature_type, metric).lower()    \n",
    "    df_results = pd.read_csv(res_fname)  \n",
    "    df_val = df_results[df_results[\"data\"]==\"val\"]\n",
    "    df_test = df_results[df_results[\"data\"]==\"test\"]\n",
    "    subgroups = [\"men\",\"women\",\"white\",\"black\",\"hispanic\",\"asian\"]\n",
    "    n_rows=2\n",
    "    n_cols = 3\n",
    "    fig, ax = plt.subplots(n_rows, n_cols,  figsize=(15,10), sharex=True, sharey=True)\n",
    "    #current coloramap\n",
    "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    coords = list(itertools.product(range(n_rows),range(n_cols)))   \n",
    "    \n",
    "    for subgroup, col, coord in zip(subgroups, cmap, coords ):                \n",
    "        cax = ax[coord[0]][coord[1]]\n",
    "        cax.scatter(x=df_val[subgroup],y=df_test[subgroup],\n",
    "                            color=col)                \n",
    "        x = df_val[subgroup]\n",
    "        y = df_test[subgroup]\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        y_hat = np.poly1d(z)(x)\n",
    "        ax[coord[0]][coord[1]].plot(x, y_hat, c=col, lw=1)\n",
    "        ax[coord[0]][coord[1]].set_title(subgroup)        \n",
    "        ax[coord[0]][coord[1]].set_xlabel(\"val\")\n",
    "        ax[coord[0]][coord[1]].set_ylabel(\"test\")\n",
    "    fig.suptitle(\"val vs test\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_val_vs_test_deltas(cache_path, dataset, feature_type, metric):\n",
    "    res_fname = cache_path+\"/val_vs_test_{}_{}_{}.pkl\".format(dataset, feature_type, metric).lower()    \n",
    "    df_results = pd.read_csv(res_fname)  \n",
    "    df_val = df_results[df_results[\"data\"]==\"val\"]\n",
    "    df_test = df_results[df_results[\"data\"]==\"test\"]    \n",
    "    subgroups = [\"men\",\"women\",\"white\",\"black\",\"hispanic\",\"asian\"]\n",
    "    df_val_all = df_val[\"all\"]\n",
    "    df_test_all = df_test[\"all\"]\n",
    "    for s in subgroups:\n",
    "        df_val[\"delta_\"+s] = (df_val[s] - df_val_all) #.abs()\n",
    "        df_test[\"delta_\"+s] = (df_test[s] - df_test_all) #.abs()\n",
    "\n",
    "    n_rows=2\n",
    "    n_cols = 3\n",
    "    fig, ax = plt.subplots(n_rows, n_cols,  figsize=(15,10), sharex=True, sharey=True)\n",
    "    #current coloramap\n",
    "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    coords = list(itertools.product(range(n_rows),range(n_cols)))   \n",
    "    \n",
    "    for subgroup, col, coord in zip(subgroups, cmap, coords ):                \n",
    "        cax = ax[coord[0]][coord[1]]\n",
    "        cax.scatter(x=df_val[\"delta_\"+subgroup],y=df_test[\"delta_\"+subgroup],\n",
    "                            color=col)                \n",
    "        x = df_val[\"delta_\"+subgroup]\n",
    "        y = df_test[\"delta_\"+subgroup]\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        y_hat = np.poly1d(z)(x)\n",
    "        ax[coord[0]][coord[1]].plot(x, y_hat, c=col, lw=1)\n",
    "        ax[coord[0]][coord[1]].set_title(subgroup)        \n",
    "        ax[coord[0]][coord[1]].set_xlabel(\"val\")\n",
    "        ax[coord[0]][coord[1]].set_ylabel(\"test\")\n",
    "    fig.suptitle(\"val vs test\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_all_val_vs_test(data_path, tasks_fname, features_path, feature_type, cache_path, metric, deltas=True):\n",
    "    with open(data_path+\"/\"+tasks_fname,\"r\") as fid:        \n",
    "        for i,l in enumerate(fid):            \n",
    "            task_abv, task_name = l.strip(\"\\n\").split(\",\")        \n",
    "            print(task_name+\"\\n\")\n",
    "            if deltas:\n",
    "                plot_val_vs_test_deltas(cache_path, task_abv, feature_type, metric)\n",
    "            else:\n",
    "                plot_val_vs_test(cache_path, task_abv, feature_type, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Hospital Mortality\n",
      "\n",
      "[train/test set size: 1000/1000]\n",
      "[running torch classifier]\n",
      "[reading cached features]\n",
      "skipped seed: 2201x2201\n",
      "skipped seed: 2201x9325\n",
      "skipped seed: 2201x1033\n",
      "skipped seed: 2201x4179\n",
      " > seed: 2201x1931\n",
      "[early stopping: 148 epochs]\n",
      " > seed: 2201x8117\n",
      "[early stopping: 115 epochs]\n",
      " > seed: 2201x7364\n",
      "[early stopping: 145 epochs]\n",
      " > seed: 2201x7737\n",
      "[early stopping: 109 epochs]\n",
      " > seed: 2201x6219\n",
      "[early stopping: 146 epochs]\n",
      " > seed: 2201x3439\n",
      "[early stopping: 153 epochs]\n",
      " > seed: 2201x1537\n",
      "[early stopping: 114 epochs]\n",
      " > seed: 2201x7993\n",
      "[early stopping: 104 epochs]\n",
      " > seed: 2201x464\n",
      "[early stopping: 111 epochs]\n",
      " > seed: 2201x6386\n",
      "[early stopping: 108 epochs]\n",
      " > seed: 2201x7090\n",
      "[early stopping: 104 epochs]\n",
      " > seed: 2201x9952\n",
      "[early stopping: 117 epochs]\n",
      " > seed: 2201x34\n",
      "[early stopping: 116 epochs]\n",
      " > seed: 2201x7297\n",
      "[early stopping: 119 epochs]\n",
      " > seed: 2201x4363\n",
      "[early stopping: 118 epochs]\n",
      " > seed: 2201x3748\n",
      "[early stopping: 145 epochs]\n",
      " > seed: 2201x9685\n",
      "[early stopping: 114 epochs]\n",
      " > seed: 2201x1674\n",
      "[early stopping: 112 epochs]\n",
      " > seed: 2201x5200\n",
      "[early stopping: 153 epochs]\n",
      " > seed: 2201x501\n",
      "[early stopping: 121 epochs]\n",
      " > seed: 2201x365\n",
      "[early stopping: 118 epochs]\n",
      "skipped seed: 9325x2201\n",
      "skipped seed: 9325x9325\n",
      "skipped seed: 9325x1033\n",
      "skipped seed: 9325x4179\n",
      " > seed: 9325x1931\n",
      "[early stopping: 224 epochs]\n",
      " > seed: 9325x8117\n",
      "[early stopping: 252 epochs]\n",
      " > seed: 9325x7364\n",
      "[early stopping: 238 epochs]\n",
      " > seed: 9325x7737\n",
      "[early stopping: 239 epochs]\n",
      " > seed: 9325x6219\n",
      "[early stopping: 272 epochs]\n",
      " > seed: 9325x3439\n",
      "[early stopping: 243 epochs]\n",
      " > seed: 9325x1537\n",
      "[early stopping: 230 epochs]\n",
      " > seed: 9325x7993\n",
      "[early stopping: 261 epochs]\n",
      " > seed: 9325x464\n",
      "[early stopping: 206 epochs]\n",
      " > seed: 9325x6386\n",
      "[early stopping: 230 epochs]\n",
      " > seed: 9325x7090\n",
      "[early stopping: 179 epochs]\n",
      " > seed: 9325x9952\n",
      "[early stopping: 249 epochs]\n",
      " > seed: 9325x34\n",
      "[early stopping: 233 epochs]\n",
      " > seed: 9325x7297\n",
      "[early stopping: 224 epochs]\n",
      " > seed: 9325x4363\n",
      "[early stopping: 220 epochs]\n",
      " > seed: 9325x3748\n",
      "[early stopping: 190 epochs]\n",
      " > seed: 9325x9685\n",
      "[early stopping: 168 epochs]\n",
      " > seed: 9325x1674\n",
      "[early stopping: 241 epochs]\n",
      " > seed: 9325x5200\n",
      "[early stopping: 199 epochs]\n",
      " > seed: 9325x501\n",
      "[early stopping: 186 epochs]\n",
      " > seed: 9325x365\n",
      "[early stopping: 191 epochs]\n",
      "skipped seed: 1033x2201\n",
      "skipped seed: 1033x9325\n",
      "skipped seed: 1033x1033\n",
      "skipped seed: 1033x4179\n",
      " > seed: 1033x1931\n",
      "[early stopping: 216 epochs]\n",
      " > seed: 1033x8117\n",
      "[early stopping: 237 epochs]\n",
      " > seed: 1033x7364\n",
      "[early stopping: 223 epochs]\n",
      " > seed: 1033x7737\n",
      "[early stopping: 232 epochs]\n",
      " > seed: 1033x6219\n",
      "[early stopping: 200 epochs]\n",
      " > seed: 1033x3439\n",
      "[early stopping: 252 epochs]\n",
      " > seed: 1033x1537\n",
      "[early stopping: 222 epochs]\n",
      " > seed: 1033x7993\n",
      "[early stopping: 235 epochs]\n",
      " > seed: 1033x464\n",
      "[early stopping: 233 epochs]\n",
      " > seed: 1033x6386\n",
      "[early stopping: 230 epochs]\n",
      " > seed: 1033x7090\n",
      "[early stopping: 224 epochs]\n",
      " > seed: 1033x9952\n",
      "[early stopping: 228 epochs]\n",
      " > seed: 1033x34\n",
      "[early stopping: 233 epochs]\n",
      " > seed: 1033x7297\n",
      "[early stopping: 252 epochs]\n",
      " > seed: 1033x4363\n",
      "[early stopping: 192 epochs]\n",
      " > seed: 1033x3748\n",
      "[early stopping: 239 epochs]\n",
      " > seed: 1033x9685\n",
      "[early stopping: 236 epochs]\n",
      " > seed: 1033x1674\n",
      "[early stopping: 253 epochs]\n",
      " > seed: 1033x5200\n",
      "[early stopping: 217 epochs]\n",
      " > seed: 1033x501\n",
      "[early stopping: 254 epochs]\n",
      " > seed: 1033x365\n",
      "[early stopping: 216 epochs]\n",
      "skipped seed: 4179x2201\n",
      "skipped seed: 4179x9325\n",
      "skipped seed: 4179x1033\n",
      "skipped seed: 4179x4179\n",
      " > seed: 4179x1931\n",
      "[early stopping: 212 epochs]\n",
      " > seed: 4179x8117\n",
      "[early stopping: 174 epochs]\n",
      " > seed: 4179x7364\n",
      "[early stopping: 195 epochs]\n",
      " > seed: 4179x7737\n",
      "[early stopping: 189 epochs]\n",
      " > seed: 4179x6219\n",
      "[early stopping: 188 epochs]\n",
      " > seed: 4179x3439\n",
      "[early stopping: 197 epochs]\n",
      " > seed: 4179x1537\n",
      "[early stopping: 196 epochs]\n",
      " > seed: 4179x7993\n",
      "[early stopping: 198 epochs]\n",
      " > seed: 4179x464\n",
      "[early stopping: 178 epochs]\n",
      " > seed: 4179x6386\n",
      "[early stopping: 191 epochs]\n",
      " > seed: 4179x7090\n",
      "[early stopping: 179 epochs]\n",
      " > seed: 4179x9952\n",
      "[early stopping: 186 epochs]\n",
      " > seed: 4179x34\n",
      "[early stopping: 192 epochs]\n",
      " > seed: 4179x7297\n",
      "[early stopping: 175 epochs]\n",
      " > seed: 4179x4363\n",
      "[early stopping: 204 epochs]\n",
      " > seed: 4179x3748\n",
      "[early stopping: 200 epochs]\n",
      " > seed: 4179x9685\n",
      "[early stopping: 199 epochs]\n",
      " > seed: 4179x1674\n",
      "[early stopping: 206 epochs]\n",
      " > seed: 4179x5200\n",
      "[early stopping: 185 epochs]\n",
      " > seed: 4179x501\n",
      "[early stopping: 191 epochs]\n",
      " > seed: 4179x365\n",
      "[early stopping: 169 epochs]\n",
      " > seed: 1931x2201\n",
      "[early stopping: 134 epochs]\n",
      " > seed: 1931x9325\n",
      "[early stopping: 124 epochs]\n",
      " > seed: 1931x1033\n",
      "[early stopping: 120 epochs]\n",
      " > seed: 1931x4179\n",
      "[early stopping: 127 epochs]\n",
      " > seed: 1931x1931\n",
      "[early stopping: 113 epochs]\n",
      " > seed: 1931x8117\n",
      "[early stopping: 147 epochs]\n",
      " > seed: 1931x7364\n",
      "[early stopping: 117 epochs]\n",
      " > seed: 1931x7737\n",
      "[early stopping: 116 epochs]\n",
      " > seed: 1931x6219\n",
      "[early stopping: 155 epochs]\n",
      " > seed: 1931x3439\n",
      "[early stopping: 155 epochs]\n",
      " > seed: 1931x1537\n",
      "[early stopping: 116 epochs]\n",
      " > seed: 1931x7993\n",
      "[early stopping: 125 epochs]\n",
      " > seed: 1931x464\n",
      "[early stopping: 140 epochs]\n",
      " > seed: 1931x6386\n",
      "[early stopping: 161 epochs]\n",
      " > seed: 1931x7090\n",
      "[early stopping: 115 epochs]\n",
      " > seed: 1931x9952\n",
      "[early stopping: 123 epochs]\n",
      " > seed: 1931x34\n",
      "[early stopping: 112 epochs]\n",
      " > seed: 1931x7297\n",
      "[early stopping: 125 epochs]\n",
      " > seed: 1931x4363\n",
      "[early stopping: 129 epochs]\n",
      " > seed: 1931x3748\n",
      "[early stopping: 125 epochs]\n",
      " > seed: 1931x9685\n",
      "[early stopping: 124 epochs]\n",
      " > seed: 1931x1674\n",
      "[early stopping: 127 epochs]\n",
      " > seed: 1931x5200\n",
      "[early stopping: 112 epochs]\n",
      " > seed: 1931x501\n",
      "[early stopping: 117 epochs]\n",
      " > seed: 1931x365\n",
      "[early stopping: 119 epochs]\n",
      " > seed: 8117x2201\n",
      "[early stopping: 152 epochs]\n",
      " > seed: 8117x9325\n",
      "[early stopping: 157 epochs]\n",
      " > seed: 8117x1033\n",
      "[early stopping: 150 epochs]\n",
      " > seed: 8117x4179\n",
      "[early stopping: 179 epochs]\n",
      " > seed: 8117x1931\n",
      "[early stopping: 173 epochs]\n",
      " > seed: 8117x8117\n",
      "[early stopping: 157 epochs]\n",
      " > seed: 8117x7364\n",
      "[early stopping: 173 epochs]\n",
      " > seed: 8117x7737\n",
      "[early stopping: 156 epochs]\n",
      " > seed: 8117x6219\n",
      "[early stopping: 160 epochs]\n",
      " > seed: 8117x3439\n",
      "[early stopping: 162 epochs]\n",
      " > seed: 8117x1537\n",
      "[early stopping: 155 epochs]\n",
      " > seed: 8117x7993\n",
      "[early stopping: 175 epochs]\n",
      " > seed: 8117x464\n",
      "[early stopping: 167 epochs]\n",
      " > seed: 8117x6386\n",
      "[early stopping: 147 epochs]\n",
      " > seed: 8117x7090\n",
      "[early stopping: 141 epochs]\n",
      " > seed: 8117x9952\n",
      "[early stopping: 162 epochs]\n",
      " > seed: 8117x34\n",
      "[early stopping: 161 epochs]\n",
      " > seed: 8117x7297\n",
      "[early stopping: 164 epochs]\n",
      " > seed: 8117x4363\n",
      "[early stopping: 139 epochs]\n",
      " > seed: 8117x3748\n",
      "[early stopping: 158 epochs]\n",
      " > seed: 8117x9685\n",
      "[early stopping: 159 epochs]\n",
      " > seed: 8117x1674\n",
      "[early stopping: 165 epochs]\n",
      " > seed: 8117x5200\n",
      "[early stopping: 173 epochs]\n",
      " > seed: 8117x501\n",
      "[early stopping: 157 epochs]\n",
      " > seed: 8117x365\n",
      "[early stopping: 155 epochs]\n",
      " > seed: 7364x2201\n",
      "[early stopping: 227 epochs]\n",
      " > seed: 7364x9325\n",
      "[early stopping: 186 epochs]\n",
      " > seed: 7364x1033\n",
      "[early stopping: 180 epochs]\n",
      " > seed: 7364x4179\n",
      "[early stopping: 208 epochs]\n",
      " > seed: 7364x1931\n",
      "[early stopping: 193 epochs]\n",
      " > seed: 7364x8117\n",
      "[early stopping: 218 epochs]\n",
      " > seed: 7364x7364\n",
      "[early stopping: 216 epochs]\n",
      " > seed: 7364x7737\n",
      "[early stopping: 198 epochs]\n",
      " > seed: 7364x6219\n",
      "[early stopping: 243 epochs]\n",
      " > seed: 7364x3439\n",
      "[early stopping: 234 epochs]\n",
      " > seed: 7364x1537\n",
      "[early stopping: 175 epochs]\n",
      " > seed: 7364x7993\n",
      "[early stopping: 173 epochs]\n",
      " > seed: 7364x464\n",
      "[early stopping: 225 epochs]\n",
      " > seed: 7364x6386\n",
      "[early stopping: 164 epochs]\n",
      " > seed: 7364x7090\n",
      "[early stopping: 236 epochs]\n",
      " > seed: 7364x9952\n",
      "[early stopping: 212 epochs]\n",
      " > seed: 7364x34\n",
      "[early stopping: 204 epochs]\n",
      " > seed: 7364x7297\n",
      "[early stopping: 201 epochs]\n",
      " > seed: 7364x4363\n",
      "[early stopping: 207 epochs]\n",
      " > seed: 7364x3748\n",
      "[early stopping: 175 epochs]\n",
      " > seed: 7364x9685\n",
      "[early stopping: 190 epochs]\n",
      " > seed: 7364x1674\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-814ffe01efb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mini-IHM\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# z = val_vs_test(INPUT_PATH, dataset, FEATURES_PATH, MODEL, TMP_PATH, METRIC, n_seeds=N_SEEDS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_val_vs_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tasks.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURES_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTMP_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMETRIC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# MODEL=\"BERT-POOL\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_all_val_vs_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tasks.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURES_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTMP_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMETRIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-53bd0626987c>\u001b[0m in \u001b[0;36mall_val_vs_test\u001b[0;34m(data_path, tasks_fname, features_path, feature_type, cache_path, metric, n_seeds)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mtask_abv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mval_vs_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mini-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtask_abv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-53bd0626987c>\u001b[0m in \u001b[0;36mval_vs_test\u001b[0;34m(data_path, dataset, features_path, feature_type, cache_path, metric, n_seeds)\u001b[0m\n\u001b[1;32m     56\u001b[0m                                     \u001b[0minput_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                                     \u001b[0minit_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                                     shuffle_seed=shuffle_seed)                                                                                \n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m#test each subgroup on test data (note thtat *all* is also a subgroup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1f89ca2a022d>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(X_train, Y_train, X_val, Y_val, init_seed, shuffle_seed, input_dimension)\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mshuffle_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     shuffle=True) \n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mCLASSIFIER\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mseq\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         x = core.models.MultiSeqLinearModel(in_dim=input_dimension, out_dim=1, \n",
      "\u001b[0;32m~/Dev/projects/TADAT/TADAT/tadat/core/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, Y_train, X_val, Y_val)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0my_hat_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;31m# print(y_hat_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/projects/TADAT/TADAT/tadat/core/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset=\"mini-IHM\"\n",
    "# z = val_vs_test(INPUT_PATH, dataset, FEATURES_PATH, MODEL, TMP_PATH, METRIC, n_seeds=N_SEEDS)\n",
    "all_val_vs_test(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, TMP_PATH, METRIC, n_seeds=25)\n",
    "# MODEL=\"BERT-POOL\"\n",
    "plot_all_val_vs_test(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, TMP_PATH, METRIC)\n",
    "# plot_all_val_vs_test(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, TMP_PATH, METRIC, deltas=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3vclNK44JIu"
   },
   "source": [
    "# Data Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "U-aKRf_04JIu"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def data_mapper(data_path, dataset, features_path, feature_type, cache_path, \n",
    "                shuffle_seed=1,init_seed=2, update_cache=False):\n",
    "    seed = \"{}x{}\".format(init_seed, shuffle_seed)\n",
    "    res_fname = cache_path+\"/datamap_{}_{}_{}.pkl\".format(dataset, feature_type, seed).lower()    \n",
    "    res = read_cache(res_fname)    \n",
    "    #read patients data\n",
    "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
    "                                sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])    \n",
    "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
    "    \n",
    "    if not res or update_cache:\n",
    "        df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
    "        print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
    "        train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
    "        train_idx, train_Y = train[\"all\"]\n",
    "        val_idx, val_Y = val[\"all\"]\n",
    "        #get a subset of the data only\n",
    "        max_instances = 12000\n",
    "        train_idx = train_idx[:max_instances]\n",
    "        train_Y = train_Y[:max_instances]\n",
    "        #slice the feature matrix to get the corresponding instances\n",
    "        train_X = feature_matrix[train_idx, :]    \n",
    "        val_X = feature_matrix[val_idx, :]    \n",
    "        input_dimension = train_X.shape[-1]        \n",
    "        model = core.models.LinearDataMaps(in_dim=input_dimension, out_dim=1, \n",
    "                            loss_fn=torch.nn.BCEWithLogitsLoss(), \n",
    "                            init_seed=init_seed, n_epochs=300, \n",
    "                            default_lr=0.1, batch_size=None, \n",
    "                            shuffle_seed=shuffle_seed, silent=True,\n",
    "                            shuffle=True) \n",
    "        tr_losses, val_losses, tr_preds = model.fit(train_X, train_Y, val_X, val_Y)\n",
    "        Y = np.array(train_Y).reshape(-1,1)\n",
    "        write_cache(res_fname, [train_idx, tr_preds, Y])\n",
    "    else:\n",
    "        train_idx, tr_preds, Y = res\n",
    "        print(\"found cached results\")\n",
    "    #get probability of correct class\n",
    "    threshold = 0.5\n",
    "    tr_probs = np.abs((1-Y)-tr_preds)\n",
    "    confidence = tr_probs.mean(axis=1) + (np.random.random(len(tr_probs)) * 0.01)\n",
    "    variability = tr_probs.std(axis=1) + (np.random.random(len(tr_probs)) * 0.01)\n",
    "    correctness = np.where(tr_probs >= threshold, 1, 0).mean(axis=1) + (np.random.random(len(tr_probs)) * 0.01)   \n",
    "    \n",
    "    fig2, ax2 = plt.subplots(1, 3,  figsize=(30,5))\n",
    "    paleta = \"BuRd\"\n",
    "    sns.scatterplot(ax=ax2[0], x=variability, y=confidence, hue=correctness, palette=paleta)\n",
    "    ax2[0].set_title(\"{} (seed: {})\".format(dataset, seed))    \n",
    "    train_X = feature_matrix[train_idx, :]    \n",
    "    pca = PCA(n_components=2)\n",
    "    train_z = pca.fit_transform(train_X)    \n",
    "    sns.scatterplot(ax=ax2[1], x=train_z[:,0], y=train_z[:,1], hue=confidence, palette=paleta)\n",
    "    sns.scatterplot(ax=ax2[2], x=train_z[:,0], y=train_z[:,1], hue=variability, palette=paleta)\n",
    "    ax2[1].set_title(\"confidence\")\n",
    "    ax2[2].set_title(\"variability\")    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return tr_preds, Y\n",
    "\n",
    "def datamap_tasks(data_path, tasks_fname, features_path, feature_type, cache_path, mini_tasks=True):\n",
    "    with open(data_path+tasks_fname,\"r\") as fid:        \n",
    "        for i,l in enumerate(fid):            \n",
    "            task_abv, task_name = l.strip(\"\\n\").split(\",\")\n",
    "            dataset = \"mini-\"+task_abv if mini_tasks else task_abv\n",
    "            data_mapper(data_path, dataset, features_path, feature_type, cache_path, update_cache=False)\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL=\"CLINICALBERT-POOL\"\n",
    "# dataset=\"CD\"\n",
    "# tr_preds, Y = data_mapper(INPUT_PATH, dataset, FEATURES_PATH, MODEL, TMP_PATH, update_cache=False)\n",
    "# datamap_tasks(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, TMP_PATH, mini_tasks=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIMIC_Outcomes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
