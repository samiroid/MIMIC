{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIMIC_Outcomes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8-final"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTNg0aoYxumN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')      \n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h40mg5Ocpz2",
        "tags": []
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUQ7MBldvx5d"
      },
      "source": [
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import fnmatch\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy.random import RandomState\n",
        "import os\n",
        "from pdb import set_trace\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pprint\n",
        "import random\n",
        "from sklearn.linear_model import SGDClassifier \n",
        "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, auc, precision_recall_curve\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import torch\n",
        "import uuid\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/collab/TADAT/\") \n",
        "#local\n",
        "from tadat.pipeline import plots\n",
        "import tadat.core as core\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set(style=\"whitegrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK8cWdcXmzju"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWBAvaw5zIe_"
      },
      "source": [
        "\n",
        "BASE_PATH = \"/content/drive/My Drive/collab/MIMIC/\"\n",
        "# BASE_PATH = \"/Users/samir/Dev/projects/MIMIC/MIMIC/\"\n",
        "INPUT_PATH = BASE_PATH+\"/DATA/input/\"\n",
        "FEATURES_PATH = BASE_PATH+\"/DATA/features/\"\n",
        "OUTPUT_PATH = BASE_PATH+\"/DATA/results/\"\n",
        "TMP_PATH = BASE_PATH+\"/DATA/processed/\"\n",
        "\n",
        "TUNE_OUTPUT_PATH = BASE_PATH+\"/DATA/results_fine/\"\n",
        "TUNE_TMP_PATH = BASE_PATH+\"/DATA/processed_fine/\"\n",
        "\n",
        "GRID_OUTPUT_PATH = BASE_PATH+\"/DATA/results_grid/\"\n",
        "GRID_TMP_PATH = BASE_PATH+\"/DATA/processed_grid/\"\n",
        "\n",
        "#configs\n",
        "N_SEEDS=4\n",
        "N_VAL_SEEDS = 5\n",
        "N_VAL_RUNS = 25\n",
        "N_TASKS = 3\n",
        "N_TASKS = 50\n",
        "# PLOT_VARS=[\"auroc\",\"auprc\",\"sensitivity\",\"specificity\"]\n",
        "PLOT_VARS=[\"auroc\",\"sensitivity\"]\n",
        "MODEL=\"BERT-POOL\"\n",
        "PLOT_VAR = \"auroc\"\n",
        "\n",
        "GROUPS = { \"GENDER\": [\"M\",\"F\"],   \n",
        "         \"ETHNICITY\": [\"WHITE\",\"BLACK\",\"ASIAN\",\"HISPANIC\"]\n",
        "}\n",
        "\n",
        "CLASSIFIER = 'sklearn'\n",
        "CLASSIFIER = 'torch'\n",
        "# CLASSIFIER = 'mseq'\n",
        "CLINICALBERT = \"emilyalsentzer/Bio_ClinicalBERT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GEYGvy7CzIn"
      },
      "source": [
        "\n",
        "SMALL_SIZE = 13\n",
        "MEDIUM_SIZE = 16\n",
        "BIGGER_SIZE = 16\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sygeVFaACzI8"
      },
      "source": [
        "# Modeling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5SjTOgye0pB"
      },
      "source": [
        "def train_classifier(X_train, Y_train, X_val, Y_val, \n",
        "                     init_seed, shuffle_seed=None, input_dimension=None):    \n",
        "    if CLASSIFIER == \"torch\":        \n",
        "        x = core.models.MyLinearModel(in_dim=input_dimension, out_dim=1, \n",
        "                    loss_fn=torch.nn.BCEWithLogitsLoss(), \n",
        "                    init_seed=init_seed, n_epochs=500, \n",
        "                    default_lr=0.1, batch_size=None, \n",
        "                    shuffle_seed=shuffle_seed, silent=True,\n",
        "                    shuffle=True) \n",
        "        x.fit(X_train, Y_train, X_val, Y_val)\n",
        "    elif CLASSIFIER == \"mseq\":        \n",
        "        x = core.models.MultiSeqLinearModel(in_dim=input_dimension, out_dim=1, \n",
        "                    loss_fn=torch.nn.BCELoss(), \n",
        "                    init_seed=init_seed, n_epochs=500, \n",
        "                    default_lr=0.1, batch_size=None, \n",
        "                    shuffle_seed=shuffle_seed, silent=True,\n",
        "                    shuffle=True) \n",
        "        x.fit(X_train, Y_train, X_val, Y_val)\n",
        "    elif CLASSIFIER == \"sklearn\":\n",
        "        x = SGDClassifier(loss=\"log\", random_state=shuffle_seed)\n",
        "        x.fit(X_train, Y_train)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return x\n",
        "\n",
        "def evaluate_classifier(model, X_test, Y_test,\n",
        "                   labels, model_name, random_seed, subgroup, res_path=None):\n",
        "    Y_hat = model.predict(X_test)\n",
        "    Y_hat_prob = model.predict_proba(X_test)\n",
        "    #get probabilities for the positive class\n",
        "    if CLASSIFIER == 'sklearn':\n",
        "        Y_hat_prob = Y_hat_prob[:,labels[1]]    \n",
        "    microF1 = f1_score(Y_test, Y_hat, average=\"micro\") \n",
        "    macroF1 = f1_score(Y_test, Y_hat, average=\"macro\") \n",
        "    try:\n",
        "        aurocc = roc_auc_score(Y_test, Y_hat_prob)\n",
        "    except ValueError:\n",
        "        aurocc = 0\n",
        "    try:\n",
        "        prec, rec, thresholds = precision_recall_curve(Y_test, Y_hat_prob)       \n",
        "        auprc = auc(rec, prec)\n",
        "    except ValueError:\n",
        "        auprc = 0\n",
        "    try:\n",
        "        tn, fp, fn, tp = confusion_matrix(Y_test, Y_hat).ravel()\n",
        "        specificity = tn / (tn+fp)\n",
        "        sensitivity = tp / (fn+tp)\n",
        "    except ValueError:\n",
        "        specificity, sensitivity = 0, 0\n",
        "    \n",
        "    res = {\"model\":model_name, \n",
        "            \"seed\":random_seed,  \n",
        "            \"group\":subgroup,    \n",
        "            \"microF1\":round(microF1,3),\n",
        "            \"macroF1\":round(macroF1,3),\n",
        "            \"auroc\":round(aurocc,3),\n",
        "            \"auprc\":round(auprc,3),\n",
        "            \"specificity\":round(specificity,3),\n",
        "            \"sensitivity\":round(sensitivity,3)           \n",
        "            }\n",
        "\n",
        "    if res_path is not None:    \n",
        "        core.helpers.save_results(res, res_path, sep=\"\\t\")\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "def vectorize(df_train, df_val, df_test, subject_ids):\n",
        "    #vectorize labels\n",
        "    train_Y = df_train[\"Y\"]\n",
        "    val_Y = df_val[\"Y\"]           \n",
        "    test_Y = df_test[\"Y\"]           \n",
        "    \n",
        "    label_vocab = core.vectorizer.get_labels_vocab(train_Y+val_Y)\n",
        "    train_Y,_ = core.vectorizer.label2idx(train_Y, label_vocab)\n",
        "    val_Y,_ = core.vectorizer.label2idx(val_Y, label_vocab)\n",
        "    test_Y,_ = core.vectorizer.label2idx(test_Y, label_vocab)\n",
        "    \n",
        "    \n",
        "    #get the subject id indices\n",
        "    train_idxs = [subject_ids.index(i) for i in list(df_train[\"SUBJECT_ID\"])] \n",
        "    val_idxs = [subject_ids.index(i) for i in list(df_val[\"SUBJECT_ID\"])] \n",
        "    test_idxs = [subject_ids.index(i) for i in list(df_test[\"SUBJECT_ID\"])] \n",
        "    \n",
        "    train = {}\n",
        "    test = {}\n",
        "    val = {}\n",
        "    train[\"all\"] = [train_idxs, train_Y]\n",
        "    test[\"all\"] = [test_idxs, test_Y]\n",
        "    val[\"all\"] = [val_idxs, val_Y]\n",
        "\n",
        "    for group in list(GROUPS.keys()):\n",
        "        #and subgroups\n",
        "        for subgroup in GROUPS[group]:                \n",
        "            df_train_sub = df_train[df_train[group] == subgroup]\n",
        "            df_test_sub = df_test[df_test[group] == subgroup]\n",
        "            df_val_sub = df_val[df_val[group] == subgroup]\n",
        "            print(\"[subgroup: {} | tr: {} | ts: {} | val: {}]\".format(subgroup, len(df_train_sub), len(df_test_sub), len(df_val_sub)))\n",
        "            #vectorize labels               \n",
        "            train_Y_sub,_ = core.vectorizer.label2idx(df_train_sub[\"Y\"], label_vocab)            \n",
        "            test_Y_sub,_ = core.vectorizer.label2idx(df_test_sub[\"Y\"], label_vocab)            \n",
        "            val_Y_sub,_ = core.vectorizer.label2idx(df_val_sub[\"Y\"], label_vocab)            \n",
        "\n",
        "            #get indices into the feature matrix\n",
        "            train_idxs_sub = [subject_ids.index(i) for i in list(df_train_sub[\"SUBJECT_ID\"])] \n",
        "            test_idxs_sub = [subject_ids.index(i) for i in list(df_test_sub[\"SUBJECT_ID\"])] \n",
        "            val_idxs_sub = [subject_ids.index(i) for i in list(df_val_sub[\"SUBJECT_ID\"])] \n",
        "            if subgroup == \"M\":\n",
        "                subgroup = \"men\"\n",
        "            elif subgroup == \"F\":\n",
        "                subgroup = \"women\"\n",
        "            train[subgroup.lower()] = [train_idxs_sub, train_Y_sub]\n",
        "            test[subgroup.lower()] = [test_idxs_sub, test_Y_sub]\n",
        "            val[subgroup.lower()] = [val_idxs_sub, val_Y_sub]\n",
        "\n",
        "    return train, val, test, label_vocab\n",
        "\n",
        "\n",
        "def get_features(data, vocab_size, feature_type, word_vectors=None):\n",
        "    if feature_type == \"BOW-BIN\":\n",
        "        X = core.features.BOW(data, vocab_size,sparse=True)\n",
        "    elif feature_type == \"BOW-FREQ\":\n",
        "        X = core.features.BOW_freq(data, vocab_size,sparse=True)\n",
        "    elif feature_type == \"BOE-BIN\":\n",
        "        X = core.features.BOE(data, word_vectors,\"bin\")\n",
        "    elif feature_type == \"BOE-SUM\": \n",
        "        X = core.features.BOE(data, word_vectors,\"sum\")\n",
        "    elif feature_type == \"BERT-POOL\":\n",
        "        X =  core.transformer_encoders.encode_sequences(data, batchsize=64)        \n",
        "    elif feature_type == \"BERT-CLS\":\n",
        "        X =  core.transformer_encoders.encode_sequences(data, cls_features=True,\n",
        "                                                        batchsize=64)            \n",
        "    elif feature_type == \"MULTI-BERT-POOL\":\n",
        "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, batchsize=32,\n",
        "                                                         tmp_path=TMP_PATH)\n",
        "    elif feature_type == \"MULTI-BERT-CLS\":\n",
        "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, \n",
        "                                                         cls_features=True,\n",
        "                                                         batchsize=32,\n",
        "                                                         tmp_path=TMP_PATH)\n",
        "    elif feature_type == \"CLINICALBERT-POOL\":\n",
        "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
        "        X =  core.transformer_encoders.encode_sequences(data, batchsize=64, tokenizer=tokenizer,\n",
        "                                                                    encoder=encoder)        \n",
        "    elif feature_type == \"CLINICALBERT-CLS\":\n",
        "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
        "        X =  core.transformer_encoders.encode_sequences(data, cls_features=True,batchsize=64,\n",
        "                                                                    tokenizer=tokenizer, encoder=encoder)        \n",
        "    elif feature_type == \"CLINICALMULTI-BERT-POOL\":\n",
        "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
        "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, batchsize=32,tmp_path=TMP_PATH,\n",
        "                                                              tokenizer=tokenizer, encoder=encoder)\n",
        "    elif feature_type == \"CLINICALMULTI-BERT-CLS\":\n",
        "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
        "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, cls_features=True, \n",
        "                                                                batchsize=32,tmp_path=TMP_PATH,\n",
        "                                                                tokenizer=tokenizer, encoder=encoder)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return X\n",
        "\n",
        "def extract_features(feature_type, path):\n",
        "    X = read_cache(path+\"feats_{}\".format(feature_type))\n",
        "    if X:\n",
        "        print(\"[reading cached features]\")\n",
        "        subject_ids, X_feats = X\n",
        "    else:\n",
        "        print(\"[computing {} features]\".format(feature_type))\n",
        "        df = pd.read_csv(path+\"patients.csv\", sep=\"\\t\", header=0)\n",
        "        subject_ids = list(df[\"SUBJECT_ID\"])\n",
        "        docs = list(df[\"TEXT\"])\n",
        "        if \"BERT\" in feature_type:\n",
        "            X_feats = get_features(docs, None, feature_type)\n",
        "        else:\n",
        "            X, word_vocab = core.vectorizer.docs2idx(docs)\n",
        "            X_feats = get_features(X,len(word_vocab),feature_type)\n",
        "        #save features\n",
        "        print(\"[saving features]\")\n",
        "        write_cache(path+\"feats_{}\".format(feature_type), \n",
        "                    [subject_ids, X_feats])\n",
        "    return subject_ids, X_feats\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCwwyC1RCzJC"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v8lR5X0cp0i"
      },
      "source": [
        "def read_dataset(path, dataset_name, df_patients):    \n",
        "    df_train = pd.read_csv(\"{}/{}_train.csv\".format(path, dataset_name), \n",
        "                           sep=\"\\t\", header=0)\n",
        "    df_test  = pd.read_csv(\"{}/{}_test.csv\".format(path, dataset_name),\n",
        "                           sep=\"\\t\", header=0)\n",
        "    df_val   = pd.read_csv(\"{}/{}_val.csv\".format(path, dataset_name),\n",
        "                           sep=\"\\t\", header=0)\n",
        "    #set indices\n",
        "    df_patients.set_index(\"SUBJECT_ID\", inplace=True)\n",
        "    df_train.set_index(\"SUBJECT_ID\", inplace=True)\n",
        "    df_test.set_index(\"SUBJECT_ID\", inplace=True)\n",
        "    df_val.set_index(\"SUBJECT_ID\", inplace=True)\n",
        "\n",
        "    df_train = df_train.join(df_patients, on=\"SUBJECT_ID\", \n",
        "                             how=\"inner\", lsuffix=\"N_\").reset_index()\n",
        "    df_test = df_test.join(df_patients, on=\"SUBJECT_ID\", \n",
        "                           how=\"inner\", lsuffix=\"N_\").reset_index()\n",
        "    df_val = df_val.join(df_patients, on=\"SUBJECT_ID\", \n",
        "                         how=\"inner\", lsuffix=\"N_\").reset_index()\n",
        "\n",
        "    return df_train, df_test, df_val   \n",
        "\n",
        "\n",
        "def read_cache(path):\n",
        "    X = None\n",
        "    try:\n",
        "        with open(path, \"rb\") as fi:            \n",
        "            X = pickle.load(fi)\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "    return X\n",
        "\n",
        "def write_cache(path, o):\n",
        "    dirname = os.path.dirname(path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    with open(path, \"wb\") as fo:\n",
        "        pickle.dump(o, fo)\n",
        "        \n",
        "def clear_cache(cache_path, model=\"*\", dataset=\"*\", group=\"*\", ctype=\"*\"):\n",
        "    assert ctype in [\"*\",\"res*\",\"feats\"]\n",
        "    file_paths = os.listdir(cache_path)\n",
        "    pattern = \"{}_{}_{}_*_{}.pkl\".format(dataset, model, group, ctype).lower()\n",
        "    for fname in file_paths:\n",
        "        if fnmatch.fnmatch(fname, pattern):\n",
        "            os.remove(cache_path+\"/\"+fname)\n",
        "            print(\"cleared file: {}\".format(fname))      \n",
        "\n",
        "def run(data_path, dataset, features_path, feature_type, cache_path, metric, n_seeds=N_SEEDS):\n",
        "    #read patients data\n",
        "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
        "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
        "\n",
        "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
        "    \n",
        "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
        "    print(\"[running 2 {} classifier]\".format(CLASSIFIER))\n",
        "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
        "    train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
        "    train_idx, train_Y = train[\"all\"]\n",
        "    val_idx, val_Y = val[\"all\"]\n",
        "    #slice the feature matrix to get the corresponding instances\n",
        "    train_X = feature_matrix[train_idx, :]    \n",
        "    val_X = feature_matrix[val_idx, :]    \n",
        "    \n",
        "    dirname = os.path.dirname(cache_path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    res_fname = cache_path+\"/seeds_{}_{}_{}.pkl\".format(dataset, feature_type, metric).lower()    \n",
        "    try:\n",
        "        df_results = pd.read_csv(res_fname)\n",
        "    except FileNotFoundError:\n",
        "        df_results = pd.DataFrame(columns = [\"seed\"] +  list(val.keys()))\n",
        "        df_results.to_csv(res_fname, index=False, header=True)        \n",
        "    groups = list(val.keys())\n",
        "    skip_seeds = set(df_results[\"seed\"])\n",
        "\n",
        "    random.seed(1) #ensure repeateable runs \n",
        "    random_seeds = random.sample(range(0, 10000), n_seeds)        \n",
        "    incremental_results = {}     \n",
        "    ##train/test classifier for each random seed pair\n",
        "    for init_seed, shuffle_seed in itertools.product(random_seeds,repeat=2):        \n",
        "        seed = \"{}x{}\".format(init_seed, shuffle_seed)          \n",
        "        if seed in skip_seeds:\n",
        "            print(\"skipped seed: {}\".format(seed))\n",
        "            continue\n",
        "        curr_results = {\"seed\":seed}\n",
        "        print(\" > seed: {}\".format(seed))                        \n",
        "        model = train_classifier(train_X, train_Y,val_X, val_Y,  \n",
        "                                    input_dimension=train_X.shape[-1],\n",
        "                                    init_seed=init_seed, \n",
        "                                    shuffle_seed=shuffle_seed)                                                                                \n",
        "        #test each subgroup (note thtat *all* is also a subgroup)\n",
        "        for subgroup in groups:                                \n",
        "            test_idx_sub, test_Y_sub = test[subgroup]                 \n",
        "            test_X_sub = feature_matrix[test_idx_sub, :]                \n",
        "            res_sub = evaluate_classifier(model, test_X_sub, test_Y_sub, \n",
        "                                        label_vocab, feature_type, seed, subgroup)                \n",
        "            curr_results[subgroup]= res_sub[metric]     \n",
        "\n",
        "        df_results = df_results.append(curr_results, ignore_index=True)\n",
        "        df_results.to_csv(res_fname, index=False, header=True)\n",
        "\n",
        "    return df_results\n",
        "\n",
        "def grid_search(data_path, dataset, features_path, feature_type, cache_path, tune_metric, n_runs=300):\n",
        "    #read patients data\n",
        "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
        "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
        "\n",
        "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)    \n",
        "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
        "    print(\"[grid searching with {} classifier]\".format(CLASSIFIER))\n",
        "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
        "    train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
        "    train_idx, train_Y = train[\"all\"]\n",
        "    val_idx, val_Y = val[\"all\"]\n",
        "    #slice the feature matrix to get the corresponding instances\n",
        "    train_X = feature_matrix[train_idx, :]    \n",
        "    val_X = feature_matrix[val_idx, :]    \n",
        "\n",
        "    init_randomizer = RandomState(1)\n",
        "    shuffle_randomizer = RandomState(2)    \n",
        "    results = []\n",
        "    dirname = os.path.dirname(cache_path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    res_fname = cache_path+\"/grid_{}_{}_{}.pkl\".format(dataset, feature_type, tune_metric).lower()    \n",
        "    try:\n",
        "        df_results = pd.read_csv(res_fname)\n",
        "    except FileNotFoundError:\n",
        "        df_results = pd.DataFrame(columns = [\"seed\"] +  list(val.keys()))\n",
        "        df_results.to_csv(res_fname, index=False, header=True)        \n",
        "    groups = list(val.keys())\n",
        "    skip_seeds = set(df_results[\"seed\"])\n",
        "    ##train/test classifier for each random seed pair\n",
        "    for j in range(n_runs):         \n",
        "        init_seed = init_randomizer.randint(10000)\n",
        "        shuffle_seed = shuffle_randomizer.randint(10000)\n",
        "        seed = \"{}x{}\".format(init_seed, shuffle_seed)        \n",
        "        if seed in skip_seeds:\n",
        "            print(\"skipped seed: {}\".format(seed))\n",
        "            continue\n",
        "        curr_results = {\"seed\":seed}\n",
        "        print(\" > seed: {}\".format(seed))                        \n",
        "        model = train_classifier(train_X, train_Y,val_X, val_Y,  \n",
        "                                    input_dimension=train_X.shape[-1],\n",
        "                                    init_seed=init_seed, \n",
        "                                    shuffle_seed=shuffle_seed)                                                                \n",
        "        ####### VALIDATION ########\n",
        "        \n",
        "        #test each subgroup (note thtat *all* is also a subgroup)\n",
        "        for subgroup in groups:                                \n",
        "            val_idx_sub, val_Y_sub = val[subgroup]                 \n",
        "            val_X_sub = feature_matrix[val_idx_sub, :]                \n",
        "            res_sub = evaluate_classifier(model, val_X_sub, val_Y_sub, \n",
        "                                        label_vocab, feature_type, seed, subgroup)                \n",
        "            curr_results[subgroup]= res_sub[tune_metric]          \n",
        "\n",
        "        df_results = df_results.append(curr_results, ignore_index=True)\n",
        "        df_results.to_csv(res_fname, index=False, header=True)    \n",
        "        \n",
        "    #return the best seeds    \n",
        "    return get_best_seeds(df_results, groups)\n",
        "\n",
        "def get_best_seeds(df_grid, groups, k=20):    \n",
        "\n",
        "    groups.remove(\"all\")    \n",
        "    for g in groups:\n",
        "        df_grid[\"delta_\"+g] = (df_grid[\"all\"] - df_grid[g]).abs()    \n",
        "    \n",
        "    df_grid[\"perf_avg\"] = df_grid[[g for g in groups]].mean(axis=1)\n",
        "    df_grid[\"perf_std\"] = df_grid[[g for g in groups]].std(axis=1)\n",
        "    df_grid[\"perf_avg_std\"] = df_grid[\"perf_avg\"] - df_grid[\"perf_std\"]\n",
        "    df_grid[\"delta_avg\"] = df_grid[[\"delta_\"+g for g in groups]].mean(axis=1)\n",
        "    df_grid[\"perf_avg_delta\"] = df_grid[\"perf_avg\"] - df_grid[\"delta_avg\"]\n",
        "    df_grid[\"all_avg_delta\"] = df_grid[\"all\"] - df_grid[\"delta_avg\"]\n",
        "\n",
        "    z = int(len(df_grid)/k)+1\n",
        "    res = []\n",
        "    for i in range(z):\n",
        "        step = (i+1)*k\n",
        "        df = df_grid.iloc[:step,:]      \n",
        "        #get the seed with the minimum delta\n",
        "        best_df = df.iloc[df[\"delta_avg\"].idxmin()]\n",
        "        seed = best_df[\"seed\"]\n",
        "        perf = best_df[\"delta_avg\"]\n",
        "        res.append({\"seed\":seed, \"criterion\":\"delta_avg\", \"k\":step, \"val\":perf })        \n",
        "        #get seeds with max criteria       \n",
        "        for crit in [\"all\", \"perf_avg\", \"perf_avg_std\", \"perf_avg_delta\",\n",
        "                     \"all_avg_delta\"]:        \n",
        "            best_df = df.iloc[df[crit].idxmax()]\n",
        "            seed = best_df[\"seed\"]\n",
        "            perf = best_df[crit]\n",
        "            res.append({\"seed\":seed, \"criterion\":crit, \"k\":step, \"val\":perf })        \n",
        "        df_best_seeds = pd.DataFrame(res)       \n",
        "        \n",
        "    return df_best_seeds\n",
        "\n",
        "def test_seeds(data_path, dataset, features_path, feature_type, cache_path, metric, best_seeds):\n",
        "    #read patients data\n",
        "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
        "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
        "\n",
        "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)    \n",
        "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
        "    print(\"[testing seeds with {} classifier]\".format(CLASSIFIER))\n",
        "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
        "    train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
        "    train_idx, train_Y = train[\"all\"]\n",
        "    val_idx, val_Y = val[\"all\"]\n",
        "    #slice the feature matrix to get the corresponding instances\n",
        "    train_X = feature_matrix[train_idx, :]    \n",
        "    val_X = feature_matrix[val_idx, :]    \n",
        "    groups = list(val.keys())\n",
        "    dirname = os.path.dirname(cache_path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    res_fname = cache_path+\"/test_seeds_{}_{}_{}.pkl\".format(dataset, feature_type, metric).lower()                    \n",
        "    try:\n",
        "        df_results = pd.read_csv(res_fname)\n",
        "    except FileNotFoundError:\n",
        "        cols = [\"seed\"] +  groups + [\"delta_\"+g for g in groups]\n",
        "        df_results = pd.DataFrame(columns = cols)    \n",
        "    skip_seeds = set(df_results[\"seed\"])    \n",
        "    for seed in set(best_seeds[\"seed\"]):\n",
        "        if seed in skip_seeds:\n",
        "            print(\"skipped seed: {}\".format(seed))\n",
        "            continue \n",
        "        curr_results = {\"seed\":seed}\n",
        "        print(\" > seed: {}\".format(seed))                        \n",
        "        init_seed, shuffle_seed = seed.split(\"x\") \n",
        "        model = train_classifier(train_X, train_Y,val_X, val_Y,  \n",
        "                                    input_dimension=train_X.shape[-1],\n",
        "                                    init_seed=int(init_seed), \n",
        "                                    shuffle_seed=int(shuffle_seed))\n",
        "        ####### TEST ########\n",
        "        \n",
        "        #test each subgroup (note thtat *all* is also a subgroup)\n",
        "        for subgroup in test.keys():                                \n",
        "            test_idx_sub, test_Y_sub = test[subgroup]                 \n",
        "            test_X_sub = feature_matrix[test_idx_sub, :]                \n",
        "            res_sub = evaluate_classifier(model, test_X_sub, test_Y_sub, \n",
        "                                        label_vocab, feature_type, seed, subgroup)                \n",
        "            curr_results[subgroup]= res_sub[metric]          \n",
        "\n",
        "        df_results = df_results.append(curr_results, ignore_index=True)\n",
        "    \n",
        "    for g in groups:\n",
        "        df_results[\"delta_\"+g] = (df_results[\"all\"] - df_results[g]).abs()    \n",
        "        \n",
        "    df_results[\"avg_delta_test\"] = df_results[[\"delta_\"+g for g in groups]].mean(axis=1)\n",
        "    df_results.to_csv(res_fname,index=False, header=True)\n",
        "    \n",
        "    return df_results\n",
        "\n",
        "def subsample_run(data_path, dataset, features_path, feature_type, cache_path, n_draws=10):\n",
        "    #read patients data\n",
        "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
        "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
        "\n",
        "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
        "    \n",
        "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
        "    print(\"[{} classifier]\".format(CLASSIFIER))\n",
        "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
        "    train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
        "    train_idx, train_Y = train[\"all\"]\n",
        "    val_idx, val_Y = val[\"all\"]\n",
        "    #slice the feature matrix to get the corresponding instances\n",
        "    train_X = feature_matrix[train_idx, :]    \n",
        "    val_X = feature_matrix[val_idx, :]    \n",
        "    random.seed(1) #ensure repeateable runs \n",
        "    random_seeds = random.sample(range(0, 10000), N_SEEDS)        \n",
        "    incremental_results = {}     \n",
        "    sample_size = min([len(test[subgroup][0]) for subgroup in test.keys()])\n",
        "    print(sample_size)\n",
        "    ##train/test classifier for each random seed pair    \n",
        "    for init_seed, shuffle_seed in itertools.product(random_seeds,repeat=2):        \n",
        "        seed = \"{}x{}\".format(init_seed, shuffle_seed)\n",
        "        res_fname = \"{}_{}_res{}.pkl\".format(dataset, feature_type, seed).lower()                        \n",
        "        curr_results = {}\n",
        "        print(\" > seed: {}\".format(seed))                        \n",
        "        model = train_classifier(train_X, train_Y,val_X, val_Y,  \n",
        "                                    input_dimension=train_X.shape[-1],\n",
        "                                    init_seed=init_seed, \n",
        "                                    shuffle_seed=shuffle_seed)                                                      \n",
        "        for i in range(n_draws):\n",
        "            #evaluate different random samples of the data\n",
        "            #test each subgroup (note thtat *all* is also a subgroup)            \n",
        "            for subgroup in test.keys():                                \n",
        "                test_idx_sub, test_Y_sub = test[subgroup]            \n",
        "                test_Y_sub = np.array(test_Y_sub)\n",
        "                test_idx_sub = np.array(test_idx_sub)                    \n",
        "                random_sample = random.sample(range(len(test_idx_sub)), sample_size)                \n",
        "                test_Y_sub_sample = test_Y_sub[random_sample]\n",
        "                test_idx_sub_sample = test_idx_sub[random_sample]                    \n",
        "                test_X_sub_sample = feature_matrix[test_idx_sub_sample, :]                \n",
        "                res_sub = evaluate_classifier(model, test_X_sub_sample, test_Y_sub_sample, \n",
        "                                            label_vocab, feature_type, seed+\"x\"+str(i), subgroup)                \n",
        "                curr_results[subgroup]= res_sub                       \n",
        "        \n",
        "            incremental_results = merge_results(curr_results, incremental_results, \n",
        "                                            list(test.keys()))\n",
        "    #build dataframes \n",
        "    df_results = results_to_df(incremental_results, test.keys())\n",
        "    return df_results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTyzgoy4vx64"
      },
      "source": [
        "# Analyses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVpc3ZQ-jsaV"
      },
      "source": [
        "def run_analyses(data_path, dataset, features_path, feature_type, results_path, \n",
        "                 cache_path, metric, tuning, clear_results=False):    \n",
        "\n",
        "    if not os.path.exists(results_path): os.makedirs(results_path)  \n",
        "    if not os.path.exists(cache_path): os.makedirs(cache_path)  \n",
        "    if clear_results:\n",
        "        clear_cache(cache_path, model=feature_type, dataset=dataset, ctype=\"res*\")\n",
        "    if tuning: \n",
        "        best_seeds = grid_search(data_path, dataset, features_path, feature_type, cache_path, metric)  \n",
        "        df_results = test_seeds(data_path, dataset, features_path, feature_type, cache_path, metric, best_seeds) \n",
        "        #join the results\n",
        "        best_seeds = best_seeds.set_index(\"seed\")\n",
        "        df_results = df_results.set_index(\"seed\")\n",
        "        df_results = best_seeds.join(df_results, how=\"inner\", on=[\"seed\"])\n",
        "        df_results = df_results.sort_values(by=[\"criterion\",\"k\"])\n",
        "        fname = \"{}_{}_{}_tuned.csv\".format(dataset, feature_type, metric).lower()            \n",
        "    else:\n",
        "        df_results = run(data_path, dataset, features_path, feature_type, cache_path, metric)         \n",
        "        fname = \"{}_{}_{}.csv\".format(dataset, feature_type, metric).lower()\n",
        "    \n",
        "    df_results.to_csv(results_path+fname, index=False, header=True)\n",
        "    return df_results               \n",
        "\n",
        "#Run All the tasks\n",
        "def run_tasks(data_path, tasks_fname, features_path, feature_type, results_path, cache_path,  \n",
        "             metric, tuning=False, reset=False, mini_tasks=True):\n",
        "    #if reset delete the completed tasks file\n",
        "    if reset: reset_tasks(cache_path)    \n",
        "    with open(data_path+tasks_fname,\"r\") as fid:\n",
        "        for i,l in enumerate(fid):\n",
        "            if i > N_TASKS: break\n",
        "            fname, task_name = l.strip(\"\\n\").split(\",\")            \n",
        "            dataset = \"mini-\"+fname if mini_tasks else fname\n",
        "            # dataset = fname\n",
        "            if is_task_done(cache_path, dataset): \n",
        "                print(\"[dataset: {} already processed]\".format(dataset))\n",
        "                continue                        \n",
        "            print(\"******** {} {} ********\".format(task_name, dataset))      \n",
        "            run_analyses(data_path, dataset, features_path, feature_type, results_path, \n",
        "                         cache_path, metric=metric, tuning=tuning, clear_results=False)\n",
        "            task_done(cache_path, dataset)\n",
        "\n",
        "def task_done(path,  task):\n",
        "    dirname = os.path.dirname(path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    with open(path+\"completed_tasks.txt\", \"a\") as fod:\n",
        "        fod.write(task+\"\\n\")\n",
        "\n",
        "def reset_tasks(path):\n",
        "    dirname = os.path.dirname(path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    with open(path+\"completed_tasks.txt\", \"w\") as fod:\n",
        "        fod.write(\"\")\n",
        "\n",
        "def is_task_done(path,  task):\n",
        "    try:\n",
        "        with open(path+\"completed_tasks.txt\", \"r\") as fid:\n",
        "            tasks = fid.read().split(\"\\n\")            \n",
        "        return task in set(tasks)\n",
        "    except FileNotFoundError:\n",
        "        #create file if not found\n",
        "        dirname = os.path.dirname(path)\n",
        "        if not os.path.exists(dirname):\n",
        "            os.makedirs(dirname)\n",
        "        with open(path+\"completed_tasks.txt\", \"w\") as fid:\n",
        "            fid.write(\"\")\n",
        "        return False\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-jPDUVwDAv1"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxk0TBDYC_mC"
      },
      "source": [
        "def plot_densities(df, ax, title):\n",
        "    ax.set_title(title)\n",
        "    for y in PLOT_VARS:        \n",
        "        try:\n",
        "            df.plot.kde(ax=ax, x=\"seed\", y=y)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "def plot_analyses(results_path, dataset, task_name, feature_type, metric):\n",
        "\n",
        "    fname = \"{}_{}_{}.csv\".format(dataset, feature_type, metric).lower()       \n",
        "    try:\n",
        "        df_results = pd.read_csv(results_path+fname)            \n",
        "        plot_minority_gaps(df_results, task_name)\n",
        "        plot_minority_scatters(df_results, task_name)        \n",
        "    except FileNotFoundError:\n",
        "        print(\"{} not found...\".format(fname))        \n",
        "\n",
        "\n",
        "def plot_tasks(tasks_fname, feature_type, results_path, metric, mini_tasks=True):\n",
        "    with open(tasks_fname,\"r\") as fid:        \n",
        "        for i,l in enumerate(fid):            \n",
        "            task_abv, task_name = l.strip(\"\\n\").split(\",\")\n",
        "            dataset = \"mini-\"+task_abv if mini_tasks else task_abv\n",
        "            plot_analyses(results_path, dataset, task_name, feature_type, metric)\n",
        "\n",
        "def get_gaps(results, dataset_name, metric):    \n",
        "    all_dfs = [results[g][\"results\"][[\"seed\", metric]] for g in results.keys()]\n",
        "    all_dfs = pd.concat(all_dfs)\n",
        "    diff_df = (all_dfs.groupby(\"seed\").max() - all_dfs.groupby(\"seed\").min()).reset_index()\n",
        "    diff_df[\"dataset\"] = [dataset_name]*len(diff_df)\n",
        "    return diff_df\n",
        "\n",
        "def plot_gaps(tasks_fname, feature_type, results_path, \n",
        "               mini_tasks=True, plot_metric=None, tune_metric=None):\n",
        "    dfs = []\n",
        "    with open(tasks_fname,\"r\") as fid:        \n",
        "        for i,l in enumerate(fid):            \n",
        "            task_abv, task_name = l.strip(\"\\n\").split(\",\")\n",
        "            dataset = \"mini-\"+task_abv if mini_tasks else task_abv\n",
        "            if tune_metric:\n",
        "                fname = \"{}_{}_all_tuned_res.pkl\".format(dataset, feature_type).lower()\n",
        "            else:        \n",
        "                fname = \"{}_{}_all_res.pkl\".format(dataset, feature_type).lower()\n",
        "\n",
        "            df_results = read_cache(results_path+fname)    \n",
        "\n",
        "            if df_results:\n",
        "                df_gaps = get_gaps(df_results, task_abv, plot_metric)\n",
        "                dfs.append(df_gaps)\n",
        "    dfs = pd.concat(dfs)\n",
        "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "    g = sns.catplot(x=plot_metric,y=\"dataset\", data=dfs, sharey=True,legend=True, \n",
        "                    legend_out=True, height=6, aspect=0.85,palette=cmap)        \n",
        "    g.set_xlabels(plot_metric + \" gap\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()  \n",
        "    \n",
        "def plot_performances(tasks_fname, feature_type, results_path, \n",
        "               mini_tasks=True, plot_metric=None, tune_metric=None):\n",
        "    dfs = []\n",
        "    with open(tasks_fname,\"r\") as fid:        \n",
        "        for i,l in enumerate(fid):            \n",
        "            task_abv, task_name = l.strip(\"\\n\").split(\",\")\n",
        "            dataset = \"mini-\"+task_abv if mini_tasks else task_abv\n",
        "            if tune_metric:\n",
        "                fname = \"{}_{}_all_tuned_res.pkl\".format(dataset, feature_type).lower()\n",
        "            else:        \n",
        "                fname = \"{}_{}_all_res.pkl\".format(dataset, feature_type).lower()\n",
        "\n",
        "            df_results = read_cache(results_path+fname)    \n",
        "            if df_results:\n",
        "                df = df_results[\"all\"][\"results\"]\n",
        "                df[\"dataset\"] = [task_abv]*len(df)\n",
        "                dfs.append(df)\n",
        "    dfs = pd.concat(dfs)\n",
        "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "    g = sns.catplot(x=plot_metric,y=\"dataset\", data=dfs, sharey=True,legend=True, \n",
        "                    legend_out=True, height=6, aspect=0.85,palette=cmap)        \n",
        "#     g.set_xlabels(plot_metric + \" gap\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()  \n",
        "\n",
        "def plot_gridsearch(cache_path, dataset, feature_type, tune_metric, title, subgroups=False, delta=False):    \n",
        "    res_fname = cache_path+\"{}_{}_{}_all_tuned.csv\".format(dataset, feature_type, tune_metric).lower()              \n",
        "    df_results = pd.read_csv(res_fname)  \n",
        "    df_all = df_results[df_results[\"criterion\"] == \"all\"]    \n",
        "    if delta:\n",
        "        last_all = df_all.iloc[df_all[\"k\"].idxmax()][\"avg_delta_test\"]\n",
        "        best_all = df_all.iloc[df_all[\"avg_delta_test\"].idxmin()][\"avg_delta_test\"]    \n",
        "    else:\n",
        "        last_all = df_all.iloc[df_all[\"k\"].idxmax()][\"all\"]    \n",
        "        best_all = df_all.iloc[df_all[\"all\"].idxmax()][\"all\"]    \n",
        "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "#     criteria = [\"all\", \"perf_avg\", \"perf_avg_std\", \"delta_avg\", \"perf_avg_delta\"]\n",
        "    criteria = [\"all\",\"perf_avg\", \"all_avg_delta\", \"perf_avg_delta\"]\n",
        "    crit_labels = [\"performance\",\"subgroup avg\", \"performance - avg subgroup delta\", \"subgroup avg - avg subgroup delta\"]\n",
        "#     set_trace()\n",
        "    groups = [\"all\", \"men\", \"women\", \"white\", \"black\",\"asian\", \"hispanic\"]\n",
        "    fig, ax = plt.subplots(1, len(criteria),  figsize=(25,8), sharex=True, sharey=True)    \n",
        "    if delta:\n",
        "        groups = [\"delta_\" + l for l in groups]\n",
        "    for i in range(len(criteria)):\n",
        "        df = df_results[df_results[\"criterion\"] == criteria[i]]    \n",
        "        ax[i].set_title(crit_labels[i])\n",
        "        if delta:\n",
        "            ax[i].axhline(best_all, linewidth=3, c=\"gray\", linestyle=\"--\")\n",
        "            ax[i].axhline(last_all, linewidth=2, c=\"gray\", linestyle=\"--\")\n",
        "            df.plot(x=\"k\", y=\"avg_delta_test\",   linewidth=4, ax=ax[i], c=\"gray\")    \n",
        "        else:\n",
        "            ax[i].axhline(best_all, linewidth=3, c=cmap[0], linestyle=\"--\")\n",
        "            ax[i].axhline(last_all, linewidth=2, c=cmap[0], linestyle=\"--\")\n",
        "            df.plot(x=\"k\", y=\"all\",   linewidth=4, ax=ax[i])    \n",
        "        #plot indiv\n",
        "        if subgroups:\n",
        "            for g in groups[1:]:\n",
        "                df.plot(x=\"k\", y=g, ax=ax[i])\n",
        "            \n",
        "        #place legend on the last subplot\n",
        "        if i == len(criteria)-1: \n",
        "            ax[i].legend(bbox_to_anchor=(1.05, 0), loc='lower left', borderaxespad=0.)\n",
        "        else:\n",
        "            ax[i].get_legend().remove()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "        \n",
        "def plot_gridsearches(tasks_fname, results_path, feature_type, \n",
        "               mini_tasks=True, metric=None, subgroups=False):\n",
        "    with open(tasks_fname,\"r\") as fid:        \n",
        "        for i,l in enumerate(fid):     \n",
        "            fname, task_name = l.strip(\"\\n\").split(\",\")\n",
        "            dataset = \"mini-\"+fname if mini_tasks else fname\n",
        "            print(dataset)\n",
        "            try:\n",
        "                plot_gridsearch(results_path, dataset, feature_type, metric, task_name, subgroups)\n",
        "                plot_gridsearch(results_path, dataset, feature_type, metric, task_name, subgroups, delta=True)\n",
        "            except FileNotFoundError:\n",
        "                print(\"Dataset {} not found\".format(dataset))\n",
        "\n",
        "def plot_minority_gaps(results, title):    \n",
        "    subgroups = [\"women\",\"black\",\"asian\",\"hispanic\"]\n",
        "    df_minorities = get_minority_gaps(results)\n",
        "    df_delta_long = pd.melt(df_minorities, id_vars=[\"seed\"], value_vars=subgroups, \n",
        "                                                      value_name=\"gap\", var_name=\"group\")\n",
        "\n",
        "    g = sns.catplot(x=\"group\", y=\"gap\", data=df_delta_long, sharey=True,legend=False)       \n",
        "    \n",
        "    for ax in g.axes[0]:\n",
        "        ax.axhline(0, ls='--',c=\"r\")\n",
        "    plt.suptitle(title, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()  \n",
        "    \n",
        "\n",
        "def plot_minority_scatters(results, title):\n",
        "    n_rows=2\n",
        "    n_cols = 2    \n",
        "    fig, ax = plt.subplots(n_rows, n_cols,  figsize=(12,5), sharex=True, sharey=True)\n",
        "    #current coloramap\n",
        "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "    coords = list(itertools.product(range(n_rows),range(n_cols)))   \n",
        "    results = get_minority_gaps(results)\n",
        "    subgroups = [\"women\",\"black\",\"asian\",\"hispanic\"]\n",
        "    for subgroup, col, coord in zip(subgroups, cmap, coords ):        \n",
        "        results[subgroup] = results[subgroup].abs()         \n",
        "        results.plot.scatter(x=\"all\",y=subgroup,\n",
        "                            color=col, ax=ax[coord[0]][coord[1]])\n",
        "        x = results[\"all\"]\n",
        "        y = results[subgroup]\n",
        "        z = np.polyfit(x, y, 1)\n",
        "        y_hat = np.poly1d(z)(x)\n",
        "        ax[coord[0]][coord[1]].plot(x, y_hat, c=col, lw=1)\n",
        "        ax[coord[0]][coord[1]].set_title(subgroup)\n",
        "        ax[coord[0]][coord[1]].set_ylabel(\"gap\")\n",
        "    fig.suptitle(title, y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def get_minority_gaps(results):\n",
        "    df = pd.DataFrame()\n",
        "    df[\"seed\"] = results[\"seed\"]\n",
        "    df[\"all\"] = results[\"all\"]\n",
        "    df[\"women\"] = results[\"women\"] - results[\"men\"]\n",
        "    #race\n",
        "    df[\"black\"] = results[\"black\"] - results[\"white\"]\n",
        "    df[\"hispanic\"] = results[\"hispanic\"] - results[\"white\"]\n",
        "    df[\"asian\"] = results[\"asian\"] - results[\"white\"]\n",
        "\n",
        "    return df\n",
        "\n",
        "def plot_summary(tasks_fname, feature_type, results_path, metric, mini_tasks=True):\n",
        "    dfs = []\n",
        "    with open(tasks_fname,\"r\") as fid:                \n",
        "        for i,l in enumerate(fid):            \n",
        "            task_abv, task_name = l.strip(\"\\n\").split(\",\")\n",
        "            dataset = \"mini-\"+task_abv if mini_tasks else task_abv\n",
        "            fname = \"{}_{}_{}.csv\".format(dataset, feature_type, metric).lower()  \n",
        "            try:\n",
        "                df_results = pd.read_csv(results_path+fname)                   \n",
        "                df_max = df_results.iloc[:,2:].max(axis=1)\n",
        "                df_min = df_results.iloc[:,2:].min(axis=1)\n",
        "                df_results[\"range\"] = df_max - df_min\n",
        "                df_results[\"dataset\"] = [task_abv]*len(df_results)         \n",
        "                dfs.append(df_results)\n",
        "            except FileNotFoundError:\n",
        "                print(\"{} not found...\".format(fname))        \n",
        "\n",
        "    dfs = pd.concat(dfs)    \n",
        "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "    g = sns.catplot(x=\"all\",y=\"dataset\", data=dfs, sharey=True,legend=True, \n",
        "                    legend_out=True, height=6, aspect=0.85,palette=cmap)    \n",
        "    g = sns.catplot(x=\"range\",y=\"dataset\", data=dfs, sharey=True,legend=True, \n",
        "                    legend_out=True, height=6, aspect=0.85,palette=cmap)        \n",
        "#     g.set_xlabels(plot_metric + \" gap\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o3v3P3njsaj"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHkvpLpLxzjw"
      },
      "source": [
        "def underspecification(data_path, dataset, features_path, feature_type, cache_path, metric, task_name, epsilon = 0.01):\n",
        "    res_fname = cache_path+\"/grid_{}_{}_{}.pkl\".format(dataset, feature_type, metric).lower()    \n",
        "    df_val = pd.read_csv(res_fname)    \n",
        "    max_val = df_val[\"all\"].max()    \n",
        "    f_star = df_val[df_val[\"all\"] >= (max_val-epsilon)] \n",
        "    return f_star\n",
        "\n",
        "def plot_underspecifications(data_path, tasks_fname, features_path, feature_type, cache_path, metric, epsilon = 0.01):\n",
        "    with open(data_path+\"/\"+tasks_fname,\"r\") as fid:        \n",
        "        for i,l in enumerate(fid):            \n",
        "            task_abv, task_name = l.strip(\"\\n\").split(\",\")            \n",
        "            f_star = underspecification(data_path, task_abv, features_path, feature_type, cache_path, metric, task_name, epsilon)\n",
        "            df_test = test_seeds(data_path, task_abv, features_path, feature_type, cache_path, metric, f_star)    \n",
        "            plot_minority_gaps(df_test,task_name)\n",
        "\n",
        "def underspecification_densities(data_path, tasks_fname, features_path, feature_type, cache_path, metric):\n",
        "    # fig, ax = plt.subplots(1, 1,  figsize=(12,5), sharex=True, sharey=True)\n",
        "    epsilons = [0, 0.01, 0.02]\n",
        "    results = defaultdict(list)\n",
        "    with open(data_path+\"/\"+tasks_fname,\"r\") as fid:                        \n",
        "        for i,l in enumerate(fid):            \n",
        "            task_abv, task_name = l.strip(\"\\n\").split(\",\")            \n",
        "            for epsilon in epsilons:\n",
        "                f_star = underspecification(data_path, task_abv, features_path, feature_type, cache_path, metric, task_name, epsilon)\n",
        "                results[epsilon] += [len(f_star)]\n",
        "                \n",
        "    for epsilon in epsilons:\n",
        "        x = results[epsilon]\n",
        "        df = pd.DataFrame(x, columns=[epsilon])\n",
        "        # print(df)\n",
        "        df.plot.hist()\n",
        "            # df_test = test_seeds(data_path, dataset, features_path, feature_type, cache_path, metric, f_star)    \n",
        "            # plot_minority_gaps(df_test,task_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr-dBCfqJhLq"
      },
      "source": [
        "cache_path = BASE_PATH+\"/DATA/processed_clinical_grid/\"\n",
        "feature_type=\"CLINICALBERT-POOL\"\n",
        "metric=\"auroc\"\n",
        "dataset=\"IHM\"\n",
        "underspecification_densities(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, feature_type, cache_path, metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWuATGGezUfY"
      },
      "source": [
        "cache_path = BASE_PATH+\"/DATA/processed_clinical_grid/\"\n",
        "feature_type=\"CLINICALBERT-POOL\"\n",
        "metric=\"auroc\"\n",
        "dataset=\"IHM\"\n",
        "plot_underspecifications(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, feature_type, cache_path, metric, epsilon = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmBAlSIDFeOp",
        "scrolled": true,
        "tags": []
      },
      "source": [
        "dataset=\"IHM\"\n",
        "dataset=\"mini-\"+dataset\n",
        "CLASSIFIER=\"torch\"\n",
        "PLOT_METRIC=\"auroc\"\n",
        "TUNE_METRIC=\"auroc\"\n",
        "Z = run_analyses(INPUT_PATH, dataset, FEATURES_PATH, MODEL, OUTPUT_PATH, TMP_PATH, \n",
        "             clear_results=False, tune_metric=TUNE_METRIC, plot_metric=None)\n",
        "# plot_analyses(OUTPUT_PATH, dataset, MODEL, dataset, plot_metric=PLOT_METRIC, tune_metric=TUNE_METRIC)\n",
        "Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mXSPLmKHM_q"
      },
      "source": [
        "dataset=\"IHM\"\n",
        "# dataset=\"mini-\"+dataset\n",
        "CLASSIFIER=\"torch\"\n",
        "PLOT_METRIC=\"auroc\"\n",
        "TUNE_METRIC=\"auroc\"\n",
        "Z = run_analyses(INPUT_PATH, dataset, FEATURES_PATH, MODEL, OUTPUT_PATH, None, \n",
        "             clear_results=False, tune_metric=TUNE_METRIC, plot_metric=PLOT_VAR, fairness=True)\n",
        "plot_analyses(OUTPUT_PATH, dataset, MODEL, dataset, plot_metric=PLOT_METRIC, tune_metric=TUNE_METRIC)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxyq5bgZ3qMm"
      },
      "source": [
        "CLASSIFIER=\"torch\"\n",
        "# run_analyses(INPUT_PATH, dataset, FEATURES_PATH, MODEL, OUTPUT_PATH, None, clear_results=False, tune_metric=None, subsample=False, plots=True)\n",
        "plot_analyses(OUTPUT_PATH, dataset, MODEL, dataset, tune_metric=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YtLdW8pCzJV",
        "scrolled": true
      },
      "source": [
        "plot_performances(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=False, plot_metric=PLOT_VAR, tune_metric=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXq6jZL6pnQ_",
        "scrolled": true,
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "#Run tasks\n",
        "CLASSIFIER=\"torch\"\n",
        "PLOT_METRIC=\"auroc\"\n",
        "TUNE_METRIC=\"auroc\"\n",
        "OUTPUT_PATH=BASE_PATH+\"/DATA/results_newrun/\"\n",
        "MODEL=\"CLINICALBERT-POOL\"\n",
        "# run_tasks(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, OUTPUT_PATH, TMP_PATH, reset=True, \n",
        "#            metric=TUNE_METRIC, tuning=False, mini_tasks=True)\n",
        "plot_tasks(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=False,  metric=PLOT_VAR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlqYtt7keme6"
      },
      "source": [
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TssWeoy5eRMc"
      },
      "source": [
        "#Run tasks\n",
        "CLASSIFIER=\"torch\"\n",
        "OUTPUT_PATH=BASE_PATH+\"/DATA/results_newrun/\"\n",
        "MODEL=\"CLINICALBERT-POOL\"\n",
        "\n",
        "plot_summary(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=False,  metric=PLOT_VAR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "tags": [],
        "id": "6_ao2P11rKKA"
      },
      "source": [
        "RESULTS_PATH = BASE_PATH+\"/DATA/results_clinical2/\"\n",
        "MODEL=\"BERT-POOL\"\n",
        "# (tasks_fname, feature_type, results_path, metric, mini_tasks=True)\n",
        "plot_tasks(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH , metric=PLOT_VAR, mini_tasks=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPdW1bfWmzk1"
      },
      "source": [
        "TUNE_METRIC=\"auroc\"\n",
        "plot_gridsearch(GRID_OUTPUT_PATH, \"DOLM\", MODEL, TUNE_METRIC, \"DOLM\")\n",
        "plot_gridsearch(GRID_OUTPUT_PATH, \"DOLM\", MODEL, TUNE_METRIC, \"DOLM\", delta=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "inizZ4HhrKKE"
      },
      "source": [
        "RESULTS_PATH = BASE_PATH+\"/DATA/results_clinical_GRID/\"\n",
        "MODEL=\"CLINICALBERT-POOL\"\n",
        "TUNE_METRIC=\"auroc\"\n",
        "plot_gridsearches(INPUT_PATH+\"tasks.txt\", RESULTS_PATH, MODEL, False, TUNE_METRIC, subgroups=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiEhojW2rKKF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}