{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTNg0aoYxumN"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')      \n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4h40mg5Ocpz2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUQ7MBldvx5d"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import fnmatch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import os\n",
    "from pdb import set_trace\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, auc, precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import torch\n",
    "import uuid\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/content/drive/My Drive/collab/TADAT/\") \n",
    "#local\n",
    "from tadat.pipeline import plots\n",
    "import tadat.core as core\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK8cWdcXmzju"
   },
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWBAvaw5zIe_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# BASE_PATH = \"/content/drive/My Drive/collab/MIMIC/\"\n",
    "BASE_PATH = \"/Users/samir/Dev/projects/MIMIC_random_seeds/MIMIC/\"\n",
    "INPUT_PATH = BASE_PATH+\"/DATA/input/\"\n",
    "FEATURES_PATH = BASE_PATH+\"/DATA/features/\"\n",
    "OUTPUT_PATH = BASE_PATH+\"/DATA/results/\"\n",
    "# TMP_PATH = BASE_PATH+\"/DATA/processed/\"\n",
    "\n",
    "TUNE_OUTPUT_PATH = BASE_PATH+\"/DATA/results_fine/\"\n",
    "TUNE_TMP_PATH = BASE_PATH+\"/DATA/processed_fine/\"\n",
    "\n",
    "GRID_OUTPUT_PATH = BASE_PATH+\"/DATA/results_grid/\"\n",
    "GRID_TMP_PATH = BASE_PATH+\"/DATA/processed_grid/\"\n",
    "\n",
    "#configs\n",
    "N_SEEDS=50\n",
    "N_VAL_SEEDS = 5\n",
    "N_VAL_RUNS = 5\n",
    "N_TASKS = 3\n",
    "N_TASKS = 50\n",
    "# PLOT_VARS=[\"auroc\",\"auprc\",\"sensitivity\",\"specificity\"]\n",
    "PLOT_VARS=[\"auroc\",\"sensitivity\"]\n",
    "MODEL=\"BERT-POOL\"\n",
    "METRIC = \"auroc\"\n",
    "\n",
    "GROUPS = { \"GENDER\": [\"M\",\"F\"],   \n",
    "         \"ETHNICITY\": [\"WHITE\",\"BLACK\",\"ASIAN\",\"HISPANIC\"]\n",
    "}\n",
    "\n",
    "CLASSIFIER = 'sklearn'\n",
    "CLASSIFIER = 'torch'\n",
    "# CLASSIFIER = 'mseq'\n",
    "CLINICALBERT = \"emilyalsentzer/Bio_ClinicalBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GEYGvy7CzIn"
   },
   "outputs": [],
   "source": [
    "\n",
    "SMALL_SIZE = 18\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('text', usetex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sygeVFaACzI8"
   },
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5SjTOgye0pB"
   },
   "outputs": [],
   "source": [
    "def train_classifier(X_train, Y_train, X_val, Y_val, \n",
    "                     init_seed, shuffle_seed=None, input_dimension=None):    \n",
    "    \"\"\" train a classifier\n",
    "        X_train: training instances \n",
    "        Y_yrain: training labels\n",
    "        X_val: validation instances\n",
    "        Y_val: validation labels\n",
    "        init_seed: parameter initialization seed\n",
    "        shuffle_seed: data shuffling seed\n",
    "        input_dimension: number of input features\n",
    "        \n",
    "        returns: fitted classifier\n",
    "    \"\"\"\n",
    "    #CLASSIFIER is a global variable indicating the type of classifier\n",
    "    if CLASSIFIER == \"torch\":        \n",
    "        x = core.models.MyLinearModel(in_dim=input_dimension, out_dim=1, \n",
    "                    loss_fn=torch.nn.BCEWithLogitsLoss(), \n",
    "                    init_seed=init_seed, n_epochs=500, \n",
    "                    default_lr=0.1, batch_size=None, \n",
    "                    shuffle_seed=shuffle_seed, silent=True,\n",
    "                    shuffle=True) \n",
    "        x.fit(X_train, Y_train, X_val, Y_val)\n",
    "    elif CLASSIFIER == \"mseq\":        \n",
    "        x = core.models.MultiBERTSeq(in_dim=input_dimension, out_dim=1, \n",
    "                    loss_fn=torch.nn.BCELoss(), \n",
    "                    init_seed=init_seed, n_epochs=500, \n",
    "                    default_lr=0.1, batch_size=None, \n",
    "                    shuffle_seed=shuffle_seed, silent=True,\n",
    "                    shuffle=True) \n",
    "        x.fit(X_train, Y_train, X_val, Y_val)\n",
    "    elif CLASSIFIER == \"sklearn\":\n",
    "        x = SGDClassifier(loss=\"log\", random_state=shuffle_seed)\n",
    "        x.fit(X_train, Y_train)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return x\n",
    "\n",
    "def evaluate_classifier(model, X_test, Y_test,\n",
    "                   labels, model_name, random_seed, subgroup, res_path=None):\n",
    "    \"\"\" evaluate a classifier\n",
    "        model: classifier to be evaluated        \n",
    "        X_test: test instances\n",
    "        Y_test: test labels\n",
    "        labels: label set\n",
    "        model_name: model name\n",
    "        random_seed: random seed that was used to train the classifier\n",
    "        subgroup: demographic subgroup represented in the data\n",
    "        res_path: path to save the results\n",
    "        \n",
    "        returns: dictionary of evaluation wrt to different metrics\n",
    "    \"\"\"\n",
    "    Y_hat = model.predict(X_test)\n",
    "    Y_hat_prob = model.predict_proba(X_test)\n",
    "    #get probabilities for the positive class\n",
    "    if CLASSIFIER == 'sklearn':\n",
    "        Y_hat_prob = Y_hat_prob[:,labels[1]]    \n",
    "    microF1 = f1_score(Y_test, Y_hat, average=\"micro\") \n",
    "    macroF1 = f1_score(Y_test, Y_hat, average=\"macro\") \n",
    "    try:\n",
    "        aurocc = roc_auc_score(Y_test, Y_hat_prob)\n",
    "    except ValueError:\n",
    "        aurocc = 0\n",
    "    try:\n",
    "        prec, rec, thresholds = precision_recall_curve(Y_test, Y_hat_prob)       \n",
    "        auprc = auc(rec, prec)\n",
    "    except ValueError:\n",
    "        auprc = 0\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_test, Y_hat).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "        sensitivity = tp / (fn+tp)\n",
    "    except ValueError:\n",
    "        specificity, sensitivity = 0, 0\n",
    "    \n",
    "    res = {\"model\":model_name, \n",
    "            \"seed\":random_seed,  \n",
    "            \"group\":subgroup,    \n",
    "            \"microF1\":round(microF1,3),\n",
    "            \"macroF1\":round(macroF1,3),\n",
    "            \"auroc\":round(aurocc,3),\n",
    "            \"auprc\":round(auprc,3),\n",
    "            \"specificity\":round(specificity,3),\n",
    "            \"sensitivity\":round(sensitivity,3)           \n",
    "            }\n",
    "\n",
    "    if res_path is not None:    \n",
    "        core.helpers.save_results(res, res_path, sep=\"\\t\")\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def vectorize(df_train, df_val, df_test, subject_ids):\n",
    "    \"\"\" vectorize the instances and stratify them by demographic subgroup\n",
    "        df_train: training data as a DataFrame\n",
    "        df_test: test data as a DataFrame\n",
    "        df_val: validation data as a DataFrame\n",
    "        subject_ids: list of subject ids (the order corresponds to order of the features that were extracted)\n",
    "        \n",
    "        returns: vectorized train, validation and test datasets, stratified by demographic subgroup\n",
    "                 label vocabulary                 \n",
    "    \"\"\"\n",
    "\n",
    "    #vectorize labels\n",
    "    train_Y = df_train[\"Y\"]\n",
    "    val_Y = df_val[\"Y\"]           \n",
    "    test_Y = df_test[\"Y\"]               \n",
    "    label_vocab = core.vectorizer.get_labels_vocab(train_Y+val_Y)\n",
    "    train_Y,_ = core.vectorizer.label2idx(train_Y, label_vocab)\n",
    "    val_Y,_ = core.vectorizer.label2idx(val_Y, label_vocab)\n",
    "    test_Y,_ = core.vectorizer.label2idx(test_Y, label_vocab)      \n",
    "    \n",
    "    #get indices into the feature matrix\n",
    "    train_idxs = [subject_ids.index(i) for i in list(df_train[\"SUBJECT_ID\"])] \n",
    "    val_idxs = [subject_ids.index(i) for i in list(df_val[\"SUBJECT_ID\"])] \n",
    "    test_idxs = [subject_ids.index(i) for i in list(df_test[\"SUBJECT_ID\"])] \n",
    "    #construct datasets\n",
    "    train = {}\n",
    "    test = {}\n",
    "    val = {}\n",
    "    #unstratified \n",
    "    train[\"all\"] = [train_idxs, train_Y]\n",
    "    test[\"all\"] = [test_idxs, test_Y]\n",
    "    val[\"all\"] = [val_idxs, val_Y]\n",
    "    #stratified by demographics \n",
    "    for group in list(GROUPS.keys()):\n",
    "        #and subgroups\n",
    "        for subgroup in GROUPS[group]:                \n",
    "            df_train_sub = df_train[df_train[group] == subgroup]\n",
    "            df_test_sub = df_test[df_test[group] == subgroup]\n",
    "            df_val_sub = df_val[df_val[group] == subgroup]\n",
    "            # print(\"[subgroup: {} | tr: {} | ts: {} | val: {}]\".format(subgroup, len(df_train_sub), len(df_test_sub), len(df_val_sub)))\n",
    "\n",
    "            #vectorize labels               \n",
    "            train_Y_sub,_ = core.vectorizer.label2idx(df_train_sub[\"Y\"], label_vocab)            \n",
    "            test_Y_sub,_ = core.vectorizer.label2idx(df_test_sub[\"Y\"], label_vocab)            \n",
    "            val_Y_sub,_ = core.vectorizer.label2idx(df_val_sub[\"Y\"], label_vocab)      \n",
    "            #get indices into the feature matrix\n",
    "            train_idxs_sub = [subject_ids.index(i) for i in list(df_train_sub[\"SUBJECT_ID\"])] \n",
    "            test_idxs_sub = [subject_ids.index(i) for i in list(df_test_sub[\"SUBJECT_ID\"])] \n",
    "            val_idxs_sub = [subject_ids.index(i) for i in list(df_val_sub[\"SUBJECT_ID\"])] \n",
    "            if subgroup == \"M\":\n",
    "                subgroup = \"men\"\n",
    "            elif subgroup == \"F\":\n",
    "                subgroup = \"women\"\n",
    "            train[subgroup.lower()] = [train_idxs_sub, train_Y_sub]\n",
    "            test[subgroup.lower()] = [test_idxs_sub, test_Y_sub]\n",
    "            val[subgroup.lower()] = [val_idxs_sub, val_Y_sub]\n",
    "\n",
    "    return train, val, test, label_vocab\n",
    "\n",
    "\n",
    "def get_features(data, vocab_size, feature_type, embeddings=None):\n",
    "    \"\"\" compute features from the data\n",
    "        data: data instances\n",
    "        vocab_size: size of the vocabulary\n",
    "        feature_type: type of feature (e.g bag of words, BERT)\n",
    "        word_vectors: path to pretrained (static) word vectors\n",
    "        \n",
    "        returns: feature matrix\n",
    "    \"\"\"\n",
    "    if feature_type == \"BOW-BIN\":\n",
    "        X = core.features.BOW(data, vocab_size,sparse=True)\n",
    "    elif feature_type == \"BOW-FREQ\":\n",
    "        X = core.features.BOW_freq(data, vocab_size,sparse=True)\n",
    "    elif feature_type == \"BOE-BIN\":\n",
    "        X = core.features.BOE(data, embeddings,\"bin\")\n",
    "    elif feature_type == \"BOE-SUM\": \n",
    "        X = core.features.BOE(data, embeddings,\"sum\")\n",
    "    elif feature_type == \"U2V\": \n",
    "        X = core.features.BOE(data, embeddings,\"bin\")\n",
    "    elif feature_type == \"BERT-POOL\":\n",
    "        X =  core.transformer_encoders.encode_sequences(data, batchsize=64)        \n",
    "    elif feature_type == \"BERT-CLS\":\n",
    "        X =  core.transformer_encoders.encode_sequences(data, cls_features=True,\n",
    "                                                        batchsize=64)            \n",
    "    elif feature_type == \"MULTI-BERT-POOL\":\n",
    "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, batchsize=32,\n",
    "                                                         tmp_path=TMP_PATH)\n",
    "    elif feature_type == \"MULTI-BERT-CLS\":\n",
    "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, \n",
    "                                                         cls_features=True,\n",
    "                                                         batchsize=32,\n",
    "                                                         tmp_path=TMP_PATH)\n",
    "    elif feature_type == \"CLINICALBERT-POOL\":\n",
    "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
    "        X =  core.transformer_encoders.encode_sequences(data, batchsize=64, tokenizer=tokenizer,\n",
    "                                                                    encoder=encoder)        \n",
    "    elif feature_type == \"CLINICALBERT-CLS\":\n",
    "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
    "        X =  core.transformer_encoders.encode_sequences(data, cls_features=True,batchsize=64,\n",
    "                                                                    tokenizer=tokenizer, encoder=encoder)        \n",
    "    elif feature_type == \"CLINICALMULTI-BERT-POOL\":\n",
    "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
    "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, batchsize=32,tmp_path=TMP_PATH,\n",
    "                                                              tokenizer=tokenizer, encoder=encoder)\n",
    "    elif feature_type == \"CLINICALMULTI-BERT-CLS\":\n",
    "        tokenizer, encoder = core.transformer_encoders.get_encoder(CLINICALBERT)\n",
    "        X =  core.transformer_encoders.encode_multi_sequences(data, 10, cls_features=True, \n",
    "                                                                batchsize=32,tmp_path=TMP_PATH,\n",
    "                                                                tokenizer=tokenizer, encoder=encoder)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return X\n",
    "\n",
    "def extract_features(feature_type, path):\n",
    "    \"\"\" extract features and save features\n",
    "\n",
    "        method will first look for computed features on disk and return them if found;\n",
    "        otherwise it will look for a file with name *patients.csv*;        \n",
    "        \n",
    "        feature_type: type of feature (e.g bag of words, BERT)\n",
    "        path: directory where the data can be found\n",
    "                \n",
    "        returns: list of subject ids and feature matrix -- the order of ids corresponds to order of the instances in the feature matrix\n",
    "    \"\"\"\n",
    "    X = read_cache(path+\"feats_{}\".format(feature_type))\n",
    "    if X:\n",
    "        print(\"[reading cached features]\")\n",
    "        subject_ids, X_feats = X\n",
    "    else:\n",
    "        print(\"[computing {} features]\".format(feature_type))\n",
    "        df = pd.read_csv(path+\"patients.csv\", sep=\"\\t\", header=0)\n",
    "        subject_ids = list(df[\"SUBJECT_ID\"])\n",
    "        docs = list(df[\"TEXT\"])\n",
    "        if \"BERT\" in feature_type:\n",
    "            X_feats = get_features(docs, None, feature_type)\n",
    "        elif \"U2V\" in feature_type:\n",
    "            X, user_vocab = core.vectorizer.docs2idx(subject_ids)\n",
    "            user_embeddings, _ = core.embeddings.read_embeddings(path+\"/user_embeddings.txt\", user_vocab)\n",
    "            X_feats = get_features(X, len(user_vocab), feature_type, user_embeddings)\n",
    "        else:\n",
    "            embeddings = None\n",
    "            X, word_vocab = core.vectorizer.docs2idx(docs)\n",
    "            if \"BOE\" in feature_type:\n",
    "                embeddings, _ = core.embeddings.read_embeddings(path+\"/word_embeddings.txt\", word_vocab)\n",
    "            X_feats = get_features(X, len(word_vocab), feature_type, embeddings)\n",
    "        #save features\n",
    "        print(\"[saving features]\")\n",
    "        write_cache(path+\"feats_{}\".format(feature_type), \n",
    "                    [subject_ids, X_feats])\n",
    "    return subject_ids, X_feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCwwyC1RCzJC"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6v8lR5X0cp0i"
   },
   "outputs": [],
   "source": [
    "def read_dataset(path, dataset_name, df_patients):    \n",
    "    \n",
    "    \"\"\" read dataset        \n",
    "        path: path to the dataset\n",
    "        dataset_name: name of the dataset\n",
    "        df_patients: DataFrame of patients\n",
    "                \n",
    "        returns: train, test and validation sets as DataFrames\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(\"{}/{}_train.csv\".format(path, dataset_name), \n",
    "                           sep=\"\\t\", header=0)\n",
    "    df_test  = pd.read_csv(\"{}/{}_test.csv\".format(path, dataset_name),\n",
    "                           sep=\"\\t\", header=0)\n",
    "    df_val   = pd.read_csv(\"{}/{}_val.csv\".format(path, dataset_name),\n",
    "                           sep=\"\\t\", header=0)\n",
    "    #set indices\n",
    "    df_patients.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "    df_train.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "    df_test.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "    df_val.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "\n",
    "    df_train = df_train.join(df_patients, on=\"SUBJECT_ID\", \n",
    "                             how=\"inner\", lsuffix=\"N_\").reset_index()\n",
    "    df_test = df_test.join(df_patients, on=\"SUBJECT_ID\", \n",
    "                           how=\"inner\", lsuffix=\"N_\").reset_index()\n",
    "    df_val = df_val.join(df_patients, on=\"SUBJECT_ID\", \n",
    "                         how=\"inner\", lsuffix=\"N_\").reset_index()\n",
    "\n",
    "    return df_train, df_test, df_val   \n",
    "\n",
    "\n",
    "def read_cache(path):\n",
    "    \"\"\" read a pickled object\n",
    "        \n",
    "        path: path\n",
    "        returns: object\n",
    "    \"\"\"\n",
    "    X = None\n",
    "    try:\n",
    "        with open(path, \"rb\") as fi:            \n",
    "            X = pickle.load(fi)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def write_cache(path, o):\n",
    "    \"\"\" pickle an object\n",
    "            \n",
    "        path: path\n",
    "        o: object\n",
    "    \"\"\"\n",
    "    dirname = os.path.dirname(path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    with open(path, \"wb\") as fo:\n",
    "        pickle.dump(o, fo)\n",
    "\n",
    "def old_run(data_path, dataset, features_path, feature_type, cache_path, metric, n_seeds=N_SEEDS, clear_results=False):\n",
    "    \"\"\" \n",
    "        train classifiers with different random seeds and compare the performance over each demographic subgroup\n",
    "\n",
    "        data_path: path to the data\n",
    "        dataset: dataset to be evaluted\n",
    "        features_path: path to the features\n",
    "        feature_type: type of feature (e.g bag of words, BERT)\n",
    "        cache_path: cache path \n",
    "        metric: evaluation metric\n",
    "        n_seeds: number of seeds\n",
    "\n",
    "        returns: results for each subgroup\n",
    "    \"\"\"\n",
    "    #read patients data\n",
    "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
    "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
    "    #read dataset\n",
    "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
    "    \n",
    "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
    "    print(\"[running {} classifier]\".format(CLASSIFIER))\n",
    "    #extract features\n",
    "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
    "    train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
    "    train_idx, train_Y = train[\"all\"]\n",
    "    val_idx, val_Y = val[\"all\"]\n",
    "    #slice the feature matrix to get the corresponding instances\n",
    "    train_X = feature_matrix[train_idx, :]    \n",
    "    val_X = feature_matrix[val_idx, :]    \n",
    "    #create the cache directory if it does not exist\n",
    "    dirname = os.path.dirname(cache_path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    #try to open a cached results file or create a new one if it does not exist\n",
    "    res_fname = cache_path+\"/cache_{}_{}_{}.pkl\".format(dataset, feature_type, metric).lower()    \n",
    "    try:\n",
    "        df_results = pd.read_csv(res_fname)\n",
    "    except FileNotFoundError:\n",
    "        df_results = pd.DataFrame(columns = [\"seed\"] +  list(val.keys()))\n",
    "        df_results.to_csv(res_fname, index=False, header=True)        \n",
    "    #we can skip seeds that have already been evaluated\n",
    "    skip_seeds = set([]) if clear_results else set(df_results[\"seed\"])\n",
    "    groups = list(val.keys())\n",
    "    init_randomizer = RandomState(1)\n",
    "    shuffle_randomizer = RandomState(2)    \n",
    "    # random.seed(1) #ensure repeateable runs \n",
    "    # random_seeds = random.sample(range(0, 10000), n_seeds)        \n",
    "    ##train/test classifier for each random seed pair\n",
    "    # for init_seed, shuffle_seed in itertools.product(random_seeds,repeat=2):   \n",
    "    for j in range(n_seeds):         \n",
    "        init_seed = init_randomizer.randint(10000)\n",
    "        shuffle_seed = shuffle_randomizer.randint(10000)        \n",
    "        seed = \"{}x{}\".format(init_seed, shuffle_seed)  \n",
    "        if seed in skip_seeds:\n",
    "            print(\"skipped seed: {}\".format(seed))\n",
    "            continue\n",
    "        curr_results = {\"seed\":seed}\n",
    "        print(\" > seed: {}\".format(seed))                        \n",
    "        model = train_classifier(train_X, train_Y,val_X, val_Y,  \n",
    "                                    input_dimension=train_X.shape[-1],\n",
    "                                    init_seed=init_seed, \n",
    "                                    shuffle_seed=shuffle_seed)                                                                                \n",
    "        #test each subgroup (note thtat *all* is also a subgroup)\n",
    "        for subgroup in groups:                                \n",
    "            test_idx_sub, test_Y_sub = test[subgroup]                 \n",
    "            test_X_sub = feature_matrix[test_idx_sub, :]                \n",
    "            res_sub = evaluate_classifier(model, test_X_sub, test_Y_sub, \n",
    "                                        label_vocab, feature_type, seed, subgroup)                \n",
    "            curr_results[subgroup]= res_sub[metric]     \n",
    "        #save results\n",
    "        df_results = df_results.append(curr_results, ignore_index=True)\n",
    "        df_results.to_csv(res_fname, index=False, header=True)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "def run(data_path, dataset, features_path, feature_type, results_path, metric, n_seeds=N_SEEDS, clear_results=False):\n",
    "    \"\"\" \n",
    "        train classifiers with different random seeds and compare the performance over each demographic subgroup\n",
    "\n",
    "        data_path: path to the data\n",
    "        dataset: dataset to be evaluted\n",
    "        features_path: path to the features\n",
    "        feature_type: type of feature (e.g bag of words, BERT)\n",
    "        results_path: cache path \n",
    "        metric: evaluation metric\n",
    "        n_seeds: number of seeds\n",
    "\n",
    "        returns: results for each subgroup\n",
    "    \"\"\"\n",
    "    #read patients data\n",
    "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
    "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
    "    #read dataset\n",
    "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
    "    \n",
    "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
    "    print(\"[running {} classifier]\".format(CLASSIFIER))\n",
    "    #extract features\n",
    "    subject_ids, feature_matrix = extract_features(feature_type, features_path)      \n",
    "    train, val, test, label_vocab = vectorize(df_train, df_val, df_test, subject_ids)\n",
    "    train_idx, train_Y = train[\"all\"]\n",
    "    val_idx, val_Y = val[\"all\"]\n",
    "    #slice the feature matrix to get the corresponding instances\n",
    "    train_X = feature_matrix[train_idx, :]    \n",
    "    val_X = feature_matrix[val_idx, :]    \n",
    "    #create the cache directory if it does not exist\n",
    "    dirname = os.path.dirname(results_path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    #try to open a cached results file or create a new one if it does not exist\n",
    "    res_fname = results_path+\"/{}_{}_{}.csv\".format(dataset, feature_type, metric).lower()    \n",
    "    try:\n",
    "        df_results = pd.read_csv(res_fname)\n",
    "    except FileNotFoundError:\n",
    "        df_results = pd.DataFrame(columns = [\"seed\",\"data\"] +  list(val.keys()))\n",
    "        df_results.to_csv(res_fname, index=False, header=True)        \n",
    "    #we can skip seeds that have already been evaluated\n",
    "    skip_seeds = set([]) if clear_results else set(df_results[\"seed\"])\n",
    "    groups = list(val.keys())\n",
    "    init_randomizer = RandomState(1)\n",
    "    shuffle_randomizer = RandomState(2)    \n",
    "    # random.seed(1) #ensure repeateable runs \n",
    "    # random_seeds = random.sample(range(0, 10000), n_seeds)        \n",
    "    ##train/test classifier for each random seed pair\n",
    "    # for init_seed, shuffle_seed in itertools.product(random_seeds,repeat=2):   \n",
    "    for j in range(n_seeds):         \n",
    "        init_seed = init_randomizer.randint(10000)\n",
    "        shuffle_seed = shuffle_randomizer.randint(10000)        \n",
    "        seed = \"{}x{}\".format(init_seed, shuffle_seed)  \n",
    "        if seed in skip_seeds:\n",
    "            print(\"skipped seed: {}\".format(seed))\n",
    "            continue\n",
    "        test_results = {\"seed\":seed, \"data\":\"test\"}\n",
    "        val_results = {\"seed\":seed, \"data\":\"val\"}\n",
    "        print(\" > seed: {}\".format(seed))                        \n",
    "        model = train_classifier(train_X, train_Y,val_X, val_Y,  \n",
    "                                    input_dimension=train_X.shape[-1],\n",
    "                                    init_seed=init_seed, \n",
    "                                    shuffle_seed=shuffle_seed)                                                                                \n",
    "        #test each subgroup (note thtat *all* is also a subgroup)\n",
    "        for subgroup in groups:                                \n",
    "            test_idx_sub, test_Y_sub = test[subgroup]                 \n",
    "            test_X_sub = feature_matrix[test_idx_sub, :]                \n",
    "            test_res_sub = evaluate_classifier(model, test_X_sub, test_Y_sub, \n",
    "                                        label_vocab, feature_type, seed, subgroup)                \n",
    "            test_results[subgroup]= test_res_sub[metric]     \n",
    "            \n",
    "            val_idx_sub, val_Y_sub = val[subgroup]                 \n",
    "            val_X_sub = feature_matrix[val_idx_sub, :]                \n",
    "            val_res_sub = evaluate_classifier(model, val_X_sub, val_Y_sub, \n",
    "                                        label_vocab, feature_type, seed, subgroup)                \n",
    "            val_results[subgroup]= val_res_sub[metric]                 \n",
    "            \n",
    "        #save results\n",
    "        df_results = df_results.append(test_results, ignore_index=True)\n",
    "        df_results = df_results.append(val_results, ignore_index=True)\n",
    "        df_results.to_csv(res_fname, index=False, header=True)\n",
    "\n",
    "\n",
    "    return df_results\n",
    "\n",
    "def search_analyses(results_path, dataset, task_name, feature_type, metric, groups=\"both\", k=50):\n",
    "    \n",
    "    fname = \"{}_{}_{}.csv\".format(dataset, feature_type, metric).lower()       \n",
    "    try:\n",
    "        df_results = pd.read_csv(results_path+fname) \n",
    "        print(len(df_results))\n",
    "        df_results_val = df_results[df_results[\"data\"] == \"val\"]\n",
    "        df_results_test = df_results[df_results[\"data\"] == \"test\"]\n",
    "        # print(df_results)\n",
    "        bs = get_best_seeds(df_results_val, groups, k)      \n",
    "        z = pd.merge(bs, df_results_test, how=\"left\", on=\"seed\") \n",
    "        return z\n",
    "    except FileNotFoundError:\n",
    "        print(\"{} not found...\".format(fname))        \n",
    "        return None\n",
    "\n",
    "CRITERIA = [\"all\", \"perf_avg\", \"perf_avg_std\", \"perf_avg_delta\",\"all_avg_delta\"]\n",
    "def get_best_seeds(df_seeds, groups=\"both\", k=50):    \n",
    "    \"\"\" \n",
    "        select the seeds with the best performance at different number of runs\n",
    "        we compare different seed selection criteria: mean/std performance, mean/std performance averaged over all subgroups\n",
    "        mean subgroup performance delta\n",
    "        \n",
    "        df_seeds: grid search results as DataFrame\n",
    "        groups: demographic groups\n",
    "        k: number of runs \n",
    "\n",
    "        returns: set of best seeds\n",
    "    \"\"\"\n",
    "    \n",
    "    gender = [\"men\",\"women\"]    \n",
    "    race = [\"white\",\"black\", \"hispanic\", \"asian\"]\n",
    "    subgroups = []\n",
    "    if groups == \"gender\" or groups == \"both\":\n",
    "        subgroups += gender\n",
    "    if groups == \"race\" or groups == \"both\":\n",
    "        subgroups += race\n",
    "\n",
    "    for g in subgroups:\n",
    "        df_seeds[\"delta_\"+g] = (df_seeds[\"all\"] - df_seeds[g]).abs()    \n",
    "    \n",
    "    df_seeds[\"perf_avg\"] = df_seeds[[g for g in subgroups]].mean(axis=1)\n",
    "    df_seeds[\"perf_std\"] = df_seeds[[g for g in subgroups]].std(axis=1)\n",
    "    df_seeds[\"perf_avg_std\"] = df_seeds[\"perf_avg\"] - df_seeds[\"perf_std\"]\n",
    "    df_seeds[\"delta_avg\"] = df_seeds[[\"delta_\"+g for g in subgroups]].mean(axis=1)\n",
    "    df_seeds[\"perf_avg_delta\"] = df_seeds[\"perf_avg\"] - df_seeds[\"delta_avg\"]\n",
    "    df_seeds[\"all_avg_delta\"] = df_seeds[\"all\"] - df_seeds[\"delta_avg\"]\n",
    "\n",
    "    # set_trace()\n",
    "    z = math.ceil(len(df_seeds)/k)\n",
    "    res = []\n",
    "    for i in range(z):\n",
    "        step = (i+1)*k\n",
    "        step = min(step, len(df_seeds))\n",
    "        print(\"z:{} i:{} step:{}\".format(z, i,step))\n",
    "        df = df_seeds.iloc[:step,:].reset_index()     \n",
    "        #get the seed with the minimum delta\n",
    "        best_idx = df[\"delta_avg\"].idxmin()        \n",
    "        best_df = df.iloc[best_idx]\n",
    "        seed = best_df[\"seed\"]\n",
    "        perf = best_df[\"delta_avg\"]\n",
    "        res.append({\"seed\":seed, \"criterion\":\"delta_avg\", \"k\":step, \"val\":perf })        \n",
    "        #get seeds with max criteria       \n",
    "        for crit in CRITERIA:   \n",
    "            try:        \n",
    "                best_df = df.iloc[df[crit].idxmax()]\n",
    "                seed = best_df[\"seed\"]\n",
    "                perf = best_df[crit]\n",
    "                res.append({\"seed\":seed, \"criterion\":crit, \"k\":step, \"val\":perf })        \n",
    "            except:\n",
    "                print(\"Error: {}\".format(crit))\n",
    "        df_best_seeds = pd.DataFrame(res)               \n",
    "    return df_best_seeds\n",
    "\n",
    "def find_similar_seeds(df_seeds, seed, metric, epsilon=0.01):    \n",
    "    perf = df_seeds[(df_results[\"seed\"] == seed) & (df_results[\"data\"] == \"val\")]\n",
    "    df_similar = df_results[df_results[metric] >= (perf-epsilon)] \n",
    "    return df_similar\n",
    "\n",
    "# def plot_searches(df_results):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"IHM\"\n",
    "dataset=\"mini-\"+dataset\n",
    "MODEL=\"BERT-POOL\"\n",
    "OUTPUT_PATH=BASE_PATH+\"/DATA/full_runs/\"\n",
    "# Z = run_analyses(INPUT_PATH, dataset, FEATURES_PATH, MODEL, OUTPUT_PATH, OUTPUT_PATH, \n",
    "#              clear_results=False,  metric=METRIC)\n",
    "\n",
    "# plot_analyses(OUTPUT_PATH, dataset, \"In Hospital Mortality\", MODEL, metric=METRIC)\n",
    "h = search_analyses(OUTPUT_PATH, dataset, \"In Hospital Mortality\", MODEL, metric=METRIC, groups=\"gender\", k=10)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we investigate if we can find the seeds with the best performance and fairness by grid search on validation data\n",
    "# Here we investigate if we have a case of 'underspecification', i.e. we have random seeds that perform equally well on validation data but differently on test data\n",
    "def underspecification( cache_path, dataset,  feature_type,  metric, epsilon = 0.01):\n",
    "    \"\"\" Find a set of seeds with a performance similar the best seed (i.e., within a small epsilon) \n",
    "    \n",
    "    cache_path: cache path\n",
    "    dataset: name of dataset \n",
    "    feature_type: feature type\n",
    "    metric: evaluation metric \n",
    "    epsilon: maximum performance difference compared to the best seed\n",
    "\n",
    "    return: set of seeds\n",
    "    \"\"\" \n",
    "\n",
    "    res_fname = cache_path+\"/grid_{}_{}_{}.pkl\".format(dataset, feature_type, metric).lower()    \n",
    "    df_val = pd.read_csv(res_fname)    \n",
    "    max_val = df_val[\"all\"].max()    \n",
    "    f_star = df_val[df_val[\"all\"] >= (max_val-epsilon)] \n",
    "    print(\"Found {} seeds\".format(len(f_star)))    \n",
    "    f_star = f_star[[\"seed\"]]\n",
    "    return f_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTyzgoy4vx64"
   },
   "source": [
    "# Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVpc3ZQ-jsaV"
   },
   "outputs": [],
   "source": [
    "def run_analyses(data_path, dataset, features_path, feature_type, results_path, \n",
    "                 metric, clear_results=False):    \n",
    "\n",
    "    if not os.path.exists(results_path): os.makedirs(results_path)     \n",
    "\n",
    "    df_results = run(data_path, dataset, features_path, feature_type, results_path, metric, clear_results=clear_results)         \n",
    "    fname = \"{}_{}_{}.csv\".format(dataset, feature_type, metric).lower()\n",
    "    \n",
    "    df_results.to_csv(results_path+fname, index=False, header=True)\n",
    "    return df_results               \n",
    "\n",
    "#Run All the tasks\n",
    "def run_tasks(data_path, tasks_fname, features_path, feature_type, results_path,   \n",
    "             metric, reset=False, mini_tasks=True):\n",
    "    #if reset delete the completed tasks file\n",
    "    if reset: reset_tasks(results_path)    \n",
    "    with open(data_path+tasks_fname,\"r\") as fid:\n",
    "        for i,l in enumerate(fid):\n",
    "            if i > N_TASKS: break\n",
    "            fname, task_name = l.strip(\"\\n\").split(\",\")            \n",
    "            dataset = \"mini-\"+fname if mini_tasks else fname\n",
    "            # dataset = fname\n",
    "            if is_task_done(results_path, dataset): \n",
    "                print(\"[dataset: {} already processed]\".format(dataset))\n",
    "                continue                        \n",
    "            print(\"******** {} {} ********\".format(task_name, dataset))      \n",
    "            run_analyses(data_path, dataset, features_path, feature_type,  \n",
    "                         results_path, metric=metric, clear_results=False)\n",
    "            task_done(results_path, dataset)\n",
    "\n",
    "def task_done(path,  task):\n",
    "    dirname = os.path.dirname(path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    with open(path+\"completed_tasks.txt\", \"a\") as fod:\n",
    "        fod.write(task+\"\\n\")\n",
    "\n",
    "def reset_tasks(path):\n",
    "    dirname = os.path.dirname(path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    with open(path+\"completed_tasks.txt\", \"w\") as fod:\n",
    "        fod.write(\"\")\n",
    "\n",
    "def is_task_done(path,  task):\n",
    "    try:\n",
    "        with open(path+\"completed_tasks.txt\", \"r\") as fid:\n",
    "            tasks = fid.read().split(\"\\n\")            \n",
    "        return task in set(tasks)\n",
    "    except FileNotFoundError:\n",
    "        #create file if not found\n",
    "        dirname = os.path.dirname(path)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        with open(path+\"completed_tasks.txt\", \"w\") as fid:\n",
    "            fid.write(\"\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-jPDUVwDAv1"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yxk0TBDYC_mC"
   },
   "outputs": [],
   "source": [
    "# Generate plots \n",
    "\n",
    "# def plot_densities(df, ax, title):\n",
    "#     ax.set_title(title)\n",
    "#     for y in PLOT_VARS:        \n",
    "#         try:\n",
    "#             df.plot.kde(ax=ax, x=\"seed\", y=y)\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "# def plot_analyses(results_path, dataset, task_name, feature_type, metric, minorities=False, data_partition=\"test\"):\n",
    "\n",
    "#     fname = \"{}_{}_{}.csv\".format(dataset, feature_type, metric).lower()       \n",
    "#     try:\n",
    "#         df_results = pd.read_csv(results_path+fname) \n",
    "#         df_results = df_results[df_results[\"data\"] == data_partition]\n",
    "#         print(df_results)        \n",
    "#         plot_deltas(df_results, task_name, minorities=minorities)\n",
    "#         plt.savefig(\"plots/deltas_{}.pdf\".format(dataset.lower()),dpi=300, bbox_inches='tight')\n",
    "#         plt.show()      \n",
    "#         plot_scatters(df_results, task_name, minorities=minorities)        \n",
    "#         plt.savefig(\"plots/scatters_{}.pdf\".format(dataset.lower()),dpi=300, bbox_inches='tight')\n",
    "#         plt.show()      \n",
    "#     except FileNotFoundError:\n",
    "#         print(\"{} not found...\".format(fname))        \n",
    "\n",
    "\n",
    "# def plot_tasks(tasks_fname, feature_type, results_path, metric, mini_tasks=True):\n",
    "#     task_abvs = []\n",
    "#     with open(tasks_fname,\"r\") as fid:        \n",
    "#         for i,l in enumerate(fid):            \n",
    "#             task_abv, task_name = l.strip(\"\\n\").split(\",\")\n",
    "#             dataset = \"mini-\"+task_abv if mini_tasks else task_abv\n",
    "#             task_abvs.append(task_abv.lower())\n",
    "#             plot_analyses(results_path, dataset, task_name, feature_type, metric)\n",
    "\n",
    "# def plot_gridsearch(cache_path, dataset, feature_type, tune_metric, title, subgroups=False, delta=False):    \n",
    "#     res_fname = cache_path+\"{}_{}_{}_all_tuned.csv\".format(dataset, feature_type, tune_metric).lower()              \n",
    "#     df_results = pd.read_csv(res_fname)  \n",
    "#     df_all = df_results[df_results[\"criterion\"] == \"all\"]    \n",
    "#     if delta:\n",
    "#         last_all = df_all.iloc[df_all[\"k\"].idxmax()][\"avg_delta_test\"]\n",
    "#         best_all = df_all.iloc[df_all[\"avg_delta_test\"].idxmin()][\"avg_delta_test\"]    \n",
    "#     else:\n",
    "#         last_all = df_all.iloc[df_all[\"k\"].idxmax()][\"all\"]    \n",
    "#         best_all = df_all.iloc[df_all[\"all\"].idxmax()][\"all\"]    \n",
    "#     cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "# #     criteria = [\"all\", \"perf_avg\", \"perf_avg_std\", \"delta_avg\", \"perf_avg_delta\"]\n",
    "#     criteria = [\"all\",\"perf_avg\", \"all_avg_delta\", \"perf_avg_delta\"]\n",
    "#     crit_labels = [\"performance\",\"subgroup avg\", \"performance - avg subgroup delta\", \"subgroup avg - avg subgroup delta\"]\n",
    "# #     set_trace()\n",
    "#     groups = [\"all\", \"men\", \"women\", \"white\", \"black\",\"asian\", \"hispanic\"]\n",
    "#     fig, ax = plt.subplots(1, len(criteria),  figsize=(25,8), sharex=True, sharey=True)    \n",
    "#     if delta:\n",
    "#         groups = [\"delta_\" + l for l in groups]\n",
    "#     for i in range(len(criteria)):\n",
    "#         df = df_results[df_results[\"criterion\"] == criteria[i]]    \n",
    "#         ax[i].set_title(crit_labels[i])\n",
    "#         if delta:\n",
    "#             ax[i].axhline(best_all, linewidth=3, c=\"gray\", linestyle=\"--\")\n",
    "#             ax[i].axhline(last_all, linewidth=2, c=\"gray\", linestyle=\"--\")\n",
    "#             df.plot(x=\"k\", y=\"avg_delta_test\",   linewidth=4, ax=ax[i], c=\"gray\")    \n",
    "#         else:\n",
    "#             ax[i].axhline(best_all, linewidth=3, c=cmap[0], linestyle=\"--\")\n",
    "#             ax[i].axhline(last_all, linewidth=2, c=cmap[0], linestyle=\"--\")\n",
    "#             df.plot(x=\"k\", y=\"all\",   linewidth=4, ax=ax[i])    \n",
    "#         #plot indiv\n",
    "#         if subgroups:\n",
    "#             for g in groups[1:]:\n",
    "#                 df.plot(x=\"k\", y=g, ax=ax[i])\n",
    "            \n",
    "#         #place legend on the last subplot\n",
    "#         if i == len(criteria)-1: \n",
    "#             ax[i].legend(bbox_to_anchor=(1.05, 0), loc='lower left', borderaxespad=0.)\n",
    "#         else:\n",
    "#             ax[i].get_legend().remove()\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "        \n",
    "# def plot_gridsearches(tasks_fname, results_path, feature_type, \n",
    "#                mini_tasks=True, metric=None, subgroups=False):\n",
    "#     with open(tasks_fname,\"r\") as fid:        \n",
    "#         for i,l in enumerate(fid):     \n",
    "#             fname, task_name = l.strip(\"\\n\").split(\",\")\n",
    "#             dataset = \"mini-\"+fname if mini_tasks else fname\n",
    "#             print(dataset)\n",
    "#             try:\n",
    "#                 plot_gridsearch(results_path, dataset, feature_type, metric, task_name, subgroups)\n",
    "#                 plot_gridsearch(results_path, dataset, feature_type, metric, task_name, subgroups, delta=True)\n",
    "#             except FileNotFoundError:\n",
    "#                 print(\"Dataset {} not found\".format(dataset))\n",
    "\n",
    "# def plot_deltas(results, title, minorities=False):      \n",
    "#     if minorities:\n",
    "#         subgroups = [\"women\",\"black\",\"asian\",\"hispanic\"]\n",
    "#         df_deltas = get_minority_deltas(results)\n",
    "#     else:\n",
    "#         subgroups = [\"men\",\"women\",\"white\",\"black\",\"asian\",\"hispanic\"]\n",
    "#         df_deltas = get_deltas(results)\n",
    "#     df_delta_long = pd.melt(df_deltas, id_vars=[\"seed\"], value_vars=subgroups, \n",
    "#                                                       value_name=\"delta\", var_name=\"group\")\n",
    "#     g = sns.catplot(x=\"group\", y=\"delta\", data=df_delta_long, sharey=True,legend=False)       \n",
    "    \n",
    "#     for ax in g.axes[0]:\n",
    "#         ax.axhline(0, ls='--',c=\"r\")\n",
    "# #         ax.set_ylabel(\"delta\")\n",
    "#         ax.set_ylabel(r\"$\\Delta$ AUC\")\n",
    "\n",
    "#     plt.gcf().set_size_inches(6, 5)\n",
    "    \n",
    "#     plt.suptitle(title, y=1.02)\n",
    "#     plt.tight_layout()\n",
    "# #     plt.show()  \n",
    "    \n",
    "\n",
    "# def plot_scatters(results, title, minorities=False):\n",
    "#     if minorities:\n",
    "#         n_rows=2\n",
    "#         n_cols = 2    \n",
    "#         figsize=(12,5)\n",
    "#         results = get_minority_deltas(results)\n",
    "#         subgroups = [\"women\",\"black\",\"asian\",\"hispanic\"]        \n",
    "#     else:\n",
    "#         n_rows=2\n",
    "#         n_cols = 3    \n",
    "#         figsize=(12,8)\n",
    "#         results = get_deltas(results)\n",
    "#         subgroups = [\"men\",\"women\",\"white\",\"black\",\"asian\",\"hispanic\"]        \n",
    "#     fig, ax = plt.subplots(n_rows, n_cols,  figsize=figsize, sharex=True, sharey=True)\n",
    "#     #current coloramap\n",
    "#     cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "#     coords = list(itertools.product(range(n_rows),range(n_cols)))   \n",
    "    \n",
    "#     for subgroup, col, coord in zip(subgroups, cmap, coords ):        \n",
    "#         results[subgroup] = results[subgroup].abs()         \n",
    "#         results.plot.scatter(x=\"all\",y=subgroup,\n",
    "#                             color=col, ax=ax[coord[0]][coord[1]])\n",
    "#         # x = results[\"all\"]\n",
    "#         # y = results[subgroup]\n",
    "#         # z = np.polyfit(x, y, 1)\n",
    "#         # y_hat = np.poly1d(z)(x)\n",
    "#         # ax[coord[0]][coord[1]].plot(x, y_hat, c=col, lw=1)\n",
    "#         ax[coord[0]][coord[1]].set_title(subgroup)\n",
    "#         ax[coord[0]][coord[1]].set_ylabel(r\"$\\Delta$ AUC\")\n",
    "#         ax[coord[0]][coord[1]].set_xlabel(r\"AUC\")\n",
    "# #         ax[coord[0]][coord[1]].set_ylabel(\"delta\")\n",
    "#     fig.suptitle(title, y=1.02)\n",
    "#     plt.tight_layout(pad=0.1) \n",
    "\n",
    "# def get_minority_deltas(results):\n",
    "#     df = pd.DataFrame()\n",
    "#     df[\"seed\"] = results[\"seed\"]\n",
    "#     df[\"all\"] = results[\"all\"]\n",
    "#     df[\"women\"] = results[\"women\"] - results[\"men\"]\n",
    "#     #race\n",
    "#     df[\"black\"] = results[\"black\"] - results[\"white\"]\n",
    "#     df[\"hispanic\"] = results[\"hispanic\"] - results[\"white\"]\n",
    "#     df[\"asian\"] = results[\"asian\"] - results[\"white\"]\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def get_deltas(results):\n",
    "#     df = pd.DataFrame()\n",
    "#     df[\"seed\"] = results[\"seed\"]\n",
    "#     df[\"all\"] = results[\"all\"]\n",
    "#     #gender\n",
    "#     df[\"men\"] = results[\"men\"] - results[\"all\"]\n",
    "#     df[\"women\"] = results[\"women\"] - results[\"all\"]\n",
    "#     #race\n",
    "#     df[\"white\"] = results[\"white\"] - results[\"all\"]\n",
    "#     df[\"black\"] = results[\"black\"] - results[\"all\"]\n",
    "#     df[\"hispanic\"] = results[\"hispanic\"] - results[\"all\"]\n",
    "#     df[\"asian\"] = results[\"asian\"] - results[\"all\"]\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def plot_summary(tasks_fname, feature_type, results_path, metric, mini_tasks=True):\n",
    "#     dfs = []\n",
    "#     with open(tasks_fname,\"r\") as fid:                \n",
    "#         for i,l in enumerate(fid):            \n",
    "#             task_abv, task_name = l.strip(\"\\n\").split(\",\")\n",
    "#             dataset = \"mini-\"+task_abv if mini_tasks else task_abv\n",
    "#             fname = \"{}_{}_{}.csv\".format(dataset, feature_type, metric).lower()  \n",
    "#             try:\n",
    "#                 df_results = pd.read_csv(results_path+fname)                   \n",
    "#                 df_max = df_results.iloc[:,2:].max(axis=1)\n",
    "#                 df_min = df_results.iloc[:,2:].min(axis=1)\n",
    "#                 df_results[\"range\"] = df_max - df_min\n",
    "#                 df_results[\"dataset\"] = [task_abv]*len(df_results)         \n",
    "#                 dfs.append(df_results)\n",
    "#             except FileNotFoundError:\n",
    "#                 print(\"{} not found...\".format(fname))        \n",
    "\n",
    "#     dfs = pd.concat(dfs)    \n",
    "#     cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "#     aucs = sns.catplot(x=\"all\",y=\"dataset\", data=dfs, sharey=True, legend=True, \n",
    "#                     legend_out=True, height=6.5, aspect=0.85,palette=cmap)    \n",
    "#     aucs.axes[0][0].set_ylabel(r\"Task\")    \n",
    "#     aucs.axes[0][0].set_xlabel(r\"AUC\") \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"plots/tasks.pdf\",dpi=300, bbox_inches='tight')\n",
    "#     plt.show()          \n",
    "#     gaps = sns.catplot(x=\"range\",y=\"dataset\", data=dfs, sharey=True,legend=True, \n",
    "#                     legend_out=True, height=6.5, aspect=0.85,palette=cmap)        \n",
    "#     gaps.axes[0][0].set_ylabel(\"\")    \n",
    "#     gaps.axes[0][0].set_xlabel(r\"AUC gap\")    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"plots/gaps.pdf\",dpi=300, bbox_inches='tight')\n",
    "#     plt.show()      \n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()  \n",
    "    \n",
    "\n",
    "# def plot_all_underspecifications(data_path, tasks_fname, features_path, feature_type, cache_path, metric, epsilon = 0.01):\n",
    "#     with open(data_path+\"/\"+tasks_fname,\"r\") as fid:        \n",
    "#         for i,l in enumerate(fid):            \n",
    "#             task_abv, task_name = l.strip(\"\\n\").split(\",\")            \n",
    "#             f_star = underspecification(cache_path, task_abv, feature_type,  metric, epsilon)\n",
    "#             df_test = test_seeds(data_path, task_abv, features_path, feature_type, cache_path, metric, f_star)    \n",
    "#             plot_deltas(df_test,task_name,minorities=False)\n",
    "\n",
    "# def plot_underspecification(data_path, dataset, features_path, feature_type, cache_path, metric, title, epsilon = 0.01):\n",
    "#     f_star = underspecification(cache_path, dataset, feature_type,  metric, epsilon)\n",
    "#     df_test = test_seeds(data_path, dataset, features_path, feature_type, cache_path, metric, f_star)    \n",
    "#     plot_deltas(df_test,title)\n",
    "#     plt.savefig(\"underspec_{}.pdf\".format(dataset.lower()),dpi=300, bbox_inches='tight')\n",
    "#     plt.show()      \n",
    "#     return df_test\n",
    "\n",
    "# def underspecification_densities(data_path, tasks_fname, features_path, feature_type, cache_path, metric):\n",
    "#     # fig, ax = plt.subplots(1, 1,  figsize=(12,5), sharex=True, sharey=True)\n",
    "#     epsilons = [0, 0.01, 0.02]\n",
    "#     results = defaultdict(list)\n",
    "#     with open(data_path+\"/\"+tasks_fname,\"r\") as fid:                        \n",
    "#         for i,l in enumerate(fid):            \n",
    "#             task_abv, task_name = l.strip(\"\\n\").split(\",\")            \n",
    "#             for epsilon in epsilons:\n",
    "#                 f_star = underspecification(cache_path, task_abv, feature_type,  metric, epsilon)\n",
    "#                 results[epsilon] += [len(f_star)]\n",
    "                \n",
    "#     for epsilon in epsilons:\n",
    "#         x = results[epsilon]\n",
    "#         df = pd.DataFrame(x, columns=[epsilon])        \n",
    "#         df.plot.hist()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o3v3P3njsaj"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"IHM\"\n",
    "dataset=\"mini-\"+dataset\n",
    "MODEL=\"BERT-POOL\"\n",
    "OUTPUT_PATH=BASE_PATH+\"/DATA/full_runs/\"\n",
    "# Z = run_analyses(INPUT_PATH, dataset, FEATURES_PATH, MODEL, OUTPUT_PATH, OUTPUT_PATH, \n",
    "#              clear_results=False,  metric=METRIC)\n",
    "\n",
    "# plot_analyses(OUTPUT_PATH, dataset, \"In Hospital Mortality\", MODEL, metric=METRIC)\n",
    "h = search_analyses(OUTPUT_PATH, dataset, \"In Hospital Mortality\", MODEL, metric=METRIC, groups=\"gender\", k=10)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmBAlSIDFeOp",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset=\"IHM\"\n",
    "dataset=\"mini-\"+dataset\n",
    "MODEL=\"BOW-BIN\"\n",
    "OUTPUT_PATH=BASE_PATH+\"/DATA/full_runs/\"\n",
    "Z = run_analyses(INPUT_PATH, dataset, FEATURES_PATH, MODEL, OUTPUT_PATH, OUTPUT_PATH, \n",
    "             clear_results=False, metric=METRIC)\n",
    "# plot_analyses(OUTPUT_PATH, dataset, \"In Hospital Mortality\", MODEL, metric=METRIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fivMOjgs4JJI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH=BASE_PATH+\"/DATA/aftermath/\"\n",
    "#Run tasks\n",
    "CLASSIFIER=\"torch\"\n",
    "MODEL=\"CLINICALBERT-POOL\"\n",
    "run_tasks(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, OUTPUT_PATH, TMP_PATH, reset=True, \n",
    "            metric=METRIC, tuning=False, mini_tasks=True)\n",
    "plot_tasks(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=True,  metric=METRIC)\n",
    "plot_summary(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=True,  metric=METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fivMOjgs4JJI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH=BASE_PATH+\"/DATA/multiClinicalBERT_2/\"\n",
    "#Run tasks\n",
    "CLASSIFIER=\"torch\"\n",
    "MODEL=\"CLINICALMULTI-BERT-POOL\"\n",
    "# run_tasks(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, OUTPUT_PATH, TMP_PATH, reset=True, \n",
    "#             metric=METRIC, tuning=False, mini_tasks=True)\n",
    "plot_tasks(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=False,  metric=METRIC)\n",
    "plot_summary(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=False,  metric=METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXq6jZL6pnQ_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run tasks\n",
    "CLASSIFIER=\"torch\"\n",
    "OUTPUT_PATH=BASE_PATH+\"/DATA/results_newrun/\"\n",
    "MODEL=\"CLINICALBERT-POOL\"\n",
    "# run_tasks(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, OUTPUT_PATH, TMP_PATH, reset=True, \n",
    "#            metric=TUNE_METRIC, tuning=False, mini_tasks=True)\n",
    "plot_tasks(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=False,  metric=METRIC)\n",
    "# plot_summary(INPUT_PATH+\"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=False,  metric=METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inizZ4HhrKKE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RESULTS_PATH = BASE_PATH+\"/DATA/results_clinical_GRID/\"\n",
    "# MODEL=\"CLINICALBERT-POOL\"\n",
    "# TUNE_METRIC=\"auroc\"\n",
    "# plot_gridsearches(INPUT_PATH+\"tasks.txt\", RESULTS_PATH, MODEL, False, TUNE_METRIC, subgroups=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIMIC_Outcomes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}