{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIMIC_Outcomes_Torch",
      "provenance": [],
      "collapsed_sections": [
        "nIwSdPdMjsbJ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8-final"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTNg0aoYxumN"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h40mg5Ocpz2",
        "tags": []
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWBAvaw5zIe_"
      },
      "source": [
        "import sys\n",
        "# BASE_PATH = \"/content/drive/My Drive/collab/MIMIC/\"\n",
        "BASE_PATH = \"/Users/samir/Dev/projects/MIMIC/MIMIC/\"\n",
        "INPUT_PATH = BASE_PATH+\"/DATA/input/\"\n",
        "FEATURES_PATH = BASE_PATH+\"/DATA/features/\"\n",
        "OUTPUT_PATH = BASE_PATH+\"/DATA/results/\"\n",
        "TMP_PATH = BASE_PATH+\"/DATA/processed/\"\n",
        "\n",
        "TUNE_OUTPUT_PATH = BASE_PATH+\"/DATA/tune_results/\"\n",
        "TUNE_TMP_PATH = BASE_PATH+\"/DATA/tune_processed/\"\n",
        "\n",
        "\n",
        "sys.path.append(\"/content/drive/My Drive/collab/TADAT/\") \n",
        "\n",
        "#configs\n",
        "N_SEEDS=5\n",
        "N_VAL_SEEDS = 4\n",
        "N_VAL_RUNS = 4\n",
        "N_TASKS = 3\n",
        "# N_TASKS = 50\n",
        "# PLOT_VARS=[\"auroc\",\"auprc\",\"sensitivity\",\"specificity\"]\n",
        "PLOT_VARS=[\"auroc\",\"sensitivity\"]\n",
        "MODEL=\"BERT-POOL\"\n",
        "\n",
        "GROUPS = { \"GENDER\": [\"M\"],\n",
        "         \"ETHNICITY_BINARY\": [\"WHITE\",\"NON-WHITE\"],\n",
        "         \"ETHNICITY\": [\"WHITE\",\"BLACK\",\"ASIAN\",\"HISPANIC\"]\n",
        "}\n",
        "MAJORITY_GROUP = { \"GENDER\": \"M\",\n",
        "                   \"ETHNICITY_BINARY\": \"WHITE\",\n",
        "                    \"ETHNICITY\": \"WHITE\" }\n",
        "CLASSIFIER = 'sklearn'\n",
        "CLASSIFIER = 'torch'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUQ7MBldvx5d"
      },
      "source": [
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import dill\n",
        "import fnmatch\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy.random import RandomState\n",
        "import os\n",
        "from pdb import set_trace\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pprint\n",
        "import random\n",
        "from sklearn.linear_model import SGDClassifier \n",
        "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, auc, precision_recall_curve\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import torch\n",
        "import uuid\n",
        "import time\n",
        "\n",
        "#local\n",
        "from tadat.pipeline import plots\n",
        "from tadat.core import data, vectorizer, features, helpers, embeddings, berter, transformer_lms, transformer_encoders\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set(style=\"darkgrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5SjTOgye0pB"
      },
      "source": [
        "class MyLinearModel(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, loss_fn, optimizer=None, \n",
        "                 default_lr=None, init_seed=None, n_epochs=4, \n",
        "                 batch_size=None, shuffle_seed=None, silent=False, \n",
        "                 shuffle=False, device=None):\n",
        "        super().__init__()\n",
        "        if not device: self.device = get_device(silent=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle_seed = shuffle_seed\n",
        "        self.shuffle = shuffle\n",
        "        self.silent = silent\n",
        "        self.loss_fn = loss_fn\n",
        "        self.n_epochs = n_epochs\n",
        "        self.model = torch.nn.Linear(in_dim, out_dim)\n",
        "        if init_seed: \n",
        "            torch.manual_seed(init_seed)        \n",
        "            #initialize random weights\n",
        "            torch.nn.init.uniform_(self.model.weight, a=0.0, b=1.0)\n",
        "        if optimizer:\n",
        "            self.optimizer = optimizer(self.model.parameters())\n",
        "        else:\n",
        "            if default_lr:\n",
        "                self.optimizer = torch.optim.Adam(self.model.parameters(), \n",
        "                                                  lr=default_lr)\n",
        "            else:\n",
        "                self.optimizer = torch.optim.Adam(self.model.parameters())\n",
        "\n",
        "    def forward(self, in_dim, out_dim):\n",
        "        return self.model(in_dim, out_dim)\n",
        "\n",
        "    def fit(self, X_train, Y_train, X_val, Y_val):      \n",
        "        X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "        Y_train = torch.tensor(Y_train, dtype=torch.float32).reshape(-1, 1)\n",
        "        X_val = torch.from_numpy(X_val.astype(np.float32))\n",
        "        Y_val = torch.tensor(Y_val, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "        train_len = X_train.shape[0]        \n",
        "        rng = RandomState(self.shuffle_seed)        \n",
        "        if not self.batch_size:        \n",
        "            self.batch_size = train_len\n",
        "            n_batches = 1\n",
        "        else:\n",
        "            n_batches = int(train_len/self.batch_size)+1            \n",
        "        #send validation data and model to device\n",
        "        X_val_ = X_val.to(self.device) \n",
        "        Y_val_ = Y_val.to(self.device)\n",
        "        X_train_ = X_train.to(self.device)\n",
        "        Y_train_ = Y_train.to(self.device)\n",
        "        self.model = self.model.to(self.device) \n",
        "        idx = torch.tensor(rng.permutation(train_len))\n",
        "        idx_ = idx.to(self.device) \n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        val_loss_value=float('inf') \n",
        "        best_val_loss=float('inf')     \n",
        "        n_val_drops=0   \n",
        "        MAX_VAL_DROPS=10\n",
        "        loss_margin = 1e-3      \n",
        "        tmp_model_fname = str(uuid.uuid4())+\".pt\"\n",
        "        if not self.silent: print(\"[tmp: {}]\".format(tmp_model_fname))\n",
        "        for it in range(self.n_epochs):    \n",
        "            t0_epoch = time.time()\n",
        "            if self.shuffle:                                     \n",
        "                X_train_ = X_train[idx_].to(self.device)\n",
        "                Y_train_ = Y_train[idx_].to(self.device)                        \n",
        "                idx = torch.tensor(rng.permutation(train_len))\n",
        "                idx_ = idx.to(self.device) \n",
        "            for j in range(n_batches):                \n",
        "                x_train = X_train_[j*self.batch_size:(j+1)*self.batch_size, :]\n",
        "                y_train = Y_train_[j*self.batch_size:(j+1)*self.batch_size]                \n",
        "                y_hat_train = self.model(x_train)\n",
        "                train_loss = self.loss_fn(y_hat_train, y_train)                \n",
        "                train_loss_value = train_loss.item()\n",
        "                self.optimizer.zero_grad()\n",
        "                train_loss.backward()\n",
        "                self.optimizer.step()                \n",
        "                train_losses.append(train_loss_value)        \n",
        "                val_losses.append(val_loss_value)   \n",
        "            outputs_val = self.model(X_val_)\n",
        "            val_loss = self.loss_fn(outputs_val, Y_val_)      \n",
        "            val_loss_value =  val_loss.item()     \n",
        "            if val_loss_value < best_val_loss:    \n",
        "                n_val_drops=0            \n",
        "                best_val_loss = val_loss\n",
        "                #save best model\n",
        "                # print(\"[updating best model]\")\n",
        "                torch.save(self.model.state_dict(), tmp_model_fname)\n",
        "            elif val_loss_value > best_val_loss - loss_margin:                \n",
        "                n_val_drops+=1\n",
        "                # if n_val_drops == MAX_VAL_DROPS:\n",
        "                #     print(\"[early stopping: {} epochs]\".format(it))\n",
        "                    # break\n",
        "            if (it + 1) % 50 == 0 and not self.silent:\n",
        "                time_elapsed = time.time() - t0_epoch\n",
        "                print(f'[Epoch {it+1}/{self.n_epochs} | Training loss: {train_loss_value:.4f} | Val loss: {val_loss_value:.4f} | ET: {time_elapsed:.2f}]')\n",
        "        self.model.load_state_dict(torch.load(tmp_model_fname))\n",
        "        os.remove(tmp_model_fname)\n",
        "        # self.model = self.model.cpu()\n",
        "        return train_losses, val_losses  \n",
        "\n",
        "    def predict_proba(self, X):        \n",
        "        X = torch.from_numpy(X.astype(np.float32))\n",
        "        X_ = X.to(self.device)        \n",
        "        self.model = self.model.to(self.device) \n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_hat_prob = torch.nn.functional.sigmoid(self.model(X_))\n",
        "            y_hat_prob =  y_hat_prob.cpu().numpy()\n",
        "        return y_hat_prob\n",
        "\n",
        "    def predict(self, X):        \n",
        "        y_hat_prob = self.predict_proba(X)\n",
        "        threshold = 0.5 \n",
        "        y_hat = (y_hat_prob > threshold)\n",
        "        return y_hat\n",
        "\n",
        "def get_device(silent=False):\n",
        "    if torch.cuda.is_available():       \n",
        "        device = torch.device(\"cuda\")\n",
        "        if not silent:            \n",
        "            print('GPU device name:', torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        if not silent:\n",
        "            print('No GPU available, using the CPU instead.')        \n",
        "    return device\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-uckRCEvx61",
        "scrolled": true
      },
      "source": [
        "def train_classifier(X_train, Y_train, X_val, Y_val, \n",
        "                     init_seed, shuffle_seed=None, input_dimension=None):    \n",
        "    if CLASSIFIER == \"torch\":        \n",
        "        x = MyLinearModel(in_dim=input_dimension, out_dim=1, \n",
        "                    loss_fn=torch.nn.BCEWithLogitsLoss(), \n",
        "                    init_seed=init_seed, n_epochs=500, \n",
        "                    default_lr=0.001, batch_size=512, \n",
        "                    shuffle_seed=shuffle_seed, silent=True,\n",
        "                    shuffle=True) \n",
        "        x.fit(X_train, Y_train, X_val, Y_val)\n",
        "    elif CLASSIFIER == \"sklearn\":\n",
        "        x = SGDClassifier(loss=\"log\", random_state=shuffle_seed)\n",
        "        x.fit(X_train, Y_train)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return x\n",
        "\n",
        "def read_cache(path):\n",
        "    X = None\n",
        "    try:\n",
        "        with open(path, \"rb\") as fi:            \n",
        "            X = dill.load(fi)\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "    return X\n",
        "\n",
        "def write_cache(path, o):\n",
        "    dirname = os.path.dirname(path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    with open(path, \"wb\") as fo:\n",
        "        dill.dump(o, fo)\n",
        "\n",
        "def clear_cache(cache_path, model=\"*\", dataset=\"*\", group=\"*\", ctype=\"*\"):\n",
        "    assert ctype in [\"*\",\"res*\",\"feats\"]\n",
        "    file_paths = os.listdir(cache_path)\n",
        "    pattern = \"{}_{}_{}_*_{}.pkl\".format(dataset, model, group, ctype).lower()\n",
        "    for fname in file_paths:\n",
        "        if fnmatch.fnmatch(fname, pattern):\n",
        "            os.remove(cache_path+\"/\"+fname)\n",
        "            print(\"cleared file: {}\".format(fname))      \n",
        "\n",
        "def plot_densities(df, ax, title):\n",
        "    ax.set_title(title)\n",
        "    for y in PLOT_VARS:        \n",
        "        try:\n",
        "            df.plot.kde(ax=ax, x=\"seed\", y=y)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "def plot_performance(df, title):\n",
        "    #plots\n",
        "    fig, ax = plt.subplots(1,1, figsize=(18,5))\n",
        "#     plots.plot_df(df=df,ax=ax,x=\"seed\",ys=[\"auroc\",\"auprc\",\"sensitivity\",\"specificity\"], annotation_size=10)    \n",
        "    fig.suptitle(title ,y=1.02)\n",
        "    plot_densities(df, ax, \"\") \n",
        "#     ax[0].legend(loc='best')\n",
        "    ax.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_scatter_metrics(df, title):\n",
        "    n_rows=2\n",
        "    n_cols = 3\n",
        "    mets = list(itertools.combinations(PLOT_VARS,2))\n",
        "    fig, ax = plt.subplots(n_rows, n_cols,  figsize=(16,9))\n",
        "    #current coloramap\n",
        "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "    coords = list(itertools.product(range(n_rows),range(n_cols)))\n",
        "    for m,col,coord in zip(mets, cmap, coords ):\n",
        "        df.plot.scatter(x=m[0],y=m[1],c=col, ax=ax[coord[0]][coord[1]])\n",
        "    fig.suptitle(title, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()  \n",
        "\n",
        "def plot_scatter_performance(df, title):\n",
        "    n_rows=2\n",
        "    n_cols = 2\n",
        "    mets = [[x+\"_delta\", x] for x in PLOT_VARS] \n",
        "    fig, ax = plt.subplots(n_rows, n_cols,  figsize=(16,9))\n",
        "    #current coloramap\n",
        "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "    coords = list(itertools.product(range(n_rows),range(n_cols)))\n",
        "    #get absolute values for the deltas\n",
        "    for m in PLOT_VARS:\n",
        "        df[m+\"_delta\"] = df[m+\"_delta\"].abs()\n",
        "    for m,col,coord in zip(mets, cmap, coords ):\n",
        "        df.plot.scatter(x=m[0],y=m[1],c=col,\n",
        "                        ax=ax[coord[0]][coord[1]])\n",
        "    fig.suptitle(title, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "    \n",
        "def read_dataset(path, dataset_name, df_patients):    \n",
        "    df_train = pd.read_csv(\"{}/{}_train.csv\".format(path, dataset_name), \n",
        "                           sep=\"\\t\", header=0)\n",
        "    df_test  = pd.read_csv(\"{}/{}_test.csv\".format(path, dataset_name),\n",
        "                           sep=\"\\t\", header=0)\n",
        "    df_val   = pd.read_csv(\"{}/{}_val.csv\".format(path, dataset_name),\n",
        "                           sep=\"\\t\", header=0)\n",
        "    #set indices\n",
        "    df_patients.set_index(\"SUBJECT_ID\", inplace=True)\n",
        "    df_train.set_index(\"SUBJECT_ID\", inplace=True)\n",
        "    df_test.set_index(\"SUBJECT_ID\", inplace=True)\n",
        "    df_val.set_index(\"SUBJECT_ID\", inplace=True)\n",
        "\n",
        "    df_train = df_train.join(df_patients, on=\"SUBJECT_ID\", \n",
        "                             how=\"inner\", lsuffix=\"N_\").reset_index()\n",
        "    df_test = df_test.join(df_patients, on=\"SUBJECT_ID\", \n",
        "                           how=\"inner\", lsuffix=\"N_\").reset_index()\n",
        "    df_val = df_val.join(df_patients, on=\"SUBJECT_ID\", \n",
        "                         how=\"inner\", lsuffix=\"N_\").reset_index()\n",
        "\n",
        "    return df_train, df_test, df_val   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v8lR5X0cp0i"
      },
      "source": [
        "def get_features(data, vocab_size, feature_type, word_vectors=None):\n",
        "    if feature_type == \"BOW-BIN\":\n",
        "        X = features.BOW(data, vocab_size,sparse=True)\n",
        "    elif feature_type == \"BOW-FREQ\":\n",
        "        X = features.BOW_freq(data, vocab_size,sparse=True)\n",
        "    elif feature_type == \"BOE-BIN\":\n",
        "        X = features.BOE(data, word_vectors,\"bin\")\n",
        "    elif feature_type == \"BOE-SUM\": \n",
        "        X = features.BOE(data, word_vectors,\"sum\")\n",
        "    elif feature_type == \"BERT-POOL\":\n",
        "        X =  transformer_lms.transformer_encode_batches(data, batchsize=64)        \n",
        "    elif feature_type == \"BERT-CLS\":\n",
        "        X =  transformer_lms.transformer_encode_batches(data, cls_features=True,\n",
        "                                                        batchsize=64)        \n",
        "    elif feature_type == \"MULTI-BERT-POOL\":\n",
        "        X =  transformer_encoders.encode_multi_sequences(data, 10, batchsize=32,\n",
        "                                                         tmp_path=TMP_PATH)\n",
        "    elif feature_type == \"MULTI-BERT-CLS\":\n",
        "        X =  transformer_encoders.encode_multi_sequences(data, 10, \n",
        "                                                         cls_features=True,\n",
        "                                                         batchsize=64,\n",
        "                                                         tmp_path=TMP_PATH)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return X\n",
        "\n",
        "def extract_features(feature_type, path):\n",
        "    X = read_cache(path+\"feats_{}\".format(feature_type))\n",
        "    if X:\n",
        "        print(\"[reading cached features]\")\n",
        "        subject_ids, X_feats = X\n",
        "    else:\n",
        "        print(\"[computing {} features]\".format(feature_type))\n",
        "        df = pd.read_csv(path+\"patients.csv\", sep=\"\\t\", header=0)\n",
        "        subject_ids = list(df[\"SUBJECT_ID\"])\n",
        "        docs = list(df[\"TEXT\"])\n",
        "        if \"BERT\" in feature_type:\n",
        "            X_feats = get_features(docs, None, feature_type)\n",
        "        else:\n",
        "            X, word_vocab = vectorizer.docs2idx(docs)\n",
        "            X_feats = get_features(X,len(word_vocab),feature_type)\n",
        "        #save features\n",
        "        print(\"[saving features]\")\n",
        "        write_cache(path+\"feats_{}\".format(feature_type), \n",
        "                    [subject_ids, X_feats])\n",
        "    return subject_ids, X_feats\n",
        "\n",
        "\n",
        "def tune_SGD(train_X, train_Y, val_X, val_Y, label_vocab, feature_type, seeds, metric):\n",
        "    best_model = None\n",
        "    best_perf = -1\n",
        "    best_seed = None\n",
        "    runs = {}    \n",
        "    for seed in seeds:\n",
        "        model = train_classifier(train_X, train_Y,\n",
        "                                     val_X, val_Y, seed, \n",
        "                                     input_dimension=train_X.shape[1])        \n",
        "        res = evaluate_classifier(model, val_X, val_Y, \n",
        "                                  label_vocab, feature_type, seed)\n",
        "        try:\n",
        "            perf = res[metric]\n",
        "        except KeyError:\n",
        "            raise KeyError(\"Metric {} Unknown\".format(metric))\n",
        "        \n",
        "        runs[seed] = perf\n",
        "        if perf > best_perf:\n",
        "            best_perf = perf\n",
        "            best_model = model\n",
        "            best_seed = seed\n",
        "    return best_model, best_perf, best_seed, runs\n",
        "\n",
        "def tune_SGD_two_seeds(train_X, train_Y, val_X, val_Y, label_vocab, feature_type, seeds, metric):\n",
        "    best_model = None\n",
        "    best_perf = -1\n",
        "    best_seed = None\n",
        "    runs = {}    \n",
        "    #all seed pairs\n",
        "    for init_seed, shuffle_seed in itertools.product(seeds,repeat=2):        \n",
        "        seed = \"{}x{}\".format(init_seed, shuffle_seed)    \n",
        "        model = train_classifier(train_X, train_Y,\n",
        "                                     val_X, val_Y, init_seed=init_seed,\n",
        "                                     shuffle_seed=shuffle_seed, \n",
        "                                     input_dimension=train_X.shape[1])\n",
        "        \n",
        "        res = evaluate_classifier(model, val_X, val_Y, \n",
        "                                  label_vocab, feature_type, seed)\n",
        "        try:\n",
        "            perf = res[metric]\n",
        "        except KeyError:\n",
        "            raise KeyError(\"Metric {} Unknown\".format(metric))\n",
        "        \n",
        "        runs[seed] = perf\n",
        "        if perf > best_perf:\n",
        "            best_perf = perf\n",
        "            best_model = model\n",
        "            best_seed = seed\n",
        "    return best_model, best_perf, best_seed, runs\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_classifier(model, X_test, Y_test,\n",
        "                   labels, model_name, random_seed, res_path=None):\n",
        "    Y_hat = model.predict(X_test)\n",
        "    Y_hat_prob = model.predict_proba(X_test)\n",
        "    #get probabilities for the positive class\n",
        "    if CLASSIFIER == 'sklearn':\n",
        "        Y_hat_prob = Y_hat_prob[:,labels[1]]    \n",
        "    microF1 = f1_score(Y_test, Y_hat, average=\"micro\") \n",
        "    macroF1 = f1_score(Y_test, Y_hat, average=\"macro\") \n",
        "    try:\n",
        "        aurocc = roc_auc_score(Y_test, Y_hat_prob)\n",
        "    except ValueError:\n",
        "        aurocc = 0\n",
        "    try:\n",
        "        prec, rec, thresholds = precision_recall_curve(Y_test, Y_hat)\n",
        "        auprc = auc(rec, prec)\n",
        "    except ValueError:\n",
        "        auprc = 0\n",
        "    try:\n",
        "        tn, fp, fn, tp = confusion_matrix(Y_test, Y_hat).ravel()\n",
        "        specificity = tn / (tn+fp)\n",
        "        sensitivity = tp / (fn+tp)\n",
        "    except ValueError:\n",
        "        specificity, sensitivity = 0, 0\n",
        "    \n",
        "    res = {\"model\":model_name, \n",
        "            \"seed\":random_seed,    \n",
        "            \"microF1\":round(microF1,3),\n",
        "            \"macroF1\":round(macroF1,3),\n",
        "            \"auroc\":round(aurocc,3),\n",
        "            \"auprc\":round(auprc,3),\n",
        "            \"specificity\":round(specificity,3),\n",
        "            \"sensitivity\":round(sensitivity,3)           \n",
        "            }\n",
        "\n",
        "    if res_path is not None:    \n",
        "        helpers.save_results(res, res_path, sep=\"\\t\")\n",
        "    return res\n",
        "\n",
        "def vectorize_train(df_train, df_val, subject_ids, feature_matrix):\n",
        "    #vectorize labels\n",
        "    train_Y = df_train[\"Y\"]\n",
        "    val_Y = df_val[\"Y\"]           \n",
        "    \n",
        "    label_vocab = vectorizer.get_labels_vocab(train_Y+val_Y)\n",
        "    train_Y,_ = vectorizer.label2idx(train_Y, label_vocab)\n",
        "    val_Y,_ = vectorizer.label2idx(val_Y, label_vocab)\n",
        "    \n",
        "    #get the subject id indices\n",
        "    train_idxs = [subject_ids.index(i) for i in list(df_train[\"SUBJECT_ID\"])] \n",
        "    val_idxs = [subject_ids.index(i) for i in list(df_val[\"SUBJECT_ID\"])] \n",
        "    \n",
        "    #slice the feature matrix to get the corresponding instances\n",
        "    train_feats = feature_matrix[train_idxs, :]\n",
        "    val_feats = feature_matrix[val_idxs, :]    \n",
        "\n",
        "    return train_feats, train_Y, val_feats, val_Y, label_vocab\n",
        "\n",
        "def vectorize_test(df_test, subject_ids, feature_matrix, label_vocab, group_label, subgroup, subsample=False):\n",
        "    #target subgroup vs others\n",
        "    df_test_G = df_test[df_test[group_label] == subgroup]\n",
        "    df_test_O = df_test[df_test[group_label] != subgroup]    \n",
        "    \n",
        "    if subsample and MAJORITY_GROUP[group_label] == subgroup:\n",
        "        n_groups = len(GROUPS[group_label])\n",
        "        avg_len = int(len(df_test_O)/n_groups)\n",
        "        df_test_G = df_test_G.head(avg_len)\n",
        "\n",
        "    print(\"{}: {} | others: {}\".format(subgroup,\n",
        "                                       len(df_test_G),len(df_test_O)))        \n",
        "    #vectorize labels           \n",
        "    test_Y = df_test[\"Y\"]\n",
        "    test_Y_G = df_test_G[\"Y\"]\n",
        "    test_Y_O = df_test_O[\"Y\"]               \n",
        "    test_Y,_ = vectorizer.label2idx(test_Y, label_vocab)\n",
        "    test_Y_G,_ = vectorizer.label2idx(test_Y_G, label_vocab)\n",
        "    test_Y_O,_ = vectorizer.label2idx(test_Y_O, label_vocab)\n",
        "    #get the subject id indices    \n",
        "    test_idxs = [subject_ids.index(i) for i in list(df_test[\"SUBJECT_ID\"])] \n",
        "    test_idxs_G = [subject_ids.index(i) for i in list(df_test_G[\"SUBJECT_ID\"])] \n",
        "    test_idxs_O = [subject_ids.index(i) for i in list(df_test_O[\"SUBJECT_ID\"])] \n",
        "    #slice the feature matrix to get the corresponding instances    \n",
        "    test_feats = feature_matrix[test_idxs, :]\n",
        "    test_feats_G = feature_matrix[test_idxs_G, :]\n",
        "    test_feats_O = feature_matrix[test_idxs_O, :]  \n",
        "\n",
        "    return test_feats, test_Y, test_feats_G, test_Y_G, test_feats_O, test_Y_O\n",
        "\n",
        "\n",
        "def tune_run(data_path, dataset, features_path, feature_type, cache_path, subsample, metric):\n",
        "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
        "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
        "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
        "\n",
        "    subject_ids, X_feats = extract_features(feature_type, features_path)\n",
        "    train_feats, train_Y, val_feats, val_Y, label_vocab = vectorize_train(df_train, df_val, subject_ids, X_feats)\n",
        "    \n",
        "    print(\"train/test set size: {}/{}\".format(len(df_train), len(df_test)))\n",
        "    #train/test classifier for each random seed\n",
        "    random.seed(1) #ensure repeateable runs and leverage cache    \n",
        "    random_seeds = random.sample(range(0, 10000), N_VAL_SEEDS*N_VAL_RUNS)\n",
        "    results = []\n",
        "    results_g = []\n",
        "    results_o = []    \n",
        "    \n",
        "    incremental_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))  \n",
        "    test_sets = {}\n",
        "\n",
        "    for i in range(N_VAL_RUNS):        \n",
        "        res_fname = \"{}_{}_tuned_res{}.pkl\".format(dataset, feature_type, i).lower()\n",
        "        #look for cached results\n",
        "        curr_results = None\n",
        "        if cache_path: read_cache(cache_path+res_fname)              \n",
        "        if not curr_results:\n",
        "            curr_results = defaultdict(lambda: defaultdict(dict))  \n",
        "            seeds = random_seeds[i*N_VAL_SEEDS:(i+1)*N_VAL_SEEDS]\n",
        "            model, perf, seed, val_run = tune_SGD(train_feats, train_Y, val_feats, val_Y, label_vocab, \n",
        "                                            feature_type, seeds, metric)                        \n",
        "            print(\"[seed: {}| {}: {}]\".format(seed, metric, perf))    \n",
        "            #evaluate across all groups\n",
        "            for group in list(GROUPS.keys()):\n",
        "                #and subgroups\n",
        "                for subgroup in GROUPS[group]:                \n",
        "                    # print(\"TESTING {}/{}\".format(group, subgroup))\n",
        "                    try:\n",
        "                        #check if test set has already been loaded\n",
        "                        Z = test_sets[\"{}-{}\".format(group, subgroup)]\n",
        "                    except KeyError:    \n",
        "                        #load and cache test sets\n",
        "                        Z = vectorize_test(df_test, subject_ids, X_feats, label_vocab, group, subgroup, subsample) \n",
        "                        test_sets[\"{}-{}\".format(group, subgroup)] = Z\n",
        "                    #evaluate\n",
        "                    test_feats, test_Y, test_feats_G, test_Y_G, test_feats_O, test_Y_O = Z\n",
        "                    #all test examples\n",
        "                    res = evaluate_classifier(model, test_feats, test_Y, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "                    #target subgroup\n",
        "                    res_g = evaluate_classifier(model, test_feats_G, test_Y_G, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "                    #\"others\" subgroup\n",
        "                    res_o = evaluate_classifier(model, test_feats_O, test_Y_O, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "\n",
        "                    curr_results[group][subgroup][\"results\"] = res\n",
        "                    curr_results[group][subgroup][\"results_g\"] = res_g\n",
        "                    curr_results[group][subgroup][\"results_o\"] = res_o\n",
        "            #cache results\n",
        "            if cache_path: write_cache(cache_path+res_fname, curr_results)                \n",
        "        else:\n",
        "            print(\"loaded cached results | run: {}\".format(i))        \n",
        "        incremental_results = merge_results(curr_results, incremental_results)                \n",
        "    \n",
        "    #build dataframes \n",
        "    df_results = results_to_df(incremental_results)   \n",
        "\n",
        "    return df_results\n",
        "\n",
        "def run(data_path, dataset, features_path, feature_type, cache_path, subsample=False):\n",
        "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
        "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
        "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
        "\n",
        "    subject_ids, X_feats = extract_features(feature_type, features_path)\n",
        "    train_feats, train_Y, val_feats, val_Y, label_vocab = vectorize_train(df_train, df_val, subject_ids, X_feats)\n",
        "    \n",
        "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
        "    print(\"[{} classifier]\".format(CLASSIFIER))\n",
        "    #train/test classifier for each random seed\n",
        "    random.seed(1) #ensure repeateable runs and leverage cache\n",
        "    random_seeds = random.sample(range(0, 10000), N_SEEDS)\n",
        "    results = []\n",
        "    results_g = []\n",
        "    results_o = []    \n",
        "    \n",
        "    # master_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))  \n",
        "    incremental_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))  \n",
        "    test_sets = {}\n",
        "    for seed in random_seeds:        \n",
        "        res_fname = \"{}_{}_res{}.pkl\".format(dataset, feature_type, seed).lower()                \n",
        "        #look for cached results\n",
        "        curr_results = None\n",
        "        if cache_path: curr_results = read_cache(cache_path+res_fname)              \n",
        "        if not curr_results:\n",
        "            curr_results = defaultdict(lambda: defaultdict(dict))  \n",
        "            print(\" > seed: {}\".format(seed))\n",
        "            model = train_classifier(train_feats, train_Y,\n",
        "                                     val_feats, val_Y, seed, \n",
        "                                     input_dimension=train_feats.shape[1])\n",
        "            # model.fit(train_feats, train_Y)\n",
        "            #evaluate across all groups\n",
        "            for group in list(GROUPS.keys()):\n",
        "                #and subgroups\n",
        "                for subgroup in GROUPS[group]:                \n",
        "                    # print(\"TESTING {}/{}\".format(group, subgroup))\n",
        "                    try:\n",
        "                        #check if test set has already been loaded\n",
        "                        Z = test_sets[\"{}-{}\".format(group, subgroup)]\n",
        "                    except KeyError:    \n",
        "                        #load and cache test sets\n",
        "                        Z = vectorize_test(df_test, subject_ids, X_feats, \n",
        "                                           label_vocab, group, subgroup, \n",
        "                                           subsample) \n",
        "                        test_sets[\"{}-{}\".format(group, subgroup)] = Z\n",
        "                    #evaluate\n",
        "                    test_feats, test_Y, test_feats_G, test_Y_G, test_feats_O, test_Y_O = Z\n",
        "                    #all test examples\n",
        "                    res = evaluate_classifier(model, test_feats, test_Y, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "                    #target subgroup\n",
        "                    res_g = evaluate_classifier(model, test_feats_G, test_Y_G, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "                    #\"others\" subgroup\n",
        "                    res_o = evaluate_classifier(model, test_feats_O, test_Y_O, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "\n",
        "                    curr_results[group][subgroup][\"results\"] = res\n",
        "                    curr_results[group][subgroup][\"results_g\"] = res_g\n",
        "                    curr_results[group][subgroup][\"results_o\"] = res_o\n",
        "            #cache results\n",
        "            if cache_path: write_cache(cache_path+res_fname, curr_results)                \n",
        "        else:\n",
        "            print(\"loaded cached results | seed: {}\".format(seed))        \n",
        "        incremental_results = merge_results(curr_results, incremental_results)\n",
        "    #build dataframes \n",
        "    df_results = results_to_df(incremental_results)\n",
        "    return df_results\n",
        "\n",
        "def tune_run_two_seeds(data_path, dataset, features_path, feature_type, cache_path, subsample, metric):\n",
        "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
        "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
        "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
        "\n",
        "    subject_ids, X_feats = extract_features(feature_type, features_path)\n",
        "    train_feats, train_Y, val_feats, val_Y, label_vocab = vectorize_train(df_train, df_val, subject_ids, X_feats)\n",
        "    \n",
        "    print(\"train/test set size: {}/{}\".format(len(df_train), len(df_test)))\n",
        "    #train/test classifier for each random seed\n",
        "    random.seed(1) #ensure repeateable runs and leverage cache    \n",
        "    random_seeds = random.sample(range(0, 10000), N_VAL_SEEDS*N_VAL_RUNS)\n",
        "    results = []\n",
        "    results_g = []\n",
        "    results_o = []    \n",
        "    \n",
        "    incremental_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))  \n",
        "    test_sets = {}\n",
        "\n",
        "    for i in range(N_VAL_RUNS):        \n",
        "        res_fname = \"{}_{}_tuned_res{}.pkl\".format(dataset, feature_type, i).lower()\n",
        "        #look for cached results\n",
        "        curr_results = None\n",
        "        if cache_path: read_cache(cache_path+res_fname)              \n",
        "        if not curr_results:\n",
        "            curr_results = defaultdict(lambda: defaultdict(dict))  \n",
        "            seeds = random_seeds[i*N_VAL_SEEDS:(i+1)*N_VAL_SEEDS]\n",
        "            model, perf, seed, val_run = tune_SGD_two_seeds(train_feats, train_Y, val_feats, val_Y, label_vocab, \n",
        "                                            feature_type, seeds, metric)                        \n",
        "            print(\"[seed: {}| {}: {}]\".format(seed, metric, perf))    \n",
        "            #evaluate across all groups\n",
        "            for group in list(GROUPS.keys()):\n",
        "                #and subgroups\n",
        "                for subgroup in GROUPS[group]:                \n",
        "                    # print(\"TESTING {}/{}\".format(group, subgroup))\n",
        "                    try:\n",
        "                        #check if test set has already been loaded\n",
        "                        Z = test_sets[\"{}-{}\".format(group, subgroup)]\n",
        "                    except KeyError:    \n",
        "                        #load and cache test sets\n",
        "                        Z = vectorize_test(df_test, subject_ids, X_feats, label_vocab, group, subgroup, subsample) \n",
        "                        test_sets[\"{}-{}\".format(group, subgroup)] = Z\n",
        "                    #evaluate\n",
        "                    test_feats, test_Y, test_feats_G, test_Y_G, test_feats_O, test_Y_O = Z\n",
        "                    #all test examples\n",
        "                    res = evaluate_classifier(model, test_feats, test_Y, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "                    #target subgroup\n",
        "                    res_g = evaluate_classifier(model, test_feats_G, test_Y_G, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "                    #\"others\" subgroup\n",
        "                    res_o = evaluate_classifier(model, test_feats_O, test_Y_O, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "\n",
        "                    curr_results[group][subgroup][\"results\"] = res\n",
        "                    curr_results[group][subgroup][\"results_g\"] = res_g\n",
        "                    curr_results[group][subgroup][\"results_o\"] = res_o\n",
        "            #cache results\n",
        "            if cache_path: write_cache(cache_path+res_fname, curr_results)                \n",
        "        else:\n",
        "            print(\"loaded cached results | run: {}\".format(i))        \n",
        "        incremental_results = merge_results(curr_results, incremental_results)                \n",
        "    \n",
        "    #build dataframes \n",
        "    df_results = results_to_df(incremental_results)   \n",
        "\n",
        "    return df_results\n",
        "\n",
        "def run_two_seeds(data_path, dataset, features_path, feature_type, cache_path, \n",
        "                  subsample=False):\n",
        "    df_patients = pd.read_csv(features_path+\"patients.csv\", \n",
        "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
        "    df_train, df_test, df_val = read_dataset(data_path, dataset, df_patients)\n",
        "\n",
        "    subject_ids, X_feats = extract_features(feature_type, features_path)\n",
        "    train_feats, train_Y, val_feats, val_Y, label_vocab = vectorize_train(df_train, df_val, subject_ids, X_feats)\n",
        "    print(\"[two seeds analysis]\")\n",
        "    print(\"[train/test set size: {}/{}]\".format(len(df_train), len(df_test)))\n",
        "    print(\"[{} classifier]\".format(CLASSIFIER))\n",
        "    #train/test classifier for each random seed\n",
        "    random.seed(1) #ensure repeateable runs and leverage cache\n",
        "    random_seeds = random.sample(range(0, 10000), N_SEEDS)\n",
        "    results = []\n",
        "    results_g = []\n",
        "    results_o = []        \n",
        "\n",
        "    # master_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))  \n",
        "    incremental_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))  \n",
        "    test_sets = {}\n",
        "    #all seed pairs\n",
        "    for init_seed, shuffle_seed in itertools.product(random_seeds,repeat=2):        \n",
        "        seed = \"{}x{}\".format(init_seed, shuffle_seed)\n",
        "        res_fname = \"{}_{}_res{}.pkl\".format(dataset, feature_type, seed).lower()                \n",
        "        #look for cached results\n",
        "        curr_results = None\n",
        "        if cache_path: curr_results = read_cache(cache_path+res_fname)              \n",
        "        if not curr_results:\n",
        "            curr_results = defaultdict(lambda: defaultdict(dict))  \n",
        "            print(\" > seed: {}\".format(seed))\n",
        "            model = train_classifier(train_feats, train_Y,val_feats, val_Y,  \n",
        "                                     input_dimension=train_feats.shape[1],\n",
        "                                     init_seed=init_seed, \n",
        "                                     shuffle_seed=shuffle_seed)\n",
        "            # model.fit(train_feats, train_Y)\n",
        "            #evaluate across all groups\n",
        "            for group in list(GROUPS.keys()):\n",
        "                #and subgroups\n",
        "                for subgroup in GROUPS[group]:                \n",
        "                    # print(\"TESTING {}/{}\".format(group, subgroup))\n",
        "                    try:\n",
        "                        #check if test set has already been loaded\n",
        "                        Z = test_sets[\"{}-{}\".format(group, subgroup)]\n",
        "                    except KeyError:    \n",
        "                        #load and cache test sets\n",
        "                        Z = vectorize_test(df_test, subject_ids, X_feats, \n",
        "                                           label_vocab, group, subgroup,\n",
        "                                           subsample) \n",
        "                        test_sets[\"{}-{}\".format(group, subgroup)] = Z\n",
        "                    #evaluate\n",
        "                    test_feats, test_Y, test_feats_G, test_Y_G, test_feats_O, test_Y_O = Z\n",
        "                    #all test examples\n",
        "                    res = evaluate_classifier(model, test_feats, test_Y, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "                    #target subgroup\n",
        "                    res_g = evaluate_classifier(model, test_feats_G, test_Y_G, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "                    #\"others\" subgroup\n",
        "                    res_o = evaluate_classifier(model, test_feats_O, test_Y_O, \n",
        "                                                label_vocab, feature_type, seed)\n",
        "\n",
        "                    curr_results[group][subgroup][\"results\"] = res\n",
        "                    curr_results[group][subgroup][\"results_g\"] = res_g\n",
        "                    curr_results[group][subgroup][\"results_o\"] = res_o\n",
        "            #cache results\n",
        "            if cache_path: write_cache(cache_path+res_fname, curr_results)                \n",
        "        else:\n",
        "            print(\"loaded cached results | seed: {}\".format(seed))        \n",
        "        incremental_results = merge_results(curr_results, incremental_results)\n",
        "    #build dataframes \n",
        "    df_results = results_to_df(incremental_results)\n",
        "    return df_results\n",
        "\n",
        "def merge_results(curr_results, results):\n",
        "    #append results\n",
        "    for group in list(GROUPS.keys()):\n",
        "        #and subgroups\n",
        "        for subgroup in GROUPS[group]:                \n",
        "            res = curr_results[group][subgroup][\"results\"] \n",
        "            res_g = curr_results[group][subgroup][\"results_g\"] \n",
        "            res_o = curr_results[group][subgroup][\"results_o\"] \n",
        "            \n",
        "            results[group][subgroup][\"results\"].append(res)\n",
        "            results[group][subgroup][\"results_g\"].append(res_g)\n",
        "            results[group][subgroup][\"results_o\"].append(res_o)                \n",
        "    return results\n",
        "\n",
        "def results_to_df(results):\n",
        "    df_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
        "    for group in list(GROUPS.keys()):        \n",
        "        for subgroup in GROUPS[group]:\n",
        "            df_res = pd.DataFrame(results[group][subgroup][\"results\"])\n",
        "            df_res_g = pd.DataFrame(results[group][subgroup][\"results_g\"])\n",
        "            df_res_o = pd.DataFrame(results[group][subgroup][\"results_o\"])\n",
        "            df_res_delta = df_res_g.sub(df_res_o.iloc[:,2:])\n",
        "            df_res_delta[\"model\"] = df_res_g[\"model\"]\n",
        "            df_res_delta[\"seed\"] = df_res_g[\"seed\"]   \n",
        "\n",
        "            df_results[group][subgroup][\"results\"] = df_res\n",
        "            df_results[group][subgroup][\"results_g\"] = df_res_g\n",
        "            df_results[group][subgroup][\"results_o\"] = df_res_o\n",
        "            df_results[group][subgroup][\"delta\"] = df_res_delta\n",
        "    return df_results\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0Qg14_rmeG8"
      },
      "source": [
        "# subject_idsm, X = extract_features(\"MULTI-BERT-POOL\", FEATURES_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTyzgoy4vx64"
      },
      "source": [
        "# Analyses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVpc3ZQ-jsaV"
      },
      "source": [
        "#Run All the tasks\n",
        "def run_tasks(data_path, tasks_fname, features_path, feature_type, results_path, cache_path,  \n",
        "             reset=False, tune_metric=None, subsample=False, mini_tasks=True):\n",
        "    #if reset delete the completed tasks file\n",
        "    if reset: reset_tasks(cache_path)    \n",
        "    with open(data_path+tasks_fname,\"r\") as fid:\n",
        "        for i,l in enumerate(fid):\n",
        "            if i > N_TASKS: break\n",
        "            fname, task_name = l.strip(\"\\n\").split(\",\")            \n",
        "            dataset = \"mini-\"+fname if mini_tasks else fname\n",
        "            # dataset = fname\n",
        "            if is_task_done(cache_path, dataset): \n",
        "                print(\"[dataset: {} already processed]\".format(dataset))\n",
        "                continue                        \n",
        "            print(\"******** {} {} ********\".format(task_name, dataset))      \n",
        "            run_analyses(data_path, dataset, features_path, feature_type, results_path, \n",
        "                         cache_path, clear_results=False, tune_metric=tune_metric, \n",
        "                         subsample=subsample)\n",
        "            task_done(cache_path, dataset)\n",
        "\n",
        "def task_done(path,  task):\n",
        "    dirname = os.path.dirname(path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    with open(path+\"completed_tasks.txt\", \"a\") as fod:\n",
        "        fod.write(task+\"\\n\")\n",
        "\n",
        "def reset_tasks(path):\n",
        "    dirname = os.path.dirname(path)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    with open(path+\"completed_tasks.txt\", \"w\") as fod:\n",
        "        fod.write(\"\")\n",
        "\n",
        "def is_task_done(path,  task):\n",
        "    try:\n",
        "        with open(path+\"completed_tasks.txt\", \"r\") as fid:\n",
        "            tasks = fid.read().split(\"\\n\")            \n",
        "        return task in set(tasks)\n",
        "    except FileNotFoundError:\n",
        "        #create file if not found\n",
        "        dirname = os.path.dirname(path)\n",
        "        if not os.path.exists(dirname):\n",
        "            os.makedirs(dirname)\n",
        "        with open(path+\"completed_tasks.txt\", \"w\") as fid:\n",
        "            fid.write(\"\")\n",
        "        return False\n",
        "\n",
        "def plot_tasks(data_path, tasks_fname, feature_type, results_path, mini_tasks=True, tune_metric=None):\n",
        "    with open(data_path+tasks_fname,\"r\") as fid:        \n",
        "        for i,l in enumerate(fid):            \n",
        "            fname, task_name = l.strip(\"\\n\").split(\",\")\n",
        "            dataset = \"mini-\"+fname if mini_tasks else fname\n",
        "            plot_analyses(results_path, dataset, feature_type, task_name, tune_metric)\n",
        "\n",
        "def run_analyses(data_path, dataset, features_path, feature_type, results_path, \n",
        "                 cache_path, clear_results=False, tune_metric=None, subsample=False, \n",
        "                 plots=False):    \n",
        "\n",
        "    if clear_results:\n",
        "        clear_cache(cache_path, model=feature_type, dataset=dataset, ctype=\"res*\")\n",
        "    if tune_metric:\n",
        "        df_results = tune_run_two_seeds(data_path, dataset, features_path, feature_type, cache_path, subsample, tune_metric)  \n",
        "    else:\n",
        "        df_results = run_two_seeds(data_path, dataset, features_path, feature_type, cache_path, subsample)                  \n",
        "    process_gender(df_results[\"GENDER\"], dataset, feature_type, results_path, tune_metric, plots)\n",
        "    process_ethnicity_binary(df_results[\"ETHNICITY_BINARY\"], dataset, feature_type, results_path, \n",
        "                             tune_metric, plots=plots)\n",
        "    process_ethnicity(df_results[\"ETHNICITY\"], dataset, feature_type, results_path, tune_metric, plots=plots)\n",
        "\n",
        "def plot_analyses(cache_path, dataset, feature_type, title, tune_metric=None):\n",
        "    file_paths = os.listdir(cache_path)\n",
        "    if tune_metric:\n",
        "        pattern = \"{}_{}_*_all_tuned_res.pkl\".format(dataset, feature_type).lower()\n",
        "    else:        \n",
        "        pattern = \"{}_{}_*_all_res.pkl\".format(dataset, feature_type).lower()\n",
        "    print(\"\\n\\n{} {} {}\\n\".format(\"*\"*30, title, \"*\"*30))\n",
        "    for fname in file_paths:\n",
        "        if fnmatch.fnmatch(fname, pattern):\n",
        "            R = list(read_cache(cache_path+fname))\n",
        "            if \"gender\" in fname:\n",
        "                gender_plots(*R)\n",
        "                print(\"-\"*100)\n",
        "            elif \"ethnicity_binary\" in fname:\n",
        "                ethnicity_binary_plots(*R)                \n",
        "                print(\"-\"*100)\n",
        "            elif \"ethnicity\" in fname:\n",
        "                ethnicity_plots(*R)\n",
        "                print(\"-\"*100)\n",
        "    print(\"*\"*100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6CiKv4xEp8"
      },
      "source": [
        "## Ethnicity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNxNa35Cvx65"
      },
      "source": [
        "def ethnicity_plot_deltas(df_delta_W,df_delta_N,df_delta_A,df_delta_H, title):\n",
        "    df_delta = pd.concat([df_delta_W,df_delta_N,df_delta_A,df_delta_H])    \n",
        "    #transform results into \"long format\"\n",
        "    df_delta_long = df_delta.melt(id_vars=[\"seed\",\"model\",\"group\"], \n",
        "                                  value_vars=PLOT_VARS, \n",
        "                                  var_name=\"metric\", value_name=\"delta\")\n",
        "    g = sns.catplot(x=\"metric\", y=\"delta\", data=df_delta_long, \n",
        "                    col=\"group\",sharey=True,legend=False)\n",
        "    ax1, ax2, ax3, ax4 = g.axes[0]\n",
        "    ax1.axhline(0, ls='--',c=\"r\")\n",
        "    ax2.axhline(0, ls='--',c=\"r\")\n",
        "    ax3.axhline(0, ls='--',c=\"r\")\n",
        "    ax4.axhline(0, ls='--',c=\"r\")\n",
        "    lim = max(df_delta_long[\"delta\"].abs()) + 0.05\n",
        "    ax1.set_ylim([-lim,lim])\n",
        "    ax2.set_ylim([-lim,lim])\n",
        "    ax3.set_ylim([-lim,lim])\n",
        "    ax4.set_ylim([-lim,lim])\n",
        "    plt.suptitle(title, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()  \n",
        "\n",
        "def ethnicity_plot_densities(df_W, df_N, df_A, df_H, title):\n",
        "    #plots\n",
        "    fig, ax = plt.subplots(1,4, sharey=True, sharex=True, figsize=(18,5))\n",
        "    plot_densities(df_W, ax[0], \"White\")\n",
        "    plot_densities(df_N, ax[1], \"Black\")\n",
        "    plot_densities(df_A, ax[2], \"Asian\")\n",
        "    plot_densities(df_H, ax[3], \"Hispanic\")\n",
        "    fig.suptitle(title,  y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def ethnicity_plots(df_res, df_res_W, df_res_N, df_res_A, df_res_H, df_res_delta_W, \n",
        "                      df_res_delta_N,df_res_delta_A, df_res_delta_H, title):\n",
        "    plot_performance(df_res, title)\n",
        "    ethnicity_plot_densities(df_res_W,df_res_N, \n",
        "                             df_res_A,df_res_H,title)\n",
        "    ethnicity_plot_deltas(df_res_delta_W, df_res_delta_N,\n",
        "                          df_res_delta_A,df_res_delta_H, title)\n",
        "\n",
        "def process_ethnicity(df_results, dataset, feature_type, results_path, tune_metric=None, plots=False):\n",
        "\n",
        "    df_res = df_results[\"WHITE\"][\"results\"] \n",
        "    df_res_W = df_results[\"WHITE\"][\"results_g\"] \n",
        "    df_res_W_O = df_results[\"WHITE\"][\"results_o\"] \n",
        "    df_res_delta_W = df_results[\"WHITE\"][\"delta\"] \n",
        "\n",
        "    df_res_N = df_results[\"BLACK\"][\"results_g\"] \n",
        "    df_res_N_O = df_results[\"BLACK\"][\"results_o\"] \n",
        "    df_res_delta_N = df_results[\"BLACK\"][\"delta\"] \n",
        "\n",
        "    df_res_H = df_results[\"HISPANIC\"][\"results_g\"] \n",
        "    df_res_H_O = df_results[\"HISPANIC\"][\"results_o\"] \n",
        "    df_res_delta_H = df_results[\"HISPANIC\"][\"delta\"] \n",
        "\n",
        "    df_res_A = df_results[\"ASIAN\"][\"results_g\"] \n",
        "    df_res_A_O = df_results[\"ASIAN\"][\"results_o\"] \n",
        "    df_res_delta_A = df_results[\"ASIAN\"][\"delta\"] \n",
        "    \n",
        "    df_res_delta_W[\"group\"] = [\"White v Others\"]*len(df_res_delta_W)\n",
        "    df_res_delta_N[\"group\"] = [\"Black v Others\"]*len(df_res_delta_N)\n",
        "    df_res_delta_A[\"group\"] = [\"Asian v Others\"]*len(df_res_delta_A)\n",
        "    df_res_delta_H[\"group\"] = [\"Hispanic v Others\"]*len(df_res_delta_H)\n",
        "    if tune_metric:\n",
        "        title=\"{} x ethnicity x {} ({} tuned)\".format(dataset, feature_type, tune_metric).lower()        \n",
        "        fname = \"{}_{}_ethnicity_all_tuned_res.pkl\".format(dataset, \n",
        "                                                     feature_type).lower()\n",
        "    else:    \n",
        "        title=\"{} x ethnicity x {}\".format(dataset, feature_type).lower()        \n",
        "        fname = \"{}_{}_ethnicity_all_res.pkl\".format(dataset, \n",
        "                                                     feature_type).lower()    \n",
        "    #save results\n",
        "    if not os.path.exists(results_path): os.makedirs(results_path)    \n",
        "    with open(results_path+fname, \"wb\") as fo:\n",
        "        pickle.dump([df_res, df_res_W, df_res_N, df_res_A, df_res_H, df_res_delta_W, \n",
        "                     df_res_delta_N,df_res_delta_A, df_res_delta_H, title], fo)\n",
        "    if plots:\n",
        "        ethnicity_plots(df_res, df_res_W, df_res_N, df_res_A, df_res_H, \n",
        "                          df_res_delta_W, df_res_delta_N,df_res_delta_A, df_res_delta_H, title)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFe5X5pWxEp3"
      },
      "source": [
        "## Ethnicity Binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIQe7-IPxEp4"
      },
      "source": [
        "def ethnicity_binary_plot_deltas(df_delta_W,df_delta_N, title):\n",
        "    df_delta = pd.concat([df_delta_W,df_delta_N])    \n",
        "    #transform results into \"long format\"\n",
        "    df_delta_long = df_delta.melt(id_vars=[\"seed\",\"model\",\"group\"], \n",
        "                                  value_vars=PLOT_VARS, var_name=\"metric\", \n",
        "                                  value_name=\"delta\")\n",
        "\n",
        "    g = sns.catplot(x=\"metric\", y=\"delta\", data=df_delta_long, \n",
        "                    col=\"group\",sharey=True,legend=False)\n",
        "    ax1, ax2 = g.axes[0]\n",
        "    ax1.axhline(0, ls='--',c=\"r\")\n",
        "    ax2.axhline(0, ls='--',c=\"r\")\n",
        "    lim = max(df_delta_long[\"delta\"].abs()) + 0.05\n",
        "    ax1.set_ylim([-lim,lim])\n",
        "    ax2.set_ylim([-lim,lim])\n",
        "    plt.suptitle(title, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()  \n",
        "    \n",
        "def ethnicity_binary_plot_densities(df_W, df_N, title):\n",
        "    #plots\n",
        "    fig, ax = plt.subplots(1,2, sharey=True, sharex=True, figsize=(18,5))\n",
        "    plot_densities(df_W, ax[0], \"White\")\n",
        "    plot_densities(df_N, ax[1], \"Non-White\")\n",
        "    fig.suptitle(title ,y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def ethnicity_binary_plots(df_res, df_res_W, df_res_N, df_res_delta_W,\n",
        "                           df_res_delta_N, title):\n",
        "    plot_performance(df_res, title)\n",
        "    ethnicity_binary_plot_densities(df_res_W,df_res_N,title)\n",
        "    ethnicity_binary_plot_deltas(df_res_delta_W, df_res_delta_N, title)\n",
        "    \n",
        "def process_ethnicity_binary(df_results, dataset, feature_type, results_path, tune_metric=None, plots=False):\n",
        "    df_res = df_results[\"WHITE\"][\"results\"] \n",
        "    df_res_W = df_results[\"WHITE\"][\"results_g\"] \n",
        "    df_res_W_O = df_results[\"WHITE\"][\"results_o\"] \n",
        "    df_res_delta_W = df_results[\"WHITE\"][\"delta\"] \n",
        "\n",
        "    df_res = df_results[\"NON-WHITE\"][\"results\"] \n",
        "    df_res_N = df_results[\"NON-WHITE\"][\"results_g\"] \n",
        "    df_res_N_O = df_results[\"NON-WHITE\"][\"results_o\"] \n",
        "    df_res_delta_N = df_results[\"NON-WHITE\"][\"delta\"] \n",
        "\n",
        "    df_res_delta_W[\"group\"] = [\"White v Others\"]*len(df_res_delta_W)\n",
        "    df_res_delta_N[\"group\"] = [\"Non-White v Others\"]*len(df_res_delta_N)\n",
        "    \n",
        "    if tune_metric:\n",
        "        title=\"{} x ethnicity-binary x {} ({} tuned)\".format(dataset, feature_type, tune_metric).lower()\n",
        "        fname = \"{}_{}_ethnicity_binary_all_tuned_res.pkl\".format(dataset, feature_type).lower()\n",
        "    else:        \n",
        "        title=\"{} x ethnicity-binary x {}\".format(dataset, feature_type).lower()\n",
        "        fname = \"{}_{}_ethnicity_binary_all_res.pkl\".format(dataset, feature_type).lower()\n",
        "    #save results\n",
        "    if not os.path.exists(results_path): os.makedirs(results_path)    \n",
        "    with open(results_path+fname, \"wb\") as fo:\n",
        "        pickle.dump([df_res, df_res_W, df_res_N, df_res_delta_W, \n",
        "                     df_res_delta_N, title], fo)\n",
        "\n",
        "    if plots:\n",
        "        ethnicity_binary_plots(df_res, df_res_W, df_res_N, \n",
        "                               df_res_delta_W, df_res_delta_N,title)   \n",
        "                            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLmcgnVHxEpo"
      },
      "source": [
        "## Gender "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SvZXhVwxEpq"
      },
      "source": [
        "def gender_plot_deltas(df_delta, title):\n",
        "    #transform results into \"long format\"\n",
        "    df_delta_long = df_delta.melt(id_vars=[\"seed\",\"model\"], value_vars=PLOT_VARS, \n",
        "                                        var_name=\"metric\", value_name=\"delta\")\n",
        "    \n",
        "    lim = max(df_delta_long[\"delta\"].abs()) + 0.05\n",
        "    g = sns.catplot(x=\"metric\", y=\"delta\",  data=df_delta_long,\n",
        "                    sharey=True,legend=False)\n",
        "    ax1 = g.axes[0][0]\n",
        "    ax1.axhline(0, ls='--',c=\"r\")\n",
        "    ax1.set_ylim([-lim,lim])\n",
        "    plt.suptitle(title,y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()  \n",
        "\n",
        "def gender_plot_densities(df_M, df_F, title):\n",
        "    #plots\n",
        "    fig, ax = plt.subplots(1,2, sharey=True, sharex=True, figsize=(18,5))\n",
        "    plot_densities(df_M, ax[0], \"Male\") \n",
        "    plot_densities(df_F, ax[1], \"Female\") \n",
        "    fig.suptitle(title, y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def gender_plots(df_res, df_res_M, df_res_F, df_res_delta, title):\n",
        "    plot_performance(df_res, title)\n",
        "    gender_plot_densities(df_res_M, df_res_F, title)\n",
        "    gender_plot_deltas(df_res_delta, title)    \n",
        "\n",
        "def process_gender(df_results, dataset, feature_type, results_path, tune_metric=None, plots=False):\n",
        "\n",
        "    df_res = df_results[\"M\"][\"results\"] \n",
        "    df_res_M = df_results[\"M\"][\"results_g\"] \n",
        "    df_res_F = df_results[\"M\"][\"results_o\"] \n",
        "    df_res_delta = df_results[\"M\"][\"delta\"] \n",
        "    if tune_metric:\n",
        "        title=\"{} x gender x {} ({} tuned)\".format(dataset, feature_type, tune_metric).lower()\n",
        "        fname = \"{}_{}_gender_all_tuned_res.pkl\".format(dataset, feature_type).lower()\n",
        "    else:\n",
        "        title=\"{} x gender x {}\".format(dataset, feature_type).lower()\n",
        "        fname = \"{}_{}_gender_all_res.pkl\".format(dataset, feature_type).lower()\n",
        "    #save results\n",
        "    if not os.path.exists(results_path): os.makedirs(results_path)    \n",
        "    with open(results_path+fname, \"wb\") as fo:\n",
        "        pickle.dump([df_res, df_res_M, df_res_F, df_res_delta, title], fo)        \n",
        "    if plots:\n",
        "        gender_plots(df_res, df_res_M, df_res_F, df_res_delta, title)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o3v3P3njsaj"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmBAlSIDFeOp",
        "scrolled": false,
        "tags": [],
        "outputId": "6466b146-c430-42b1-9e82-0c15cd0c59d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dataset=\"CD\"\n",
        "dataset=\"mini-\"+dataset\n",
        "CLASSIFIER=\"torch\"\n",
        "run_analyses(INPUT_PATH, dataset, FEATURES_PATH, MODEL, OUTPUT_PATH, None, clear_results=False, tune_metric=\"auroc\", subsample=False, plots=True)\n",
        "# plot_analyses(OUTPUT_PATH, dataset, MODEL, dataset, tune_metric=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxyq5bgZ3qMm"
      },
      "source": [
        "CLASSIFIER=\"sklearn\"\n",
        "# run_analyses(INPUT_PATH, dataset, FEATURES_PATH, MODEL, OUTPUT_PATH, None, clear_results=False, tune_metric=None, subsample=False, plots=True)\n",
        "plot_analyses(OUTPUT_PATH, dataset, MODEL, dataset, tune_metric=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXq6jZL6pnQ_",
        "tags": []
      },
      "source": [
        "#Run tasks\n",
        "run_tasks(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, OUTPUT_PATH, TMP_PATH, reset=True, tune_metric=None, subsample=False, mini_tasks=True)\n",
        "plot_tasks(INPUT_PATH, \"tasks.txt\", MODEL, OUTPUT_PATH, mini_tasks=True, tune_metric=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "42ZEeQaCjsbA"
      },
      "source": [
        "#Run tasks with subsampling\n",
        "res_path = BASE_PATH+\"/DATA/results_sub/\"\n",
        "cache_path = BASE_PATH+\"/DATA/processed_sub/\"\n",
        "run_tasks(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, res_path, cache_path, reset=True, tune_metric=None, subsample=True, mini_tasks=True)\n",
        "plot_tasks(INPUT_PATH, \"tasks.txt\", MODEL, res_path, mini_tasks=True, tune_metric=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "2ySito54jsbH"
      },
      "source": [
        "#Run tasks with tuning\n",
        "TUNE_METRIC = \"auprc\"\n",
        "run_tasks(INPUT_PATH, \"tasks.txt\", FEATURES_PATH, MODEL, TUNE_TMP_PATH, TUNE_OUTPUT_PATH, mini_tasks=True, reset=True, tune_metric=TUNE_METRIC)\n",
        "# plot_tasks(INPUT_PATH, \"tasks.txt\", MODEL, TUNE_OUTPUT_PATH, mini_tasks=True, tune_metric=TUNE_METRIC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIwSdPdMjsbJ"
      },
      "source": [
        "# Scatter Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E0FjK3WjsbK"
      },
      "source": [
        "def metric_scatter_plots(cache_path, dataset, feature_type, title):\n",
        "    file_paths = os.listdir(cache_path)\n",
        "    pattern = \"{}_{}_*_all_res.pkl\".format(dataset, feature_type).lower()\n",
        "    print(\"\\n\\n{} {} {}\\n\".format(\"*\"*30, title, \"*\"*30))\n",
        "    for fname in file_paths:\n",
        "        if fnmatch.fnmatch(fname, pattern):\n",
        "            if \"gender\" in fname:\n",
        "                df_res, df_res_M, df_res_F, df_res_delta, title = list(read_cache(cache_path+fname))\n",
        "                plot_scatter_metrics(df_res_delta, title)\n",
        "                print(\"-\"*100)\n",
        "            elif \"ethnicity_binary\" in fname:\n",
        "                df_res, df_res_W, df_res_N, df_res_delta_W, df_res_delta_N, title = list(read_cache(cache_path+fname))\n",
        "                plot_scatter_metrics(df_res_delta_W, title + \" (White)\")\n",
        "                plot_scatter_metrics(df_res_delta_N, title + \" (Non-White)\")\n",
        "                print(\"-\"*100)\n",
        "            elif \"ethnicity\" in fname:\n",
        "                R = list(read_cache(cache_path+fname))\n",
        "                df_res, df_res_W, df_res_N, df_res_A, df_res_H, df_res_delta_W, df_res_delta_N,df_res_delta_A, df_res_delta_H, title = R \n",
        "                plot_scatter_metrics(df_res_delta_W, title + \" (White)\")\n",
        "                plot_scatter_metrics(df_res_delta_N, title + \" (Black)\")\n",
        "                plot_scatter_metrics(df_res_delta_H, title + \" (Hispanic)\")\n",
        "                plot_scatter_metrics(df_res_delta_A, title + \" (Asian)\")\n",
        "                print(\"-\"*100)\n",
        "            \n",
        "    print(\"*\"*100)\n",
        "\n",
        "def performance_scatter_plots(cache_path, dataset, feature_type, title):\n",
        "    file_paths = os.listdir(cache_path)\n",
        "    pattern = \"{}_{}_*_all_res.pkl\".format(dataset, feature_type).lower()\n",
        "    print(\"\\n\\n{} {} {}\\n\".format(\"*\"*30, title, \"*\"*30))\n",
        "    for fname in file_paths:\n",
        "        if fnmatch.fnmatch(fname, pattern):\n",
        "            if \"gender\" in fname:\n",
        "                df_res, df_res_M, df_res_F, df_res_delta, title = list(read_cache(cache_path+fname))\n",
        "                df = df_res.merge(df_res_delta, on=[\"model\",\"seed\"],\n",
        "                                  suffixes=[None, \"_delta\"])\n",
        "                plot_scatter_performance(df, title)\n",
        "                print(\"-\"*100)\n",
        "            elif \"ethnicity_binary\" in fname:\n",
        "                df_res, df_res_W, df_res_N, df_res_delta_W, df_res_delta_N, title = list(read_cache(cache_path+fname))\n",
        "                df_W = df_res_W.merge(df_res_delta_W, on=[\"model\",\"seed\"], \n",
        "                                      suffixes=[None, \"_delta\"])\n",
        "                df_N = df_res_N.merge(df_res_delta_N, on=[\"model\",\"seed\"],\n",
        "                                      suffixes=[None, \"_delta\"])\n",
        "                plot_scatter_performance(df_W, title + \" (White)\")\n",
        "                plot_scatter_performance(df_N, title + \" (Non-White)\")\n",
        "                print(\"-\"*100)\n",
        "            elif \"ethnicity\" in fname:\n",
        "                R = list(read_cache(cache_path+fname))\n",
        "                df_res, df_res_W, df_res_N, df_res_A, df_res_H, df_res_delta_W, df_res_delta_N, df_res_delta_A, df_res_delta_H, title = R \n",
        "                \n",
        "                df_W = df_res_W.merge(df_res_delta_W, on=[\"model\",\"seed\"],\n",
        "                                      suffixes=[None, \"_delta\"])\n",
        "                df_N = df_res_N.merge(df_res_delta_N, on=[\"model\",\"seed\"], \n",
        "                                      suffixes=[None, \"_delta\"])\n",
        "                df_H = df_res_H.merge(df_res_delta_H, on=[\"model\",\"seed\"], \n",
        "                                      suffixes=[None, \"_delta\"])\n",
        "                df_A = df_res_A.merge(df_res_delta_A, on=[\"model\",\"seed\"],\n",
        "                                      suffixes=[None, \"_delta\"])\n",
        "                \n",
        "                plot_scatter_performance(df_W, title + \" (White)\")\n",
        "                plot_scatter_performance(df_N, title + \" (Black)\")\n",
        "                plot_scatter_performance(df_H, title + \" (Hispanic)\")\n",
        "                plot_scatter_performance(df_A, title + \" (Black)\")                \n",
        "                print(\"-\"*100)\n",
        "            \n",
        "    print(\"*\"*100) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orPpmC3ljsbM"
      },
      "source": [
        "## Metrix x Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uTF5D3E3q8G",
        "scrolled": true
      },
      "source": [
        "#Plot all the metric scatter plots\n",
        "with open(INPUT_PATH+\"tasks.txt\",\"r\") as fid:\n",
        "    for i,l in enumerate(fid):\n",
        "        fname, task_name = l.strip(\"\\n\").split(\",\")\n",
        "        # dataset = \"mini-\"+fname\n",
        "        dataset = fname\n",
        "        metric_scatter_plots(OUTPUT_PATH, dataset, MODEL, task_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w19fNF9DjsbP"
      },
      "source": [
        "## Performance x Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9HdBeLH1jsbP"
      },
      "source": [
        "#Plot all the performance scatter plots\n",
        "with open(INPUT_PATH+\"tasks.txt\",\"r\") as fid:\n",
        "    for i,l in enumerate(fid):\n",
        "        fname, task_name = l.strip(\"\\n\").split(\",\")\n",
        "        # dataset = \"mini-\"+fname\n",
        "        dataset = fname\n",
        "        performance_scatter_plots(OUTPUT_PATH, dataset, MODEL, task_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}