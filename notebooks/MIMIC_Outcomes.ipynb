{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "id": "nTNg0aoYxumN",
    "outputId": "7a2a721e-68e7-4a52-e6f4-a38fa729dbc9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4h40mg5Ocpz2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWBAvaw5zIe_"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# BASE_PATH = \"/content/drive/My Drive/collab/MIMIC/\"\n",
    "BASE_PATH = \"/Users/samir/Dev/projects/MIMIC/MIMIC/\"\n",
    "input_path = BASE_PATH+\"/DATA/input/\"\n",
    "output_path = BASE_PATH+\"/DATA/results/\"\n",
    "tmp_path = BASE_PATH+\"/DATA/processed/\"\n",
    "sys.path.append(BASE_PATH+\"TADAT/\") \n",
    "N_SEEDS=50\n",
    "N_VAL_SEEDS = 3\n",
    "N_VAL_RUNS = 3\n",
    "PLOT_VARS=[\"auroc\",\"auprc\",\"sensitivity\",\"specificity\"]\n",
    "model=\"BERT-POOL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "vUQ7MBldvx5d",
    "outputId": "4e3f099c-1480-415e-e1bd-937f534411fb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import fnmatch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pdb import set_trace\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, auc, precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n",
    "#local\n",
    "from tadat.pipeline import plots\n",
    "from tadat.core import data, vectorizer, features, helpers, embeddings, berter, transformer_lms\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-uckRCEvx61",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_cache(path):\n",
    "    X = None\n",
    "    try:\n",
    "        with open(path, \"rb\") as fi:            \n",
    "            X = pickle.load(fi)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def write_cache(path, o):\n",
    "    with open(path, \"wb\") as fo:\n",
    "        pickle.dump(o, fo)\n",
    "\n",
    "def clear_cache(cache_path, model=\"*\", dataset=\"*\", group=\"*\", ctype=\"*\"):\n",
    "    assert ctype in [\"*\",\"res*\",\"feats\"]\n",
    "    file_paths = os.listdir(cache_path)\n",
    "    pattern = \"{}_{}_{}_*_{}.pkl\".format(dataset, model, group, ctype).lower()\n",
    "    for fname in file_paths:\n",
    "        if fnmatch.fnmatch(fname, pattern):\n",
    "            os.remove(cache_path+\"/\"+fname)\n",
    "            print(\"cleared file: {}\".format(fname))      \n",
    "\n",
    "def plot_densities(df, ax, title):\n",
    "    ax.set_title(title)\n",
    "    for y in PLOT_VARS:        \n",
    "        try:\n",
    "            df.plot.kde(ax=ax, x=\"seed\", y=y)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "def plot_performance(df, title):\n",
    "    #plots\n",
    "    fig, ax = plt.subplots(1,1, figsize=(18,5))\n",
    "#     plots.plot_df(df=df,ax=ax,x=\"seed\",ys=[\"auroc\",\"auprc\",\"sensitivity\",\"specificity\"], annotation_size=10)    \n",
    "    fig.suptitle(title ,y=1.02)\n",
    "    plot_densities(df, ax, \"\") \n",
    "#     ax[0].legend(loc='best')\n",
    "    ax.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_scatter_metrics(df, title):\n",
    "    n_rows=2\n",
    "    n_cols = 3\n",
    "    mets = list(itertools.combinations(PLOT_VARS,2))\n",
    "    fig, ax = plt.subplots(n_rows, n_cols,  figsize=(16,9))\n",
    "    #current coloramap\n",
    "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    coords = list(itertools.product(range(n_rows),range(n_cols)))\n",
    "    for m,col,coord in zip(mets, cmap, coords ):\n",
    "        df.plot.scatter(x=m[0],y=m[1],c=col, ax=ax[coord[0]][coord[1]])\n",
    "    fig.suptitle(title, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "\n",
    "def plot_scatter_performance(df, title):\n",
    "    n_rows=2\n",
    "    n_cols = 2\n",
    "    mets = [[x+\"_delta\", x] for x in PLOT_VARS] \n",
    "    fig, ax = plt.subplots(n_rows, n_cols,  figsize=(16,9))\n",
    "    #current coloramap\n",
    "    cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    coords = list(itertools.product(range(n_rows),range(n_cols)))\n",
    "    #get absolute values for the deltas\n",
    "    for m in PLOT_VARS:\n",
    "        df[m+\"_delta\"] = df[m+\"_delta\"].abs()\n",
    "    for m,col,coord in zip(mets, cmap, coords ):\n",
    "        df.plot.scatter(x=m[0],y=m[1],c=col,\n",
    "                        ax=ax[coord[0]][coord[1]])\n",
    "    fig.suptitle(title, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "\n",
    "    \n",
    "def read_dataset(path, dataset_name):\n",
    "    df_patients = pd.read_csv(path+\"patients.csv\", \n",
    "                              sep=\"\\t\", header=0).drop(columns=[\"TEXT\"])\n",
    "    df_train = pd.read_csv(\"{}/{}_train.csv\".format(path, dataset_name), \n",
    "                           sep=\"\\t\", header=0)\n",
    "    df_test  = pd.read_csv(\"{}/{}_test.csv\".format(path, dataset_name),\n",
    "                           sep=\"\\t\", header=0)\n",
    "    df_val   = pd.read_csv(\"{}/{}_val.csv\".format(path, dataset_name),\n",
    "                           sep=\"\\t\", header=0)\n",
    "    #set indices\n",
    "    df_patients.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "    df_train.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "    df_test.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "    df_val.set_index(\"SUBJECT_ID\", inplace=True)\n",
    "\n",
    "    df_train = df_train.join(df_patients, on=\"SUBJECT_ID\", \n",
    "                             how=\"inner\", lsuffix=\"N_\").reset_index()\n",
    "    df_test = df_test.join(df_patients, on=\"SUBJECT_ID\", \n",
    "                           how=\"inner\", lsuffix=\"N_\").reset_index()\n",
    "    df_val = df_val.join(df_patients, on=\"SUBJECT_ID\", \n",
    "                         how=\"inner\", lsuffix=\"N_\").reset_index()\n",
    "\n",
    "    return df_train, df_test, df_val    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6v8lR5X0cp0i"
   },
   "outputs": [],
   "source": [
    "def get_features(data, vocab_size, feature_type, word_vectors=None):\n",
    "    if feature_type == \"BOW-BIN\":\n",
    "        X = features.BOW(data, vocab_size,sparse=True)\n",
    "    elif feature_type == \"BOW-FREQ\":\n",
    "        X = features.BOW_freq(data, vocab_size,sparse=True)\n",
    "    elif feature_type == \"BOE-BIN\":\n",
    "        X = features.BOE(data, word_vectors,\"bin\")\n",
    "    elif feature_type == \"BOE-SUM\": \n",
    "        X = features.BOE(data, word_vectors,\"sum\")\n",
    "    elif feature_type == \"BERT-POOL\":\n",
    "        X_cls, X_pool =  transformer_lms.transformer_encode_batches(data, \n",
    "                                                                    batchsize=64, \n",
    "                                                                    device=\"cuda\")\n",
    "        X=X_pool\n",
    "    elif feature_type == \"BERT-CLS\":\n",
    "        X_cls, X_pool =  transformer_lms.transformer_encode_batches(data, \n",
    "                                                                    batchsize=64, \n",
    "                                                                    device=\"cuda\")\n",
    "        X=X_cls\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return X\n",
    "\n",
    "def extract_features(feature_type, input_path, cache_path):\n",
    "    X = read_cache(cache_path+\"feats_{}\".format(feature_type))\n",
    "    if X:\n",
    "        print(\"[reading cached features]\")\n",
    "        subject_ids, X_feats = X\n",
    "    else:\n",
    "        df = pd.read_csv(input_path+\"patients.csv\", sep=\"\\t\", header=0)\n",
    "        subject_ids = list(df[\"SUBJECT_ID\"])\n",
    "        docs = list(df[\"TEXT\"])\n",
    "        if \"BERT\" in feature_type:\n",
    "            X_feats = get_features(docs, None, feature_type)\n",
    "        else:\n",
    "            X, word_vocab = vectorizer.docs2idx(docs)\n",
    "            X_feats = get_features(X,len(word_vocab),feature_type)\n",
    "        write_cache(cache_path+\"feats_{}\".format(feature_type), \n",
    "                    [subject_ids, X_feats])\n",
    "    return subject_ids, X_feats\n",
    "\n",
    "def vectorize(df_train, df_val, df_test, subject_ids, features, group_label, subgroup):\n",
    "    #target subgroup vs others\n",
    "    df_test_G = df_test[df_test[group_label] == subgroup]\n",
    "    df_test_O = df_test[df_test[group_label] != subgroup]    \n",
    "    print(\"{}: {} | others: {}\".format(subgroup,\n",
    "                                       len(df_test_G),len(df_test_O)))        \n",
    "    #vectorize labels\n",
    "    train_Y = df_train[\"Y\"]\n",
    "    val_Y = df_val[\"Y\"]           \n",
    "    test_Y = df_test[\"Y\"]\n",
    "    test_Y_G = df_test_G[\"Y\"]\n",
    "    test_Y_O = df_test_O[\"Y\"]           \n",
    "    label_vocab = vectorizer.get_labels_vocab(train_Y+test_Y+val_Y)\n",
    "    train_Y,_ = vectorizer.label2idx(train_Y, label_vocab)\n",
    "    val_Y,_ = vectorizer.label2idx(val_Y, label_vocab)\n",
    "    test_Y,_ = vectorizer.label2idx(test_Y, label_vocab)\n",
    "    test_Y_G,_ = vectorizer.label2idx(test_Y_G, label_vocab)\n",
    "    test_Y_O,_ = vectorizer.label2idx(test_Y_O, label_vocab)\n",
    "    #get the subject id indices\n",
    "    train_idxs = [subject_ids.index(i) for i in list(df_train[\"SUBJECT_ID\"])] \n",
    "    val_idxs = [subject_ids.index(i) for i in list(df_val[\"SUBJECT_ID\"])] \n",
    "    test_idxs = [subject_ids.index(i) for i in list(df_test[\"SUBJECT_ID\"])] \n",
    "    test_idxs_G = [subject_ids.index(i) for i in list(df_test_G[\"SUBJECT_ID\"])] \n",
    "    test_idxs_O = [subject_ids.index(i) for i in list(df_test_O[\"SUBJECT_ID\"])] \n",
    "    #slice the feature matrix to get the corresponding instances\n",
    "    train_feats = features[train_idxs, :]\n",
    "    val_feats = features[val_idxs, :]\n",
    "    test_feats = features[test_idxs, :]\n",
    "    test_feats_G = features[test_idxs_G, :]\n",
    "    test_feats_O = features[test_idxs_O, :]  \n",
    "\n",
    "    return train_feats, train_Y, val_feats, val_Y, test_feats, test_Y, \\\n",
    "           test_feats_G, test_Y_G, test_feats_O, test_Y_O, label_vocab\n",
    "\n",
    "def run(data_path, dataset, feature_type, group_label, subgroup, cache_path=None):\n",
    "   \n",
    "    df_train, df_test, df_val = read_dataset(data_path, dataset)\n",
    "    subject_ids, X_feats = extract_features(feature_type, input_path, cache_path)\n",
    "    X = vectorize(df_train, df_val, df_test, subject_ids, X_feats, group_label, subgroup)\n",
    "    train_feats, train_Y, val_feats, val_Y, test_feats, test_Y, test_feats_G, test_Y_G, test_feats_O, test_Y_O, label_vocab = X\n",
    "    print(\"train/test set size: {}/{}\".format(train_feats.shape[0], test_feats.shape[0]))\n",
    "    #train/test classifier for each random seed\n",
    "    random.seed(1) #ensure repeateable runs and leverage cache\n",
    "    random_seeds = random.sample(range(0, 10000), N_SEEDS)\n",
    "    results = []\n",
    "    results_g = []\n",
    "    results_o = []    \n",
    "    for seed in random_seeds:        \n",
    "        res_fname = \"{}_{}_{}_{}_res{}.pkl\".format(dataset, feature_type, \n",
    "                                                   group_label, subgroup, \n",
    "                                                   seed).lower()\n",
    "        R=None\n",
    "        #look for cached results\n",
    "        if cache_path: R = read_cache(cache_path+res_fname)      \n",
    "        \n",
    "        if not R:\n",
    "            model = SGDClassifier(loss=\"log\", random_state=seed)\n",
    "            model.fit(train_feats, train_Y)\n",
    "            res = evaluate_classifier(model, test_feats, test_Y, \n",
    "                                      label_vocab, feature_type, seed)\n",
    "            res_g = evaluate_classifier(model, test_feats_G, test_Y_G, \n",
    "                                        label_vocab, feature_type, seed)\n",
    "            res_o = evaluate_classifier(model, test_feats_O, test_Y_O, \n",
    "                                        label_vocab, feature_type, seed)\n",
    "            #cache results\n",
    "            if cache_path: write_cache(cache_path+res_fname, [res, res_g, res_o])                \n",
    "        else:\n",
    "            print(\"loaded cached results | seed: {}\".format(seed))\n",
    "            res, res_g, res_o = R\n",
    "        results.append(res)\n",
    "        results_g.append(res_g)\n",
    "        results_o.append(res_o)\n",
    "    #dataframes \n",
    "    df_res = pd.DataFrame(results)    \n",
    "    df_res_g = pd.DataFrame(results_g)\n",
    "    df_res_o = pd.DataFrame(results_o)\n",
    "\n",
    "    df_res_delta = df_res_g.sub(df_res_o.iloc[:,2:])\n",
    "    df_res_delta[\"model\"] = df_res_g[\"model\"]\n",
    "    df_res_delta[\"seed\"] = df_res_g[\"seed\"]   \n",
    "\n",
    "    return df_res, df_res_g, df_res_o, df_res_delta #  results, results_g, results_o\n",
    "\n",
    "def tune_SGD(train_X, train_Y, val_X, val_Y, label_vocab, feature_type, seeds, metric):\n",
    "    best_model = None\n",
    "    best_perf = 0\n",
    "    runs = {}\n",
    "    \n",
    "    for seed in seeds:\n",
    "        model = SGDClassifier(loss=\"log\", random_state=seed)\n",
    "        model.fit(train_X, train_Y)\n",
    "        res = evaluate_classifier(model, val_X, val_Y, \n",
    "                                  label_vocab, feature_type, seed)\n",
    "        perf = res[metric]\n",
    "        runs[seed] = perf\n",
    "        if perf > best_perf:\n",
    "            best_perf = perf\n",
    "            best_model = model\n",
    "    return best_model, best_perf, runs\n",
    "    \n",
    "def run_tuning(data_path, dataset, feature_type, group_label, subgroup, cache_path=None):   \n",
    "    df_train, df_test, df_val = read_dataset(data_path, dataset)\n",
    "    subject_ids, X_feats = extract_features(feature_type, input_path, cache_path)\n",
    "    X = vectorize(df_train, df_val, df_test, subject_ids, X_feats, group_label, subgroup)\n",
    "    train_X, train_Y, val_X, val_Y, test_X, test_Y, test_X_G, test_Y_G, test_X_O, test_Y_O, label_vocab = X\n",
    "    print(\"train/test set size: {}/{}\".format(train_X.shape[0], test_X.shape[0]))\n",
    "    #train/test classifier for each random seed\n",
    "\n",
    "    random.seed(1) #ensure repeateable runs and leverage cache\n",
    "    random_seeds = random.sample(range(0, 10000), N_VAL_SEEDS*N_VAL_RUNS)\n",
    "    results = []\n",
    "    results_g = []\n",
    "    results_o = []    \n",
    "    val_runs = []\n",
    "    \n",
    "    for i in range(N_VAL_RUNS):\n",
    "        res_fname = \"{}_{}_{}_{}_tuned_res{}.pkl\".format(dataset, feature_type, \n",
    "                                                         group_label, subgroup, i).lower()\n",
    "        R=None\n",
    "        #look for cached results\n",
    "        if cache_path: R = read_cache(cache_path+res_fname)                      \n",
    "        if not R:\n",
    "            seeds = random_seeds[i*N_VAL_SEEDS:(i+1)*N_VAL_SEEDS]\n",
    "            model, perf, val_run = tune_SGD(train_X, train_Y, val_X, val_Y, label_vocab, \n",
    "                                            feature_type, seeds, \"sensitivity\")\n",
    "            print(\"tuning\")\n",
    "            pprint.pprint(val_run)\n",
    "            res = evaluate_classifier(model, test_X, test_Y, \n",
    "                                      label_vocab, feature_type, i)\n",
    "            res_g = evaluate_classifier(model, test_X_G, test_Y_G, \n",
    "                                        label_vocab, feature_type, i)\n",
    "            res_o = evaluate_classifier(model, test_X_O, test_Y_O, \n",
    "                                        label_vocab, feature_type, i)\n",
    "            #cache results\n",
    "            if cache_path: write_cache(cache_path+res_fname, [res, res_g, res_o])                \n",
    "        else:\n",
    "            print(\"loaded cached results | run: {}\".format(i))\n",
    "            res, res_g, res_o = R\n",
    "        results.append(res)\n",
    "        results_g.append(res_g)\n",
    "        results_o.append(res_o)\n",
    "    #dataframes \n",
    "    df_res = pd.DataFrame(results)    \n",
    "    df_res_g = pd.DataFrame(results_g)\n",
    "    df_res_o = pd.DataFrame(results_o)\n",
    "\n",
    "    df_res_delta = df_res_g.sub(df_res_o.iloc[:,2:])\n",
    "    df_res_delta[\"model\"] = df_res_g[\"model\"]\n",
    "    df_res_delta[\"seed\"] = df_res_g[\"seed\"]   \n",
    "\n",
    "    return df_res, df_res_g, df_res_o, df_res_delta \n",
    "\n",
    "\n",
    "def evaluate_classifier(model, X_test, Y_test,\n",
    "                   labels, model_name, random_seed, res_path=None):\n",
    "    Y_hat = model.predict(X_test)\n",
    "    Y_hat_prob = model.predict_proba(X_test)\n",
    "    #get probabilities for the positive class\n",
    "    Y_hat_prob = Y_hat_prob[:,labels[1]]    \n",
    "    microF1 = f1_score(Y_test, Y_hat, average=\"micro\") \n",
    "    macroF1 = f1_score(Y_test, Y_hat, average=\"macro\") \n",
    "    try:\n",
    "        aurocc = roc_auc_score(Y_test, Y_hat_prob)\n",
    "    except ValueError:\n",
    "        aurocc = 0\n",
    "    try:\n",
    "        prec, rec, thresholds = precision_recall_curve(Y_test, Y_hat)\n",
    "        auprc = auc(rec, prec)\n",
    "    except ValueError:\n",
    "        auprc = 0\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_test, Y_hat).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "        sensitivity = tp / (fn+tp)\n",
    "    except ValueError:\n",
    "        specificity, sensitivity = 0, 0\n",
    "    \n",
    "    res = {\"model\":model_name, \n",
    "            \"seed\":random_seed,    \n",
    "            \"microF1\":round(microF1,3),\n",
    "            \"macroF1\":round(macroF1,3),\n",
    "            \"auroc\":round(aurocc,3),\n",
    "            \"auprc\":round(auprc,3),\n",
    "            \"specificity\":round(specificity,3),\n",
    "            \"sensitivity\":round(sensitivity,3)           \n",
    "            }\n",
    "\n",
    "    if res_path is not None:    \n",
    "        helpers.save_results(res, res_path, sep=\"\\t\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTyzgoy4vx64"
   },
   "source": [
    "# Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz6CiKv4xEp8"
   },
   "source": [
    "## Ethnicity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNxNa35Cvx65"
   },
   "outputs": [],
   "source": [
    "def ethnicity_plot_deltas(df_delta_W,df_delta_N,df_delta_A,df_delta_H, title):\n",
    "    df_delta = pd.concat([df_delta_W,df_delta_N,df_delta_A,df_delta_H])    \n",
    "    #transform results into \"long format\"\n",
    "    df_delta_long = df_delta.melt(id_vars=[\"seed\",\"model\",\"group\"], \n",
    "                                  value_vars=PLOT_VARS, \n",
    "                                  var_name=\"metric\", value_name=\"delta\")\n",
    "    g = sns.catplot(x=\"metric\", y=\"delta\", data=df_delta_long, \n",
    "                    col=\"group\",sharey=True,legend=False)\n",
    "    ax1, ax2, ax3, ax4 = g.axes[0]\n",
    "    ax1.axhline(0, ls='--',c=\"r\")\n",
    "    ax2.axhline(0, ls='--',c=\"r\")\n",
    "    ax3.axhline(0, ls='--',c=\"r\")\n",
    "    ax4.axhline(0, ls='--',c=\"r\")\n",
    "    lim = max(df_delta_long[\"delta\"].abs()) + 0.05\n",
    "    ax1.set_ylim([-lim,lim])\n",
    "    ax2.set_ylim([-lim,lim])\n",
    "    ax3.set_ylim([-lim,lim])\n",
    "    ax4.set_ylim([-lim,lim])\n",
    "    plt.suptitle(title, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "\n",
    "def ethnicity_plot_densities(df_W, df_N, df_A, df_H, title):\n",
    "    #plots\n",
    "    fig, ax = plt.subplots(1,4, sharey=True, sharex=True, figsize=(18,5))\n",
    "    plot_densities(df_W, ax[0], \"White\")\n",
    "    plot_densities(df_N, ax[1], \"Black\")\n",
    "    plot_densities(df_A, ax[2], \"Asian\")\n",
    "    plot_densities(df_H, ax[3], \"Hispanic\")\n",
    "    fig.suptitle(title,  y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def ethnicity_plots(df_res, df_res_W, df_res_N, df_res_A, df_res_H, df_res_delta_W, \n",
    "                      df_res_delta_N,df_res_delta_A, df_res_delta_H, title):\n",
    "    plot_performance(df_res, title)\n",
    "    ethnicity_plot_densities(df_res_W,df_res_N, \n",
    "                             df_res_A,df_res_H,title)\n",
    "    ethnicity_plot_deltas(df_res_delta_W, df_res_delta_N,\n",
    "                          df_res_delta_A,df_res_delta_H, title)\n",
    "\n",
    "\n",
    "def ethnicity_analysis(data_path, dataset, feature_type, output_path, cache_path=None, plots=True, tune=False):\n",
    "    if tune:\n",
    "        df_res, df_res_W, df_res_W_O, df_res_delta_W = run_tuning(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY\", \"WHITE\", \n",
    "                                                  cache_path=cache_path)\n",
    "        _, df_res_N, df_res_N_O, df_res_delta_N = run_tuning(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY\", \"BLACK\", \n",
    "                                                  cache_path=cache_path)\n",
    "        _, df_res_A, df_res_A_O, df_res_delta_A  = run_tuning(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY\", \"ASIAN\", \n",
    "                                                  cache_path=cache_path)\n",
    "        _, df_res_H, df_res_H_O, df_res_delta_H  = run_tuning(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY\",\"HISPANIC\", \n",
    "                                                  cache_path=cache_path)\n",
    "    else:\n",
    "        df_res, df_res_W, df_res_W_O, df_res_delta_W = run(data_path, dataset, feature_type, \n",
    "                                              \"ETHNICITY\", \"WHITE\", \n",
    "                                              cache_path=cache_path)\n",
    "        _, df_res_N, df_res_N_O, df_res_delta_N = run(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY\", \"BLACK\", \n",
    "                                                  cache_path=cache_path)\n",
    "        _, df_res_A, df_res_A_O, df_res_delta_A  = run(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY\", \"ASIAN\", \n",
    "                                                  cache_path=cache_path)\n",
    "        _, df_res_H, df_res_H_O, df_res_delta_H = run(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY\",\"HISPANIC\", \n",
    "                                                  cache_path=cache_path)\n",
    "    df_res_delta_W[\"group\"] = [\"White v Others\"]*len(df_res_delta_W)\n",
    "    df_res_delta_N[\"group\"] = [\"Black v Others\"]*len(df_res_delta_N)\n",
    "    df_res_delta_A[\"group\"] = [\"Asian v Others\"]*len(df_res_delta_A)\n",
    "    df_res_delta_H[\"group\"] = [\"Hispanic v Others\"]*len(df_res_delta_H)\n",
    "    if tune:\n",
    "        title=\"{} x ethnicity x {} (tuned)\".format(dataset, feature_type).lower()        \n",
    "        fname = \"{}_{}_ethnicity_all_tuned_res.pkl\".format(dataset, \n",
    "                                                     feature_type).lower()\n",
    "    else:\n",
    "        #save results\n",
    "        title=\"{} x ethnicity x {}\".format(dataset, feature_type).lower()        \n",
    "        fname = \"{}_{}_ethnicity_all_res.pkl\".format(dataset, \n",
    "                                                     feature_type).lower()\n",
    "    with open(output_path+fname, \"wb\") as fo:\n",
    "        pickle.dump([df_res, df_res_W, df_res_N, df_res_A, df_res_H, df_res_delta_W, \n",
    "                     df_res_delta_N,df_res_delta_A, df_res_delta_H, title], fo)\n",
    "    if plots:\n",
    "        ethnicity_plots(df_res, df_res_W, df_res_N, df_res_A, df_res_H, \n",
    "                          df_res_delta_W, df_res_delta_N,df_res_delta_A, df_res_delta_H, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFe5X5pWxEp3"
   },
   "source": [
    "## Ethnicity Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIQe7-IPxEp4"
   },
   "outputs": [],
   "source": [
    "def ethnicity_binary_plot_deltas(df_delta_W,df_delta_N, title):\n",
    "    df_delta = pd.concat([df_delta_W,df_delta_N])    \n",
    "    #transform results into \"long format\"\n",
    "    df_delta_long = df_delta.melt(id_vars=[\"seed\",\"model\",\"group\"], \n",
    "                                  value_vars=PLOT_VARS, var_name=\"metric\", \n",
    "                                  value_name=\"delta\")\n",
    "\n",
    "    g = sns.catplot(x=\"metric\", y=\"delta\", data=df_delta_long, \n",
    "                    col=\"group\",sharey=True,legend=False)\n",
    "    ax1, ax2 = g.axes[0]\n",
    "    ax1.axhline(0, ls='--',c=\"r\")\n",
    "    ax2.axhline(0, ls='--',c=\"r\")\n",
    "    lim = max(df_delta_long[\"delta\"].abs()) + 0.05\n",
    "    ax1.set_ylim([-lim,lim])\n",
    "    ax2.set_ylim([-lim,lim])\n",
    "    plt.suptitle(title, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "    \n",
    "def ethnicity_binary_plot_densities(df_W, df_N, title):\n",
    "    #plots\n",
    "    fig, ax = plt.subplots(1,2, sharey=True, sharex=True, figsize=(18,5))\n",
    "    plot_densities(df_W, ax[0], \"White\")\n",
    "    plot_densities(df_N, ax[1], \"Non-White\")\n",
    "    fig.suptitle(title ,y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def ethnicity_binary_plots(df_res, df_res_W, df_res_N, df_res_delta_W,\n",
    "                           df_res_delta_N, title):\n",
    "    plot_performance(df_res, title)\n",
    "    ethnicity_binary_plot_densities(df_res_W,df_res_N,title)\n",
    "    ethnicity_binary_plot_deltas(df_res_delta_W, df_res_delta_N, title)\n",
    "    \n",
    "def ethnicity_binary_analysis(data_path, dataset, feature_type, output_path,\n",
    "                              cache_path=None, plots=True, tune=False):\n",
    "    if tune:        \n",
    "        df_res, df_res_W, df_res_W_O, df_res_delta_W = run_tuning(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY_BINARY\", \n",
    "                                                  \"WHITE\", cache_path=cache_path)\n",
    "        df_res, df_res_N, df_res_N_O, df_res_delta_N = run_tuning(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY_BINARY\", \n",
    "                                                  \"NON-WHITE\", cache_path=cache_path)\n",
    "    else:\n",
    "        df_res, df_res_W, df_res_W_O, df_res_delta_W = run(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY_BINARY\", \n",
    "                                                  \"WHITE\", cache_path=cache_path)\n",
    "        df_res, df_res_N, df_res_N_O, df_res_delta_N = run(data_path, dataset, feature_type, \n",
    "                                                  \"ETHNICITY_BINARY\", \n",
    "                                                  \"NON-WHITE\", cache_path=cache_path)       \n",
    "        \n",
    "    df_res_delta_W[\"group\"] = [\"White v Others\"]*len(df_res_delta_W)\n",
    "    df_res_delta_N[\"group\"] = [\"Non-White v Others\"]*len(df_res_delta_N)\n",
    "    \n",
    "    if tune:\n",
    "        title=\"{} x ethnicity-binary x {} (tuned)\".format(dataset, feature_type).lower()\n",
    "        fname = \"{}_{}_ethnicity_binary_all_tuned_res.pkl\".format(dataset, feature_type).lower()\n",
    "    else:\n",
    "        #save results\n",
    "        title=\"{} x ethnicity-binary x {}\".format(dataset, feature_type).lower()\n",
    "        fname = \"{}_{}_ethnicity_binary_all_res.pkl\".format(dataset, feature_type).lower()\n",
    "    with open(output_path+fname, \"wb\") as fo:\n",
    "        pickle.dump([df_res, df_res_W, df_res_N, df_res_delta_W, \n",
    "                     df_res_delta_N, title], fo)\n",
    "\n",
    "    if plots:\n",
    "        ethnicity_binary_plots(df_res, df_res_W, df_res_N, \n",
    "                               df_res_delta_W, df_res_delta_N,title)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLmcgnVHxEpo"
   },
   "source": [
    "## Gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SvZXhVwxEpq"
   },
   "outputs": [],
   "source": [
    "def gender_plot_deltas(df_delta, title):\n",
    "    #transform results into \"long format\"\n",
    "    df_delta_long = df_delta.melt(id_vars=[\"seed\",\"model\"], value_vars=PLOT_VARS, \n",
    "                                        var_name=\"metric\", value_name=\"delta\")\n",
    "    \n",
    "    lim = max(df_delta_long[\"delta\"].abs()) + 0.05\n",
    "    g = sns.catplot(x=\"metric\", y=\"delta\",  data=df_delta_long,\n",
    "                    sharey=True,legend=False)\n",
    "    ax1 = g.axes[0][0]\n",
    "    ax1.axhline(0, ls='--',c=\"r\")\n",
    "    ax1.set_ylim([-lim,lim])\n",
    "    plt.suptitle(title,y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "\n",
    "def gender_plot_densities(df_M, df_F, title):\n",
    "    #plots\n",
    "    fig, ax = plt.subplots(1,2, sharey=True, sharex=True, figsize=(18,5))\n",
    "    plot_densities(df_M, ax[0], \"Male\") \n",
    "    plot_densities(df_F, ax[1], \"Female\") \n",
    "    fig.suptitle(title, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def gender_plots(df_res, df_res_M, df_res_F, df_res_delta, title):\n",
    "    plot_performance(df_res, title)\n",
    "    gender_plot_densities(df_res_M, df_res_F, title)\n",
    "    gender_plot_deltas(df_res_delta, title)    \n",
    "\n",
    "def gender_analysis(data_path, dataset, feature_type, output_path, cache_path=None, plots=True, tune=False):\n",
    "    if tune:\n",
    "        df_res, df_res_M, df_res_F, df_res_delta = run_tuning(data_path, dataset, feature_type, \n",
    "                                            \"GENDER\", \"M\", cache_path=cache_path)\n",
    "    else:\n",
    "        df_res, df_res_M, df_res_F, df_res_delta = run(data_path, dataset, feature_type, \n",
    "                                            \"GENDER\", \"M\", cache_path=cache_path)\n",
    "    #save results\n",
    "    if tune:\n",
    "        title=\"{} x gender x {} (tuned)\".format(dataset, feature_type).lower()\n",
    "        fname = \"{}_{}_gender_all_tuned_res.pkl\".format(dataset, feature_type).lower()\n",
    "    else:\n",
    "        title=\"{} x gender x {}\".format(dataset, feature_type).lower()\n",
    "        fname = \"{}_{}_gender_all_res.pkl\".format(dataset, feature_type).lower()\n",
    "    with open(output_path+fname, \"wb\") as fo:\n",
    "        pickle.dump([df_res, df_res_M, df_res_F, df_res_delta, title], fo)        \n",
    "\n",
    "    if plots:\n",
    "        gender_plots(df_res, df_res_M, df_res_F, df_res_delta, title)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zei3lT6kE8mV"
   },
   "outputs": [],
   "source": [
    "def run_analyses(data_path, dataset, feature_type, output_path, \n",
    "                 cache_path, clear_results=False, tune=False):    \n",
    "\n",
    "    if clear_results:\n",
    "        clear_cache(cache_path, model=model, dataset=dataset, ctype=\"res*\")\n",
    "    \n",
    "    gender_analysis(input_path, dataset, model, output_path, \n",
    "                    cache_path, plots=False, tune=tune)\n",
    "\n",
    "    ethnicity_binary_analysis(input_path, dataset, model, output_path, \n",
    "                              cache_path, plots=False, tune=tune)\n",
    "    \n",
    "    ethnicity_analysis(input_path, dataset, model, output_path, \n",
    "                       cache_path, plots=False, tune=tune)\n",
    "\n",
    "def plot_analyses(cache_path, dataset, model, title, tuned=False):\n",
    "    file_paths = os.listdir(cache_path)\n",
    "    if tuned:\n",
    "        pattern = \"{}_{}_*_all_tuned_res.pkl\".format(dataset, model).lower()\n",
    "    else:        \n",
    "        pattern = \"{}_{}_*_all_res.pkl\".format(dataset, model).lower()\n",
    "    print(\"\\n\\n{} {} {}\\n\".format(\"*\"*30, title, \"*\"*30))\n",
    "    for fname in file_paths:\n",
    "        if fnmatch.fnmatch(fname, pattern):\n",
    "            R = list(read_cache(cache_path+fname))\n",
    "            if \"gender\" in fname:\n",
    "                gender_plots(*R)\n",
    "                print(\"-\"*100)\n",
    "            elif \"ethnicity_binary\" in fname:\n",
    "                ethnicity_binary_plots(*R)                \n",
    "                print(\"-\"*100)\n",
    "            elif \"ethnicity\" in fname:\n",
    "                ethnicity_plots(*R)\n",
    "                print(\"-\"*100)\n",
    "    print(\"*\"*100)\n",
    "\n",
    "def metric_scatter_plots(cache_path, dataset, model, title):\n",
    "    file_paths = os.listdir(cache_path)\n",
    "    pattern = \"{}_{}_*_all_res.pkl\".format(dataset, model).lower()\n",
    "    print(\"\\n\\n{} {} {}\\n\".format(\"*\"*30, title, \"*\"*30))\n",
    "    for fname in file_paths:\n",
    "        if fnmatch.fnmatch(fname, pattern):\n",
    "            if \"gender\" in fname:\n",
    "                df_res, df_res_M, df_res_F, df_res_delta, title = list(read_cache(cache_path+fname))\n",
    "                plot_scatter_metrics(df_res_delta, title)\n",
    "                print(\"-\"*100)\n",
    "            elif \"ethnicity_binary\" in fname:\n",
    "                df_res, df_res_W, df_res_N, df_res_delta_W, df_res_delta_N, title = list(read_cache(cache_path+fname))\n",
    "                plot_scatter_metrics(df_res_delta_W, title + \" (White)\")\n",
    "                plot_scatter_metrics(df_res_delta_N, title + \" (Non-White)\")\n",
    "                print(\"-\"*100)\n",
    "            elif \"ethnicity\" in fname:\n",
    "                R = list(read_cache(cache_path+fname))\n",
    "                df_res, df_res_W, df_res_N, df_res_A, df_res_H, df_res_delta_W, df_res_delta_N,df_res_delta_A, df_res_delta_H, title = R \n",
    "                plot_scatter_metrics(df_res_delta_W, title + \" (White)\")\n",
    "                plot_scatter_metrics(df_res_delta_N, title + \" (Black)\")\n",
    "                plot_scatter_metrics(df_res_delta_H, title + \" (Hispanic)\")\n",
    "                plot_scatter_metrics(df_res_delta_A, title + \" (Asian)\")\n",
    "                print(\"-\"*100)\n",
    "            \n",
    "    print(\"*\"*100)\n",
    "\n",
    "def performance_scatter_plots(cache_path, dataset, model, title):\n",
    "    file_paths = os.listdir(cache_path)\n",
    "    pattern = \"{}_{}_*_all_res.pkl\".format(dataset, model).lower()\n",
    "    print(\"\\n\\n{} {} {}\\n\".format(\"*\"*30, title, \"*\"*30))\n",
    "    for fname in file_paths:\n",
    "        if fnmatch.fnmatch(fname, pattern):\n",
    "            if \"gender\" in fname:\n",
    "                df_res, df_res_M, df_res_F, df_res_delta, title = list(read_cache(cache_path+fname))\n",
    "                df = df_res.merge(df_res_delta, on=[\"model\",\"seed\"],\n",
    "                                  suffixes=[None, \"_delta\"])\n",
    "                plot_scatter_performance(df, title)\n",
    "                print(\"-\"*100)\n",
    "            elif \"ethnicity_binary\" in fname:\n",
    "                df_res, df_res_W, df_res_N, df_res_delta_W, df_res_delta_N, title = list(read_cache(cache_path+fname))\n",
    "                df_W = df_res_W.merge(df_res_delta_W, on=[\"model\",\"seed\"], \n",
    "                                      suffixes=[None, \"_delta\"])\n",
    "                df_N = df_res_N.merge(df_res_delta_N, on=[\"model\",\"seed\"],\n",
    "                                      suffixes=[None, \"_delta\"])\n",
    "                plot_scatter_performance(df_W, title + \" (White)\")\n",
    "                plot_scatter_performance(df_N, title + \" (Non-White)\")\n",
    "                print(\"-\"*100)\n",
    "            elif \"ethnicity\" in fname:\n",
    "                R = list(read_cache(cache_path+fname))\n",
    "                df_res, df_res_W, df_res_N, df_res_A, df_res_H, df_res_delta_W, df_res_delta_N, df_res_delta_A, df_res_delta_H, title = R \n",
    "                \n",
    "                df_W = df_res_W.merge(df_res_delta_W, on=[\"model\",\"seed\"],\n",
    "                                      suffixes=[None, \"_delta\"])\n",
    "                df_N = df_res_N.merge(df_res_delta_N, on=[\"model\",\"seed\"], \n",
    "                                      suffixes=[None, \"_delta\"])\n",
    "                df_H = df_res_H.merge(df_res_delta_H, on=[\"model\",\"seed\"], \n",
    "                                      suffixes=[None, \"_delta\"])\n",
    "                df_A = df_res_A.merge(df_res_delta_A, on=[\"model\",\"seed\"],\n",
    "                                      suffixes=[None, \"_delta\"])\n",
    "                \n",
    "                plot_scatter_performance(df_W, title + \" (White)\")\n",
    "                plot_scatter_performance(df_N, title + \" (Black)\")\n",
    "                plot_scatter_performance(df_H, title + \" (Hispanic)\")\n",
    "                plot_scatter_performance(df_A, title + \" (Black)\")                \n",
    "                print(\"-\"*100)\n",
    "            \n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmBAlSIDFeOp",
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset=\"IHM\"\n",
    "dataset=\"mini-\"+dataset\n",
    "run_analyses(input_path, dataset, model, output_path, tmp_path, clear_results=False, tune=False)\n",
    "plot_analyses(output_path, dataset, model, dataset, tuned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fXq6jZL6pnQ_",
    "outputId": "275e86f8-1392-4f2e-fc6e-cda23bc79e90",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run All the tasks\n",
    "def task_done(path,  task):\n",
    "    with open(path+\"completed_tasks.txt\", \"a\") as fod:\n",
    "        fod.write(task+\"\\n\")\n",
    "\n",
    "def reset_tasks(path):\n",
    "    with open(path+\"completed_tasks.txt\", \"w\") as fod:\n",
    "        fod.write(\"\")\n",
    "\n",
    "def is_task_done(path,  task):\n",
    "    try:\n",
    "        with open(path+\"completed_tasks.txt\", \"r\") as fid:\n",
    "            tasks = fid.read().split(\"\\n\")            \n",
    "        return task in set(tasks)\n",
    "    except FileNotFoundError:\n",
    "        #create file if not found\n",
    "        with open(path+\"completed_tasks.txt\", \"w\") as fid:\n",
    "            fid.write(\"\")\n",
    "        return False\n",
    "\n",
    "def run_tasks(tasks_path, cache_path, mini_tasks=True, reset=False):\n",
    "    #if reset delete the completed tasks file\n",
    "    if reset: reset_tasks(cache_path)\n",
    "    N_TASKS = 9\n",
    "    with open(tasks_path,\"r\") as fid:\n",
    "        for i,l in enumerate(fid):\n",
    "            if i > N_TASKS: break\n",
    "            fname, task_name = l.strip(\"\\n\").split(\",\")            \n",
    "            dataset = \"mini-\"+fname if mini_tasks else fname\n",
    "            # dataset = fname\n",
    "            if is_task_done(cache_path, dataset): \n",
    "                print(\"[dataset: {} already processed]\".format(dataset))\n",
    "                continue                        \n",
    "            print(\"******** {} {} ********\".format(task_name, dataset))      \n",
    "            run_analyses(input_path, dataset, model, output_path, tmp_path, clear_results=False)\n",
    "            task_done(cache_path, dataset)\n",
    "\n",
    "run_tasks(input_path+\"tasks.txt\", tmp_path, mini_tasks=True, reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0uTF5D3E3q8G",
    "outputId": "c408b2d2-fec3-4037-97a4-edc97a009e80",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot All the tasks\n",
    "with open(input_path+\"tasks.txt\",\"r\") as fid:\n",
    "    for i,l in enumerate(fid):\n",
    "#         if i < 4: \n",
    "        fname, task_name = l.strip(\"\\n\").split(\",\")\n",
    "        # dataset = \"mini-\"+fname\n",
    "        dataset = fname\n",
    "        plot_analyses(output_path, dataset, model, task_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric x Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0uTF5D3E3q8G",
    "outputId": "c408b2d2-fec3-4037-97a4-edc97a009e80",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot all the metric scatter plots\n",
    "with open(input_path+\"tasks.txt\",\"r\") as fid:\n",
    "    for i,l in enumerate(fid):\n",
    "        fname, task_name = l.strip(\"\\n\").split(\",\")\n",
    "        # dataset = \"mini-\"+fname\n",
    "        dataset = fname\n",
    "        metric_scatter_plots(output_path, dataset, model, task_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance x Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0uTF5D3E3q8G",
    "outputId": "c408b2d2-fec3-4037-97a4-edc97a009e80",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot all the performance scatter plots\n",
    "with open(input_path+\"tasks.txt\",\"r\") as fid:\n",
    "    for i,l in enumerate(fid):\n",
    "        fname, task_name = l.strip(\"\\n\").split(\",\")\n",
    "        # dataset = \"mini-\"+fname\n",
    "        dataset = fname\n",
    "        performance_scatter_plots(output_path, dataset, model, task_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIMIC_Outcomes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}